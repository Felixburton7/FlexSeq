==========================================================
                FlexSeq: Protein Flexibility ML Pipeline
==========================================================

==========================================================
Example Input Data Format
==========================================================
FlexSeq expects temperature-specific CSV files in the data directory.
Example file name format: 'temperature_320_train.csv'

Expected columns in the CSV:
- domain_id: Protein domain identifier (e.g., '1a0aA00')
- resid: Residue ID (integer position in the protein chain)
- resname: Amino acid type (e.g., ALA, LYS, etc.)
- rmsf_{temperature}: Target RMSF value at the specified temperature
- protein_size: Total number of residues in the protein
- normalized_resid: Position normalized to 0-1 range
- core_exterior: Location classification ('interior' or 'surface')
- relative_accessibility: Solvent accessibility measure (0-1)
- dssp: Secondary structure annotation (H=helix, E=sheet, C=coil, etc.)
- phi, psi: Backbone dihedral angles
- *_encoded: Various numerically encoded categorical features
- phi_norm, psi_norm: Normalized dihedral angles to [-1, 1] range

For OmniFlex mode, additional columns:
- esm_rmsf: Predictions from ESM embeddings
- voxel_rmsf: Predictions from 3D voxel representation

==========================================================
Usage Examples
==========================================================
# Training a model at a specific temperature
flexseq train --temperature 320

# Training models on all available temperatures
flexseq train-all-temps

# Evaluate a trained model
flexseq evaluate --model random_forest --temperature 320

# Generate predictions using the best model
flexseq predict --input new_proteins.csv --temperature 320

# Compare results across temperatures
flexseq compare-temperatures

# Use OmniFlex mode with advanced features
flexseq train --mode omniflex --temperature 320

Project Working Directory: /home/s_felix/flexseq

Project Tree Structure:
---------------------------------------------------------
.
./flexseq
./flexseq/data
./flexseq.egg-info
./flexseq/models
./flexseq/temperature
./flexseq/utils

File Listing (excluding cache, data, output, and test directories):
---------------------------------------------------------
./data
./default_config.yaml
./flexseq/cli.py
./flexseq/config.py
./flexseq/data/__init__.py
./flexseq/data/loader.py
./flexseq/data/processor.py
./flexseq/data/__pycache__
./flexseq/__init__.py
./flexseq/models/base.py
./flexseq/models/__init__.py
./flexseq/models/neural_network.py
./flexseq/models/__pycache__
./flexseq/models/random_forest.py
./flexseq/pipeline.py
./flexseq/__pycache__
./flexseq/temperature/comparison.py
./flexseq/temperature/__init__.py
./flexseq/temperature/__pycache__
./flexseq/utils/helpers.py
./flexseq/utils/__init__.py
./flexseq/utils/metrics.py
./flexseq/utils/__pycache__
./flexseq/utils/visualization.py
./models
./output
./pyproject.toml
./setup.py
./test

==========================================================
Default Configuration (default_config.yaml)
==========================================================
# FlexSeq Configuration

# Paths
paths:
  data_dir: ./data                # Data directory
  output_dir: ./output            # Output directory
  models_dir: ./models            # Saved models directory

# Mode configuration
mode:
  active: "flexseq"               # "flexseq" or "omniflex"
  omniflex:
    use_esm: true                 # Use ESM embeddings feature
    use_voxel: true               # Use 3D voxel feature

# Temperature configuration
temperature:
  current: 320                    # Current temperature to process
  available: [320, 348, 379, 413, 450, "average"]
  comparison:
    enabled: true                 # Generate temperature comparisons
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]

# Dataset configuration
dataset:
  # Data loading
  file_pattern: "temperature_{temperature}_train.csv"
  
  # Domain filtering
  domains:
    include: []                   # Empty means include all domains
    exclude: []                   # Domains to exclude
    min_protein_size: 0           # Minimum protein size
    max_protein_size: null        # Maximum protein size (null = no limit)
  
  # Feature configuration
  features:
    # Required columns that must exist in data
    required:
      - domain_id                 # Domain identifier
      - resid                     # Residue ID
      - resname                   # Residue name
      - rmsf_{temperature}        # Target variable
    
    # Input features with toggles
    use_features:
      protein_size: true          # Size of protein
      normalized_resid: true      # Position in sequence
      relative_accessibility: true # Solvent accessibility
      core_exterior_encoded: true # Core or exterior
      secondary_structure_encoded: true # Secondary structure
      phi_norm: true              # Normalized phi angle
      psi_norm: true              # Normalized psi angle
      resname_encoded: true       # Encoded residue name
      esm_rmsf: false             # ESM embeddings prediction (OmniFlex only)
      voxel_rmsf: false           # 3D voxel prediction (OmniFlex only)
    
    # Feature engineering
    window:
      enabled: true               # Use window-based features
      size: 3                     # Window size (residues on each side)
  
  # Target variable
  target: rmsf_{temperature}      # Templated with current temperature
  
  # Data splitting
  split:
    test_size: 0.2                # Test set size
    validation_size: 0.15         # Validation set size
    stratify_by_domain: true      # Keep domains together
    random_state: 42              # Random seed

# Evaluation settings
evaluation:
  comparison_set: "test"          # Which set to use: "validation" or "test"
  metrics:
    rmse: true                    # Root Mean Squared Error
    mae: true                     # Mean Absolute Error
    r2: true                      # R-squared
    pearson_correlation: true     # Pearson correlation
    spearman_correlation: true    # Spearman rank correlation
    root_mean_square_absolute_error: true  # Root Mean Square Absolute Error

# Model configurations
models:
  # Shared settings
  common:
    cross_validation:
      enabled: false              # Whether to use CV
      folds: 5                    # Number of folds if enabled
    save_best: true               # Save best model
  
  # Neural Network
  neural_network:
    enabled: true                 # Run this model
    architecture:
      hidden_layers: [64, 32]     # Layer sizes
      activation: relu            # Activation function
      dropout: 0.2                # Dropout rate
    training:
      optimizer: adam             # Optimizer
      learning_rate: 0.001        # Learning rate
      batch_size: 32              # Batch size
      epochs: 5                 # Max epochs
      early_stopping: true        # Use early stopping
      patience: 10                # Early stopping patience
    hyperparameter_optimization:
      enabled: false              # Enable hyperparameter optimization
      method: "bayesian"          # "grid", "random", or "bayesian"
      trials: 20                  # Number of trials
      parameters:                 # Parameters to optimize
        hidden_layers:
          - [32, 16]
          - [64, 32]
          - [128, 64]
          - [64, 32, 16]
        learning_rate: [0.01, 0.001, 0.0001]
        batch_size: [16, 32, 64]
        dropout: [0.1, 0.2, 0.3, 0.5]
        activation: ["relu", "leaky_relu"]
  
  # Random Forest
  random_forest:
    enabled: true                 # Run this model
    n_estimators: 100             # Number of trees
    max_depth: null               # Max tree depth
    min_samples_split: 2          # Min samples to split
    min_samples_leaf: 1           # Min samples in leaf
    max_features: 0.7             # Feature fraction
    bootstrap: true               # Use bootstrapping
    randomized_search:
      enabled: false              # Enable RandomizedSearchCV
      n_iter: 20                  # Number of parameter combinations to try
      cv: 3                       # Number of cross-validation folds
      param_distributions:        # Parameter distributions to search
        n_estimators: [50, 100, 200, 300]
        max_depth: [null, 10, 20, 30]
        min_samples_split: [2, 5, 10]
        min_samples_leaf: [1, 2, 4]
        max_features: ["auto", "sqrt", "log2", 0.7]
        bootstrap: [true, false]

# Analysis and visualization
# Analysis and visualization
analysis:
  feature_importance: 
    enabled: true                 # Analyze feature importance
    method: "permutation"         # Use permutation importance
    n_repeats: 10                 # Number of permutation repetitions
    use_validation_data: true     # Use validation data for importance calculation
  
  temperature_comparison:
    enabled: true                 # Compare results across temperatures
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]
    plots:
      histogram: true             # Generate histogram plots
      correlation: true           # Generate correlation plots
      performance: true           # Generate performance comparison plots

# System settings
system:
  n_jobs: -1                      # Number of parallel jobs
  random_state: 42                # Global random seed
  log_level: INFO                 # Logging level
  gpu_enabled: auto               # Auto-detect GPU
==========================================================
FlexSeq Package Files
==========================================================
### Main Package Files ###
---------------------------------------------------------
===== FILE: pyproject.toml =====
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "flexseq"
version = "0.1.0"
description = "ML pipeline for protein flexibility prediction with multi-temperature analysis"
authors = [{name = "Felix Burton", email = "Feliburton2002@gmail.com"}]
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Topic :: Scientific/Engineering :: Bio-Informatics",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "numpy>=1.20.0",
    "pandas>=1.3.0",
    "scikit-learn>=1.0.0",
    "torch>=1.9.0",
    "pyyaml>=6.0",
    "click>=8.0.0",
    "matplotlib>=3.4.0",
    "seaborn>=0.11.0",
    "joblib>=1.0.0",
    "tqdm>=4.64.0",
]

[project.scripts]
flexseq = "flexseq.cli:cli"

[project.urls]
"Homepage" = "https://github.com/Felixburton7/flexseq"
"Bug Tracker" = "https://github.com/Felixburton7/flexseq/issues"
===== FILE: setup.py =====
import os
from setuptools import setup, find_packages

# Read the content of README.md
this_directory = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

setup(
    name="flexseq",
    version="0.1.0",
    description="ML pipeline for protein flexibility prediction with multi-temperature analysis",
    long_description=long_description,
    long_description_content_type="text/markdown",
    author="Felix Burton",
    author_email="felixburton2002@gmail.comcom",
    url="https://github.com/Felixburton7/flexseq",
    packages=find_packages(),
    include_package_data=True,
    install_requires=[
        "tqdm>=4.64.0",
        "numpy>=1.20.0",
        "pandas>=1.3.0",
        "scikit-learn>=1.0.0",
        "torch>=1.9.0",
        "pyyaml>=6.0",
        "click>=8.0.0",
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "joblib>=1.0.0",
    ],
    entry_points={
        "console_scripts": [
            "flexseq=flexseq.cli:cli",
        ],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Bio-Informatics",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires=">=3.8",
)
===== FILE: README.md =====

### Core Module Files ###
---------------------------------------------------------
===== FILE: flexseq/__init__.py =====
"""
FlexSeq ML Pipeline for protein flexibility prediction.

This package provides a complete machine learning pipeline for predicting
protein flexibility (RMSF values) from sequence and structural features
across multiple temperatures.
"""

__version__ = "0.1.0"

from flexseq.pipeline import Pipeline
===== FILE: flexseq/config.py =====
"""
Configuration handling for the FlexSeq ML pipeline.

This module provides functions for loading, validating, and managing
configuration settings throughout the pipeline, with special support
for temperature-specific configurations.
"""

import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
import yaml
import re

logger = logging.getLogger(__name__)

def deep_merge(base_dict: Dict, overlay_dict: Dict) -> Dict:
    """
    Recursively merge two dictionaries, with values from overlay_dict taking precedence.
    
    Args:
        base_dict: Base dictionary to merge into
        overlay_dict: Dictionary with values that should override base_dict
        
    Returns:
        Dict containing merged configuration
    """
    result = base_dict.copy()
    
    for key, value in overlay_dict.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
            
    return result

def get_env_var_config() -> Dict[str, Any]:
    """
    Get configuration from environment variables.
    Environment variables should be prefixed with FLEXSEQ_ and use
    underscore separators for nested keys.
    
    Examples:
        FLEXSEQ_PATHS_DATA_DIR=/path/to/data
        FLEXSEQ_MODELS_RANDOM_FOREST_N_ESTIMATORS=200
        FLEXSEQ_TEMPERATURE_CURRENT=320
        
    Returns:
        Dict containing configuration from environment variables
    """
    config = {}
    
    for key, value in os.environ.items():
        if not key.startswith("FLEXSEQ_"):
            continue
            
        # Remove prefix and convert to lowercase
        key = key[8:].lower()
        
        # Split into parts and create nested dict
        parts = key.split("_")
        current = config
        
        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]
            
        # Set value, converting to appropriate type
        value_part = parts[-1]
        
        # Try to convert to appropriate type
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                if "." in value:
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                # Keep as string if not convertible
                pass
                
        current[value_part] = value
        
    return config

def parse_param_overrides(params: List[str]) -> Dict[str, Any]:
    """
    Parse parameter overrides from CLI arguments.
    
    Args:
        params: List of parameter overrides in format "key=value"
        
    Returns:
        Dict containing parameter overrides
    """
    if not params:
        return {}
        
    override_dict = {}
    
    for param in params:
        if "=" not in param:
            logger.warning(f"Ignoring invalid parameter override: {param}")
            continue
            
        key, value = param.split("=", 1)
        
        # Convert value to appropriate type
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                if "." in value:
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                # Keep as string if not convertible
                pass
                
        # Split key into parts and create nested dict
        parts = key.split(".")
        current = override_dict
        
        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]
            
        current[parts[-1]] = value
        
    return override_dict

def template_config_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> Dict[str, Any]:
    """
    Apply temperature-specific templating to configuration.
    
    Replaces {temperature} placeholders in strings with the specified temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value to use in templating
        
    Returns:
        Dictionary with templated configuration
    """
    # Create deep copy to avoid modifying the original
    import copy
    result = copy.deepcopy(config)
    
    def replace_in_dict(d):
        for key, value in d.items():
            if isinstance(value, dict):
                replace_in_dict(value)
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        replace_in_dict(item)
                    elif isinstance(item, str):
                        d[key][i] = item.replace("{temperature}", str(temperature))
            elif isinstance(value, str):
                d[key] = value.replace("{temperature}", str(temperature))
                
    replace_in_dict(result)
    
    return result

def load_config(
    config_path: Optional[str] = None,
    param_overrides: Optional[List[str]] = None,
    use_env_vars: bool = True,
    temperature: Optional[Union[int, str]] = None
) -> Dict[str, Any]:
    """
    Load configuration from default and user-provided sources.
    
    Args:
        config_path: Optional path to user config file
        param_overrides: Optional list of parameter overrides
        use_env_vars: Whether to use environment variables
        temperature: Optional temperature value for templating
        
    Returns:
        Dict containing merged configuration
        
    Raises:
        FileNotFoundError: If config_path is provided but file doesn't exist
        ValueError: If configuration is invalid
    """
    # Determine default config path
    default_path = os.path.join(os.path.dirname(__file__), "..", "default_config.yaml")
    if not os.path.exists(default_path):
        package_dir = os.path.dirname(os.path.abspath(__file__))
        default_path = os.path.join(package_dir, "..", "default_config.yaml")
        
    # Load default config
    if not os.path.exists(default_path):
        raise FileNotFoundError(f"Default config not found at {default_path}")
        
    with open(default_path, 'r') as f:
        config = yaml.safe_load(f)
        
    # Overlay user config if provided
    if config_path:
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"User config not found at {config_path}")
            
        with open(config_path, 'r') as f:
            user_config = yaml.safe_load(f)
            config = deep_merge(config, user_config)
            
    # Apply environment variable overrides
    if use_env_vars:
        env_config = get_env_var_config()
        config = deep_merge(config, env_config)
        
    # Apply CLI parameter overrides
    if param_overrides:
        override_config = parse_param_overrides(param_overrides)
        config = deep_merge(config, override_config)
    
    # Apply temperature override if provided
    if temperature is not None:
        config["temperature"]["current"] = temperature
    
    # Get current temperature from config
    current_temp = config["temperature"]["current"]
    
    # Apply temperature templating
    config = template_config_for_temperature(config, current_temp)
    
    # Handle OmniFlex mode settings
    if config["mode"]["active"].lower() == "omniflex":
        # Enable ESM and voxel features if in OmniFlex mode
        if config["mode"]["omniflex"]["use_esm"]:
            config["dataset"]["features"]["use_features"]["esm_rmsf"] = True
        
        if config["mode"]["omniflex"]["use_voxel"]:
            config["dataset"]["features"]["use_features"]["voxel_rmsf"] = True
    
    # Validate config (basic validation)
    validate_config(config)
    
    # Set system-wide logging level
    log_level = config.get("system", {}).get("log_level", "INFO")
    numeric_level = getattr(logging, log_level.upper(), None)
    if isinstance(numeric_level, int):
        logging.getLogger().setLevel(numeric_level)
    
    return config

def validate_config(config: Dict[str, Any]) -> None:
    """
    Perform basic validation of configuration.
    
    Args:
        config: Configuration dictionary to validate
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Check required sections
    required_sections = ["paths", "dataset", "models", "evaluation", "system", "temperature", "mode"]
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required config section: {section}")
            
    # Validate dataset section
    if "target" not in config["dataset"]:
        raise ValueError("Missing required dataset.target configuration")
    
    # Validate temperature section
    current_temp = config["temperature"]["current"]
    available_temps = config["temperature"]["available"]
    
    if str(current_temp) not in [str(t) for t in available_temps]:
        raise ValueError(f"Current temperature {current_temp} is not in the list of available temperatures")
    
    # Check that at least one model is enabled
    any_model_enabled = False
    for model_name, model_config in config.get("models", {}).items():
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            any_model_enabled = True
            break
            
    if not any_model_enabled:
        logger.warning("No models are enabled in configuration")
        
    # Additional validation could be added as needed
            
def get_enabled_models(config: Dict[str, Any]) -> List[str]:
    """
    Get list of enabled model names from config.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of enabled model names
    """
    enabled_models = []
    
    for model_name, model_config in config.get("models", {}).items():
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            enabled_models.append(model_name)
            
    return enabled_models

def get_model_config(config: Dict[str, Any], model_name: str) -> Dict[str, Any]:
    """
    Get configuration for a specific model, with common settings applied.
    
    Args:
        config: Full configuration dictionary
        model_name: Name of the model
        
    Returns:
        Model-specific configuration with common settings merged in
        
    Raises:
        ValueError: If model_name is not found in configuration
    """
    models_config = config.get("models", {})
    
    if model_name not in models_config:
        raise ValueError(f"Model '{model_name}' not found in configuration")
        
    model_config = models_config[model_name]
    common_config = models_config.get("common", {})
    
    # Merge common config with model-specific config
    merged_config = deep_merge(common_config, model_config)
    
    return merged_config

def get_available_temperatures(config: Dict[str, Any]) -> List[Union[int, str]]:
    """
    Get list of available temperatures from config.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of available temperatures
    """
    return config["temperature"]["available"]

def get_output_dir_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> str:
    """
    Get output directory path for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value
        
    Returns:
        Path to output directory for the specified temperature
    """
    base_output_dir = config["paths"]["output_dir"]
    return os.path.join(base_output_dir, f"outputs_{temperature}")

def get_models_dir_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> str:
    """
    Get models directory path for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value
        
    Returns:
        Path to models directory for the specified temperature
    """
    base_models_dir = config["paths"]["models_dir"]
    return os.path.join(base_models_dir, f"models_{temperature}")

def get_comparison_output_dir(config: Dict[str, Any]) -> str:
    """
    Get output directory path for temperature comparisons.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Path to output directory for temperature comparisons
    """
    base_output_dir = config["paths"]["output_dir"]
    return os.path.join(base_output_dir, "outputs_comparison")
===== FILE: flexseq/pipeline.py =====
"""
Main pipeline orchestration for the FlexSeq ML workflow.

This module provides the Pipeline class that handles the entire ML workflow
from data loading to evaluation, with temperature-specific functionality.
"""

import os
import logging
import time
from typing import Dict, List, Tuple, Optional, Any, Union

import pandas as pd
import numpy as np
import joblib

from flexseq.config import (
    load_config, 
    get_enabled_models, 
    get_model_config,
    get_output_dir_for_temperature
)
from flexseq.utils.helpers import progress_bar, ProgressCallback
from flexseq.models import get_model_class
from flexseq.data.processor import (
    load_and_process_data, 
    split_data, 
    prepare_data_for_model,
    process_features
)
from flexseq.utils.metrics import evaluate_predictions


logger = logging.getLogger(__name__)

class Pipeline:
    """
    Main pipeline orchestration for FlexSeq.
    Handles the full ML workflow from data loading to evaluation.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the pipeline with configuration.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.models = {}
        
        # Create output directories if they don't exist
        self.prepare_directories()
        
        # Log mode information
        mode = config["mode"]["active"]
        logger.info(f"Pipeline initialized in {mode.upper()} mode")
        
        # Log temperature information
        temperature = config["temperature"]["current"]
        logger.info(f"Using temperature: {temperature}")
        
    def prepare_directories(self) -> None:
        """
        Create necessary output directories.
        """
        paths = self.config["paths"]
        
        # Ensure data directory exists
        data_dir = paths.get("data_dir", "./data")
        os.makedirs(data_dir, exist_ok=True)
        
        # Ensure output directory exists
        output_dir = paths.get("output_dir", "./output")
        os.makedirs(output_dir, exist_ok=True)
        
        # Ensure models directory exists
        models_dir = paths.get("models_dir", "./models")
        os.makedirs(models_dir, exist_ok=True)
        
        # Create subdirectories for different types of output
        os.makedirs(os.path.join(output_dir, "comparisons"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "feature_importance"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "residue_analysis"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "domain_analysis"), exist_ok=True)
        
    def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
        """
        Load and process input data.
        
        Args:
            data_path: Optional explicit path to data file
            
        Returns:
            Processed DataFrame
        """
        # Use explicit file path or temperature-specific data
        return load_and_process_data(data_path, self.config)
    
    def train(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Train specified models on the data.
        
        Args:
            model_names: Optional list of model names to train (if None, use all enabled models)
            data_path: Optional explicit path to data file
            
        Returns:
            Dictionary of trained models
        """
        from flexseq.utils.helpers import progress_bar
        
        # Determine which models to train
        if model_names is None:
            model_names = get_enabled_models(self.config)
            
        if not model_names:
            logger.warning("No models specified for training")
            return {}
            
        # Load and preprocess data
        logger.info("Loading and processing data")
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        logger.info("Splitting data into train/validation/test sets")
        with ProgressCallback(total=1, desc="Splitting data") as pbar:
            train_df, val_df, test_df = split_data(df, self.config)
            pbar.update()
        
        # Prepare training data
        with ProgressCallback(total=1, desc="Preparing features") as pbar:
            X_train, y_train, feature_names = prepare_data_for_model(train_df, self.config)
            pbar.update()
        
        # Train each model
        trained_models = {}
        
        for model_name in progress_bar(model_names, desc="Training models"):
            logger.info(f"Training model: {model_name}")
            
            try:
                # Get model class and config
                model_class = get_model_class(model_name)
                model_config = get_model_config(self.config, model_name)
                
                if not model_config.get("enabled", False):
                    logger.warning(f"Model {model_name} is disabled in config. Skipping.")
                    continue
                
                # Get hyperparameter optimization config
                optimize_hyperparams = False
                if model_name == "neural_network":
                    optimize_hyperparams = model_config.get("hyperparameter_optimization", {}).get("enabled", False)
                elif model_name == "random_forest":
                    optimize_hyperparams = model_config.get("randomized_search", {}).get("enabled", False)
                
                # Remove non-init params from config
                if model_name == "neural_network":
                    init_params = {
                        "architecture": model_config.get("architecture", {}),
                        "training": model_config.get("training", {}),
                        "random_state": self.config["system"].get("random_state", 42)
                    }
                else:
                    init_params = {k: v for k, v in model_config.items() 
                                if k not in ['enabled', 'cross_validation', 'save_best', 'randomized_search', 'hyperparameter_optimization']}
                
                # Create model instance
                model = model_class(**init_params)
                
                # Perform hyperparameter optimization if enabled
                if optimize_hyperparams:
                    with ProgressCallback(total=1, desc=f"Optimizing hyperparameters for {model_name}") as pbar:
                        logger.info(f"Performing hyperparameter optimization for {model_name}")
                        
                        if model_name == "neural_network":
                            opt_config = model_config["hyperparameter_optimization"]
                            method = opt_config.get("method", "bayesian")
                            trials = opt_config.get("trials", 20)
                            param_grid = opt_config.get("parameters", {})
                            
                            # Prepare validation data for hyperparameter tuning
                            X_val, y_val, _ = prepare_data_for_model(val_df, self.config)
                            
                            # Combine train and validation for cross-validation
                            X_combined = np.vstack([X_train, X_val])
                            y_combined = np.concatenate([y_train, y_val])
                            
                            # Optimize hyperparameters
                            best_params = model.hyperparameter_optimize(
                                X_combined, y_combined, param_grid, method, trials, cv=3
                            )
                            
                            logger.info(f"Best hyperparameters for {model_name}: {best_params}")
                            
                        elif model_name == "random_forest":
                            # Random forest uses its internal RandomizedSearchCV
                            # Just log that optimization is enabled
                            logger.info("RandomizedSearchCV will be used for Random Forest training")
                        
                        pbar.update()
                
                # Train the model
                start_time = time.time()
                
                if model_name == "neural_network":
                    # Neural network training shows progress
                    model.fit(X_train, y_train, feature_names)
                else:
                    # Other models use simple progress indicator
                    with ProgressCallback(total=1, desc=f"Training {model_name}") as pbar:
                        model.fit(X_train, y_train, feature_names)
                        pbar.update()
                
                # Store trained model
                trained_models[model_name] = model
                
                # Log training time
                train_time = time.time() - start_time
                logger.info(f"Trained {model_name} in {train_time:.2f} seconds")
                
                # Save training history if available
                if hasattr(model, 'get_training_history') and model.get_training_history():
                    history = model.get_training_history()
                    history_df = pd.DataFrame(history)
                    history_path = os.path.join(self.config["paths"]["output_dir"], f"{model_name}_training_history.csv")
                    history_df.to_csv(history_path)
                    logger.info(f"Saved training history to {history_path}")
                
                # Save model if configured
                if model_config.get("save_best", True):
                    with ProgressCallback(total=1, desc=f"Saving {model_name}") as pbar:
                        self.save_model(model, model_name)
                        pbar.update()
                
                # Evaluate on validation set
                with ProgressCallback(total=1, desc=f"Validating {model_name}") as pbar:
                    X_val, y_val, _ = prepare_data_for_model(val_df, self.config)
                    val_predictions = model.predict(X_val)
                    val_metrics = evaluate_predictions(y_val, val_predictions, self.config)
                    logger.info(f"Validation metrics for {model_name}: {val_metrics}")
                    pbar.update()
                
            except Exception as e:
                logger.error(f"Error training model {model_name}: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        self.models = trained_models
        return trained_models

    
    def save_model(self, model: Any, model_name: str) -> None:
        """
        Save a trained model to disk.
        
        Args:
            model: Trained model instance
            model_name: Name of the model
        """
        models_dir = self.config["paths"]["models_dir"]
        model_path = os.path.join(models_dir, f"{model_name}.pkl")
        
        try:
            model.save(model_path)
            logger.info(f"Saved model {model_name} to {model_path}")
        except Exception as e:
            logger.error(f"Error saving model {model_name}: {e}")
    
    def load_model(self, model_name: str) -> Any:
        """
        Load a trained model from disk.
        
        Args:
            model_name: Name of the model to load
            
        Returns:
            Loaded model instance
        """
        models_dir = self.config["paths"]["models_dir"]
        model_path = os.path.join(models_dir, f"{model_name}.pkl")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        try:
            model_class = get_model_class(model_name)
            model = model_class.load(model_path)
            return model
        except Exception as e:
            logger.error(f"Error loading model {model_name}: {e}")
            raise
        
    
    def evaluate(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        Evaluate models on test data.
        
        Args:
            model_names: Optional list of model names to evaluate
            data_path: Optional explicit path to data file
            
        Returns:
            Dictionary of evaluation metrics for each model
        """
        from flexseq.utils.helpers import progress_bar, ProgressCallback
        
        # Determine which models to evaluate
        if model_names is None:
            model_names = list(self.models.keys())
            
            # If no models in memory, use enabled models from config
            if not model_names:
                model_names = get_enabled_models(self.config)
                
        if not model_names:
            logger.warning("No models specified for evaluation")
            return {}
        
        # Load data if needed
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        with ProgressCallback(total=1, desc="Splitting data") as pbar:
            train_df, val_df, test_df = split_data(df, self.config)
            pbar.update()
        
        # Use test or validation set based on config
        comparison_set = self.config["evaluation"]["comparison_set"]
        
        if comparison_set == "test":
            eval_df = test_df
            logger.info("Using test set for evaluation")
        elif comparison_set == "validation":
            eval_df = val_df
            logger.info("Using validation set for evaluation")
        else:
            logger.warning(f"Unknown comparison_set '{comparison_set}', using test set")
            eval_df = test_df
        
        # Prepare evaluation data
        with ProgressCallback(total=1, desc="Preparing features") as pbar:
            X_eval, y_eval, feature_names = prepare_data_for_model(eval_df, self.config)
            pbar.update()
        
        # Evaluate each model
        results = {}
        predictions = {}
        uncertainties = {}
        
        for model_name in progress_bar(model_names, desc="Evaluating models"):
            logger.info(f"Evaluating model: {model_name}")
            
            try:
                # Load model if not in memory
                with ProgressCallback(total=1, desc=f"Loading {model_name}", leave=False) as pbar:
                    if model_name in self.models:
                        model = self.models[model_name]
                    else:
                        model = self.load_model(model_name)
                    pbar.update()
                
                # Generate predictions
                with ProgressCallback(total=1, desc=f"Predicting with {model_name}", leave=False) as pbar:
                    # Try to get uncertainty estimates if available
                    if hasattr(model, 'predict_with_std'):
                        preds, stds = model.predict_with_std(X_eval)
                        predictions[model_name] = preds
                        uncertainties[model_name] = stds
                    else:
                        preds = model.predict(X_eval)
                        predictions[model_name] = preds
                    pbar.update()
                
                # Calculate metrics
                with ProgressCallback(total=1, desc=f"Computing metrics", leave=False) as pbar:
                    metrics = evaluate_predictions(y_eval, preds, self.config, X_eval, X_eval.shape[1])
                    pbar.update()
                
                # Store results
                results[model_name] = metrics
                
                # Log results
                logger.info(f"Evaluation metrics for {model_name}: {metrics}")
                
            except Exception as e:
                logger.error(f"Error evaluating model {model_name}: {e}")
        
        # Save evaluation results
        with ProgressCallback(total=1, desc="Saving evaluation results") as pbar:
            self.save_evaluation_results(results, eval_df, predictions, uncertainties)
            pbar.update()
            
        
        
        return results
    
    def save_evaluation_results(
        self, 
        results: Dict[str, Dict[str, float]],
        eval_df: pd.DataFrame,
        predictions: Dict[str, np.ndarray] = None,
        uncertainties: Dict[str, np.ndarray] = None
    ) -> None:
        """
        Save evaluation results to disk.
        
        Args:
            results: Dictionary of evaluation metrics for each model
            eval_df: DataFrame with evaluation data
            predictions: Dictionary of predictions by model
            uncertainties: Dictionary of prediction uncertainties by model
        """
        output_dir = self.config["paths"]["output_dir"]
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Save metrics to CSV
        results_path = os.path.join(output_dir, "evaluation_results.csv")
        results_df = pd.DataFrame(results).T
        results_df.index.name = "model"
        results_df.to_csv(results_path)
        logger.info(f"Saved evaluation metrics to {results_path}")
        
        # Save all results (predictions and optionally uncertainties)
        if predictions:
            all_results_df = eval_df.copy()
            target_col = self.config["dataset"]["target"]
            
            # Add predictions for each model
            for model_name, preds in predictions.items():
                all_results_df[f"{model_name}_predicted"] = preds
                
                # Add errors
                all_results_df[f"{model_name}_error"] = preds - all_results_df[target_col]
                all_results_df[f"{model_name}_abs_error"] = np.abs(all_results_df[f"{model_name}_error"])
                
                # Add uncertainties if available
                if uncertainties and model_name in uncertainties:
                    all_results_df[f"{model_name}_uncertainty"] = uncertainties[model_name]
            
            # Save to CSV
            all_results_path = os.path.join(output_dir, "all_results.csv")
            all_results_df.to_csv(all_results_path, index=False)
            logger.info(f"Saved detailed results to {all_results_path}")
            
            # Save domain-level metrics
            self.save_domain_metrics(all_results_df, target_col, predictions.keys())
    
    def save_domain_metrics(
        self,
        results_df: pd.DataFrame,
        target_col: str,
        model_names: List[str]
    ) -> None:
        """
        Calculate and save domain-level metrics.
        
        Args:
            results_df: DataFrame with all results
            target_col: Target column name
            model_names: List of model names
        """
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        output_dir = self.config["paths"]["output_dir"]
        domain_metrics = []
        
        # Calculate metrics per domain
        for domain_id, domain_df in results_df.groupby("domain_id"):
            domain_result = {"domain_id": domain_id}
            
            # Calculate metrics for each model
            for model_name in model_names:
                pred_col = f"{model_name}_predicted"
                if pred_col not in domain_df.columns:
                    continue
                
                actual = domain_df[target_col].values
                predicted = domain_df[pred_col].values
                
                # Calculate metrics
                rmse = np.sqrt(mean_squared_error(actual, predicted))
                mae = mean_absolute_error(actual, predicted)
                r2 = r2_score(actual, predicted)
                
                # Store metrics
                domain_result[f"{model_name}_rmse"] = rmse
                domain_result[f"{model_name}_mae"] = mae
                domain_result[f"{model_name}_r2"] = r2
                
                # Basic statistics
                domain_result[f"{model_name}_mean_error"] = np.mean(domain_df[f"{model_name}_error"])
                domain_result[f"{model_name}_std_error"] = np.std(domain_df[f"{model_name}_error"])
                
                # Calculate temperature-dependent metrics if in OmniFlex mode
                if self.config["mode"]["active"] == "omniflex":
                    # Add correlation with ESM and voxel predictions if available
                    for pred_type in ["esm_rmsf", "voxel_rmsf"]:
                        if pred_type in domain_df.columns:
                            corr = np.corrcoef(predicted, domain_df[pred_type])[0, 1]
                            domain_result[f"{model_name}_corr_with_{pred_type}"] = corr
            
            # Add domain properties
            domain_result["num_residues"] = len(domain_df)
            
            # Calculate protein properties if available
            if "core_exterior_encoded" in domain_df.columns:
                domain_result["percent_surface"] = domain_df["core_exterior_encoded"].mean() * 100
                
            if "secondary_structure_encoded" in domain_df.columns:
                # Count residues by structure type
                ss_counts = domain_df["secondary_structure_encoded"].value_counts(normalize=True) * 100
                for ss_type, value in zip([0, 1, 2], ["helix", "sheet", "loop"]):
                    if ss_type in ss_counts:
                        domain_result[f"percent_{value}"] = ss_counts[ss_type]
                    else:
                        domain_result[f"percent_{value}"] = 0.0
                    
            domain_metrics.append(domain_result)
        
        # Save domain metrics to CSV
        if domain_metrics:
            domain_metrics_df = pd.DataFrame(domain_metrics)
            domain_metrics_path = os.path.join(output_dir, "domain_metrics.csv")
            domain_metrics_df.to_csv(domain_metrics_path, index=False)
            logger.info(f"Saved domain-level metrics to {domain_metrics_path}")

    def predict(
        self, 
        data: Union[str, pd.DataFrame],
        model_name: Optional[str] = None,
        with_uncertainty: bool = False
    ) -> pd.DataFrame:
        """
        Generate predictions for new data.
        
        Args:
            data: DataFrame or path to CSV file with protein data
            model_name: Model to use for prediction (if None, use best model)
            with_uncertainty: Whether to include uncertainty estimates
            
        Returns:
            DataFrame with original data and predictions
        """
        # Load data if string path provided
        if isinstance(data, str):
            df = load_and_process_data(data, self.config)
        else:
            df = data.copy()
            df = process_features(df, self.config)
        
        # Determine which model to use
        if model_name is None:
            # Find best model based on previous evaluation
            try:
                output_dir = self.config["paths"]["output_dir"]
                results_path = os.path.join(output_dir, "evaluation_results.csv")
                
                if os.path.exists(results_path):
                    results_df = pd.read_csv(results_path, index_col="model")
                    
                    # Use R^2 or RMSE to determine best model
                    if "r2" in results_df.columns:
                        best_model = results_df["r2"].idxmax()
                    elif "rmse" in results_df.columns:
                        best_model = results_df["rmse"].idxmin()
                    else:
                        best_model = results_df.index[0]
                        
                    model_name = best_model
                    logger.info(f"Using best model based on evaluation: {model_name}")
                else:
                    # No evaluation results, use first enabled model
                    model_name = get_enabled_models(self.config)[0]
                    logger.info(f"No evaluation results found, using enabled model: {model_name}")
                    
            except Exception as e:
                logger.error(f"Error finding best model: {e}")
                # Fall back to first enabled model
                model_name = get_enabled_models(self.config)[0]
        
        # Load model if not in memory
        if model_name in self.models:
            model = self.models[model_name]
        else:
            model = self.load_model(model_name)
        
        # Prepare data for prediction
        X, _, feature_names = prepare_data_for_model(
            df, self.config, include_target=False
        )
        
        # Generate predictions, possibly with uncertainty
        target_col = self.config["dataset"]["target"]
        result_df = df.copy()
        
        if with_uncertainty and hasattr(model, 'predict_with_std'):
            try:
                predictions, uncertainties = model.predict_with_std(X)
                
                # Add predictions and uncertainties to result
                result_df[f"{target_col}_predicted"] = predictions
                result_df[f"{target_col}_uncertainty"] = uncertainties
                
                # If in OmniFlex mode, add prediction quality indicators
                if self.config["mode"]["active"] == "omniflex":
                    # Calculate z-scores (deviation / uncertainty)
                    if target_col in result_df.columns:
                        z_scores = np.abs(predictions - result_df[target_col]) / uncertainties
                        result_df[f"{target_col}_z_score"] = z_scores
                
            except Exception as e:
                logger.error(f"Error generating predictions with uncertainty: {e}")
                # Fall back to standard prediction
                predictions = model.predict(X)
                result_df[f"{target_col}_predicted"] = predictions
        else:
            # Standard prediction without uncertainty
            predictions = model.predict(X)
            result_df[f"{target_col}_predicted"] = predictions
            
            # If target is available, calculate error
            if target_col in result_df.columns:
                result_df[f"{target_col}_error"] = predictions - result_df[target_col]
                result_df[f"{target_col}_abs_error"] = np.abs(result_df[f"{target_col}_error"])
        
        return result_df
    
    def analyze(
        self,
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> None:
        """
        Perform analysis of model results.
        
        Args:
            model_names: Optional list of model names to analyze
            data_path: Optional explicit path to data file
        """
        from flexseq.utils.helpers import progress_bar, ProgressCallback
        
        # Determine which models to analyze
        if model_names is None:
            model_names = list(self.models.keys())
            
            # If no models in memory, use enabled models from config
            if not model_names:
                model_names = get_enabled_models(self.config)
                
        if not model_names:
            logger.warning("No models specified for analysis")
            return
        
        # Load data if needed
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        with ProgressCallback(total=1, desc="Preparing test data") as pbar:
            _, _, test_df = split_data(df, self.config)
            pbar.update()
        
        # Generate predictions for each model
        predictions = {}
        feature_importances = {}
        
        for model_name in progress_bar(model_names, desc="Analyzing models"):
            try:
                # Load model if not in memory
                if model_name in self.models:
                    model = self.models[model_name]
                else:
                    with ProgressCallback(total=1, desc=f"Loading {model_name}", leave=False) as pbar:
                        model = self.load_model(model_name)
                        pbar.update()
                
                # Prepare data
                with ProgressCallback(total=1, desc="Preparing features", leave=False) as pbar:
                    X_test, y_test, feature_names = prepare_data_for_model(test_df, self.config)
                    pbar.update()
                
                # Generate predictions
                with ProgressCallback(total=1, desc=f"Predicting with {model_name}", leave=False) as pbar:
                    predictions[model_name] = model.predict(X_test)
                    pbar.update()
                
                
                # Get feature importances if available
                importance = model.get_feature_importance()
                if importance:
                    feature_importances[model_name] = importance
                    
                    # Save feature importance to CSV
                    importance_df = pd.DataFrame({
                        'feature': list(importance.keys()),
                        'importance': list(importance.values())
                    })
                    importance_df = importance_df.sort_values('importance', ascending=False)
                    
                    output_dir = self.config["paths"]["output_dir"]
                    importance_path = os.path.join(
                        output_dir, 
                        "feature_importance", 
                        f"{model_name}_feature_importance.csv"
                    )
                    os.makedirs(os.path.dirname(importance_path), exist_ok=True)
                    importance_df.to_csv(importance_path, index=False)
                    logger.info(f"Saved feature importance to {importance_path}")
                    
            except Exception as e:
                logger.error(f"Error analyzing model {model_name}: {e}")
        
        # Generate combined results
        target_col = self.config["dataset"]["target"]
        target_values = test_df[target_col].values
        
        # Generate a combined results dataframe
        combined_df = test_df.copy()
        
        for model_name, preds in predictions.items():
            combined_df[f"{model_name}_predicted"] = preds
            combined_df[f"{model_name}_error"] = preds - combined_df[target_col]
            combined_df[f"{model_name}_abs_error"] = np.abs(combined_df[f"{model_name}_error"])
        
        # Save combined results
        output_dir = self.config["paths"]["output_dir"]
        combined_path = os.path.join(output_dir, "combined_analysis_results.csv")
        combined_df.to_csv(combined_path, index=False)
        logger.info(f"Saved combined analysis results to {combined_path}")
        
            # Import visualization functions
        from flexseq.utils.visualization import (
            plot_r2_comparison,
            plot_residue_level_rmsf,
            plot_amino_acid_error_analysis,
            plot_amino_acid_error_boxplot,
            plot_amino_acid_scatter_plot,
            plot_error_analysis_by_property,
            plot_r2_comparison_scatter,
            plot_scatter_with_density_contours,
            plot_flexibility_vs_dihedral_angles,
            plot_flexibility_sequence_neighborhood,
            plot_error_response_surface,
            plot_secondary_structure_error_correlation
        )

        # Prepare data for visualization
        predictions = {}
        for model_name in model_names:
            pred_col = f"{model_name}_predicted"
            if pred_col in combined_df.columns:
                predictions[model_name] = combined_df[pred_col].values

        # Generate various visualizations
        plot_r2_comparison(predictions, combined_df[target_col].values, model_names, self.config)
        plot_residue_level_rmsf(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_error_analysis(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_error_boxplot(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_scatter_plot(combined_df, predictions, target_col, model_names, self.config)
        plot_error_analysis_by_property(combined_df, predictions, target_col, model_names, self.config)
        plot_r2_comparison_scatter(predictions, combined_df[target_col].values, model_names, self.config)
        plot_scatter_with_density_contours(combined_df, predictions, target_col, model_names, self.config)
        plot_flexibility_vs_dihedral_angles(combined_df, predictions, target_col, model_names, self.config)
        plot_flexibility_sequence_neighborhood(combined_df, predictions, target_col, model_names, self.config)
        plot_error_response_surface(combined_df, predictions, target_col, model_names, self.config)
        plot_secondary_structure_error_correlation(combined_df, predictions, target_col, model_names, self.config)

        
        # Generate residue-level analysis
        self.residue_level_analysis(combined_df, model_names)
        
        # Generate secondary structure analysis
        self.secondary_structure_analysis(combined_df, model_names)
        
        # Generate amino acid type analysis
        self.amino_acid_analysis(combined_df, model_names)
        
        # Process visualization data (CSV files for later visualization)
        self.generate_visualization_data(combined_df, model_names)
    
    def residue_level_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform residue-level analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        target_col = self.config["dataset"]["target"]
        output_dir = self.config["paths"]["output_dir"]
        residue_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(residue_dir, exist_ok=True)
        
        # Calculate error statistics per residue position
        residue_stats = []
        
        # Group by normalized residue position (if available) or by residue ID
        groupby_col = "normalized_resid" if "normalized_resid" in df.columns else "resid"
        
        # Bin values if using normalized_resid
        if groupby_col == "normalized_resid":
            df["resid_bin"] = pd.cut(df[groupby_col], bins=20, labels=False)
            groupby_col = "resid_bin"
        
        for pos, group in df.groupby(groupby_col):
            row = {groupby_col: pos, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            residue_stats.append(row)
        
        if residue_stats:
            residue_df = pd.DataFrame(residue_stats)
            residue_path = os.path.join(residue_dir, "residue_position_errors.csv")
            residue_df.to_csv(residue_path, index=False)
            logger.info(f"Saved residue position analysis to {residue_path}")
    
    def secondary_structure_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform secondary structure analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        if "secondary_structure_encoded" not in df.columns:
            logger.warning("Secondary structure information not available for analysis")
            return
        
        output_dir = self.config["paths"]["output_dir"]
        ss_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(ss_dir, exist_ok=True)
        
        # Map encoded values to types
        ss_mapping = {0: "helix", 1: "sheet", 2: "loop"}
        
        # Calculate error statistics per secondary structure type
        ss_stats = []
        
        for ss_code, group in df.groupby("secondary_structure_encoded"):
            ss_type = ss_mapping.get(ss_code, f"unknown_{ss_code}")
            row = {"secondary_structure": ss_type, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            ss_stats.append(row)
        
        if ss_stats:
            ss_df = pd.DataFrame(ss_stats)
            ss_path = os.path.join(ss_dir, "secondary_structure_errors.csv")
            ss_df.to_csv(ss_path, index=False)
            logger.info(f"Saved secondary structure analysis to {ss_path}")
    
    def amino_acid_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform amino acid-specific analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        output_dir = self.config["paths"]["output_dir"]
        aa_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(aa_dir, exist_ok=True)
        
        # Calculate error statistics per amino acid type
        aa_stats = []
        
        for aa, group in df.groupby("resname"):
            row = {"resname": aa, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            aa_stats.append(row)
        
        if aa_stats:
            aa_df = pd.DataFrame(aa_stats)
            aa_path = os.path.join(aa_dir, "amino_acid_errors.csv")
            aa_df.to_csv(aa_path, index=False)
            logger.info(f"Saved amino acid analysis to {aa_path}")
    
    def generate_visualization_data(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Generate data files for visualizations.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        target_col = self.config["dataset"]["target"]
        output_dir = self.config["paths"]["output_dir"]
        vis_dir = os.path.join(output_dir, "visualization_data")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Generate histogram data for RMSF distributions
        try:
            histogram_data = []
            
            # Get actual values
            actual_values = df[target_col].dropna()
            
            actual_hist, actual_bins = np.histogram(actual_values, bins=20)
            
            for i in range(len(actual_hist)):
                histogram_data.append({
                    'source': 'actual',
                    'bin_start': actual_bins[i],
                    'bin_end': actual_bins[i+1],
                    'count': actual_hist[i]
                })
            
            # Get predicted values for each model
            for model_name in model_names:
                pred_col = f"{model_name}_predicted"
                if pred_col in df.columns:
                    pred_values = df[pred_col].dropna()
                    pred_hist, pred_bins = np.histogram(pred_values, bins=actual_bins)
                    
                    for i in range(len(pred_hist)):
                        histogram_data.append({
                            'source': model_name,
                            'bin_start': pred_bins[i],
                            'bin_end': pred_bins[i+1],
                            'count': pred_hist[i]
                        })
            
            # Save histogram data
            histogram_df = pd.DataFrame(histogram_data)
            histogram_path = os.path.join(vis_dir, "rmsf_distribution.csv")
            histogram_df.to_csv(histogram_path, index=False)
            logger.info(f"Saved RMSF distribution data to {histogram_path}")
            
        except Exception as e:
            logger.error(f"Error generating histogram data: {e}")
        
        # Generate scatter plot data
        try:
            scatter_data = []
            
            # Sample rows to avoid too large data files
            sample_size = min(5000, len(df))
            sampled_df = df.sample(sample_size, random_state=self.config["system"]["random_state"])
            
            for _, row in sampled_df.iterrows():
                data_point = {
                    'domain_id': row['domain_id'],
                    'resid': row['resid'],
                    'resname': row['resname'],
                    'actual': row[target_col]
                }
                
                # Add predicted values
                for model_name in model_names:
                    pred_col = f"{model_name}_predicted"
                    if pred_col in row:
                        data_point[model_name] = row[pred_col]
                
                # Add structural features if available
                for feature in ['secondary_structure_encoded', 'core_exterior_encoded', 'normalized_resid']:
                    if feature in row:
                        data_point[feature] = row[feature]
                
                scatter_data.append(data_point)
            
            # Save scatter data
            scatter_df = pd.DataFrame(scatter_data)
            scatter_path = os.path.join(vis_dir, "rmsf_scatter_data.csv")
            scatter_df.to_csv(scatter_path, index=False)
            logger.info(f"Saved scatter plot data to {scatter_path}")
            
        except Exception as e:
            logger.error(f"Error generating scatter plot data: {e}")
    
    def run_pipeline(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None,
        skip_visualization: bool = False
    ) -> Dict[str, Dict[str, float]]:
        """
        Run the complete pipeline: train, evaluate, and analyze.
        
        Args:
            model_names: Optional list of model names to use
            data_path: Optional explicit path to data file
            skip_visualization: Whether to skip visualization data generation
            
        Returns:
            Dictionary of evaluation metrics for each model
        """
        # Train models
        self.train(model_names, data_path)
        
        # Evaluate models
        results = self.evaluate(model_names, data_path)
        
        # Analyze (optional)
        if not skip_visualization:
            self.analyze(model_names, data_path)
        
        return results
===== FILE: flexseq/cli.py =====
"""
Command-line interface for the FlexSeq ML pipeline.

This module provides the CLI commands for training, evaluating, and
analyzing protein flexibility predictions across multiple temperatures.
"""

import os
import sys
import logging
from typing import List, Optional, Tuple, Dict, Any, Union

import click

from flexseq.config import (
    load_config, 
    get_enabled_models, 
    get_model_config,
    get_available_temperatures,
    get_output_dir_for_temperature,
    get_models_dir_for_temperature,
    get_comparison_output_dir
)
from flexseq.pipeline import Pipeline
from flexseq.models import get_available_models

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def parse_model_list(model_arg: Optional[str]) -> List[str]:
    """
    Parse comma-separated list of models.
    
    Args:
        model_arg: Comma-separated model names or None
        
    Returns:
        List of model names
    """
    if not model_arg:
        return []
        
    return [m.strip() for m in model_arg.split(",")]

@click.group()
@click.version_option(version="0.1.0")
def cli():
    """
    FlexSeq: ML pipeline for protein flexibility prediction.
    
    This tool provides a complete pipeline for predicting protein flexibility
    (RMSF values) from sequence and structural features using machine learning
    across multiple temperatures.
    """
    pass

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter (e.g. models.random_forest.n_estimators=200)")
@click.option("--domains", 
              help="Specific domains to include (comma-separated)")
@click.option("--exclude-domains", 
              help="Domains to exclude (comma-separated)")
@click.option("--disable-feature", 
              help="Features to disable (comma-separated)")
@click.option("--window-size", 
              type=int, 
              help="Window size for feature engineering")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def train(
    model, config, param, domains, exclude_domains, 
    disable_feature, window_size, input, temperature, mode
):
    """
    Train flexibility prediction models.
    
    Examples:
        flexseq train
        flexseq train --model random_forest
        flexseq train --temperature 320
        flexseq train --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Apply CLI-specific overrides
    if domains:
        domain_list = [d.strip() for d in domains.split(",")]
        cfg["dataset"]["domains"]["include"] = domain_list
        
    if exclude_domains:
        exclude_list = [d.strip() for d in exclude_domains.split(",")]
        cfg["dataset"]["domains"]["exclude"] = exclude_list
        
    if disable_feature:
        features = [f.strip() for f in disable_feature.split(",")]
        for feature in features:
            if feature in cfg["dataset"]["features"]["use_features"]:
                cfg["dataset"]["features"]["use_features"][feature] = False
                
    if window_size is not None:
        cfg["dataset"]["features"]["window"]["size"] = window_size
    
    # Determine which models to train
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Create temperature-specific output directory
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # Create pipeline and train models
    pipeline = Pipeline(cfg)
    
    try:
        trained_models = pipeline.train(model_list, input)
        click.echo(f"Successfully trained {len(trained_models)} models for temperature {current_temp}")
    except Exception as e:
        click.echo(f"Error during training: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to evaluate (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def evaluate(model, config, param, input, temperature, mode):
    """
    Evaluate trained models.
    
    Examples:
        flexseq evaluate
        flexseq evaluate --model random_forest
        flexseq evaluate --temperature 320
        flexseq evaluate --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Determine which models to evaluate
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create pipeline and evaluate models
    pipeline = Pipeline(cfg)
    
    try:
        results = pipeline.evaluate(model_list, input)
        
        # Display results
        click.echo("\nEvaluation Results:")
        for model_name, metrics in results.items():
            click.echo(f"\n{model_name}:")
            for metric, value in metrics.items():
                click.echo(f"  {metric}: {value:.4f}")
        
    except Exception as e:
        click.echo(f"Error during evaluation: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to use (defaults to best model)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              required=True,
              help="Input data file (CSV)")
@click.option("--output", 
              type=click.Path(), 
              help="Output file path (defaults to input_predictions.csv)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
@click.option("--uncertainty",
              is_flag=True,
              help="Include uncertainty estimates in predictions")
def predict(model, config, param, input, output, temperature, mode, uncertainty):
    """
    Generate predictions for new data.
    
    Examples:
        flexseq predict --input new_proteins.csv
        flexseq predict --model random_forest --input new_proteins.csv
        flexseq predict --temperature 320 --input new_proteins.csv
        flexseq predict --mode omniflex --input new_proteins.csv
        flexseq predict --uncertainty --input new_proteins.csv
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create pipeline
    pipeline = Pipeline(cfg)
    
    try:
        # Generate predictions
        predictions_df = pipeline.predict(input, model, with_uncertainty=uncertainty)
        
        # Determine output path
        if not output:
            base = os.path.splitext(input)[0]
            output = f"{base}_predictions_{current_temp}.csv"
            
        # Save predictions
        os.makedirs(os.path.dirname(os.path.abspath(output)), exist_ok=True)
        predictions_df.to_csv(output, index=False)
        click.echo(f"Saved predictions to {output}")
        
    except Exception as e:
        click.echo(f"Error generating predictions: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def train_all_temps(model, config, param, mode):
    """
    Train models on all available temperatures.
    
    Examples:
        flexseq train-all-temps
        flexseq train-all-temps --model random_forest
        flexseq train-all-temps --mode omniflex
    """
    # Load configuration without temperature override
    cfg = load_config(config, param)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get all available temperatures
    temperatures = get_available_temperatures(cfg)
    
    # Determine which models to train
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Train for each temperature
    for temp in temperatures:
        click.echo(f"\nTraining for temperature: {temp}")
        
        # Create temperature-specific config
        temp_cfg = load_config(config, param, temperature=temp)
        
        if mode:
            temp_cfg["mode"]["active"] = mode
        
        # Get temperature-specific directories
        output_dir = get_output_dir_for_temperature(temp_cfg, temp)
        models_dir = get_models_dir_for_temperature(temp_cfg, temp)
        
        # Update config with temperature-specific directories
        temp_cfg["paths"]["output_dir"] = output_dir
        temp_cfg["paths"]["models_dir"] = models_dir
        
        # Create directories
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(models_dir, exist_ok=True)
        
        # Create pipeline and train models
        pipeline = Pipeline(temp_cfg)
        
        try:
            trained_models = pipeline.train(model_list)
            click.echo(f"Successfully trained {len(trained_models)} models for temperature {temp}")
            
            # Evaluate models
            results = pipeline.evaluate(model_list)
            
            # Display results
            click.echo(f"Evaluation Results for temperature {temp}:")
            for model_name, metrics in results.items():
                click.echo(f"{model_name}:")
                for metric, value in metrics.items():
                    click.echo(f"  {metric}: {value:.4f}")
                    
        except Exception as e:
            click.echo(f"Error during training for temperature {temp}: {e}")
            # Continue with next temperature

@cli.command()
@click.option("--model", 
              help="Model to compare (defaults to random_forest)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def compare_temperatures(model, config, param, mode):
    """
    Compare results across temperatures.
    
    Examples:
        flexseq compare-temperatures
        flexseq compare-temperatures --model neural_network
    """
    from flexseq.temperature.comparison import prepare_temperature_comparison_data
    
    # Load configuration
    cfg = load_config(config, param)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get model to use
    if not model:
        model = "random_forest"  # Default to random forest
        click.echo(f"No model specified, using {model}")
    
    # Get all available temperatures
    temperatures = get_available_temperatures(cfg)
    
    # Check if temperature comparison is enabled
    if not cfg["temperature"]["comparison"]["enabled"]:
        click.echo("Temperature comparison is disabled in config")
        return
    
    # Get comparison output directory
    comparison_dir = get_comparison_output_dir(cfg)
    os.makedirs(comparison_dir, exist_ok=True)
    
    # Generate comparison data
    try:
        comparison_data = prepare_temperature_comparison_data(cfg, model, comparison_dir)
        
        click.echo(f"\nTemperature comparison data saved to {comparison_dir}")
        
        # Display available files
        click.echo("Generated files:")
        for filename in os.listdir(comparison_dir):
            file_path = os.path.join(comparison_dir, filename)
            if os.path.isfile(file_path):
                file_size = os.path.getsize(file_path)
                click.echo(f"  {filename} ({file_size} bytes)")
        
    except Exception as e:
        click.echo(f"Error during temperature comparison: {e}")
        sys.exit(1)

@cli.command()
@click.option("--input", 
              type=click.Path(exists=True), 
              required=True,
              help="Input data file (CSV)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--output", 
              type=click.Path(), 
              help="Output file path (defaults to input_processed.csv)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def preprocess(input, config, param, output, temperature, mode):
    """
    Preprocess data only without training or prediction.
    
    Examples:
        flexseq preprocess --input raw_data.csv
        flexseq preprocess --temperature 320 --input raw_data.csv
        flexseq preprocess --mode omniflex --input raw_data.csv
    """
    from flexseq.data.processor import load_and_process_data
    
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    try:
        # Process data
        processed_df = load_and_process_data(input, cfg)
        
        # Determine output path
        if not output:
            base = os.path.splitext(input)[0]
            current_temp = cfg["temperature"]["current"]
            output = f"{base}_processed_{current_temp}.csv"
            
        # Save processed data
        os.makedirs(os.path.dirname(os.path.abspath(output)), exist_ok=True)
        processed_df.to_csv(output, index=False)
        click.echo(f"Saved processed data to {output}")
        
    except Exception as e:
        click.echo(f"Error preprocessing data: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
@click.option("--skip-visualization", 
              is_flag=True,
              help="Skip visualization steps")
def run(model, config, param, input, temperature, mode, skip_visualization):
    """
    Run the complete pipeline (train, evaluate, analyze).
    
    Examples:
        flexseq run
        flexseq run --model random_forest
        flexseq run --temperature 320
        flexseq run --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Determine which models to use
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # Create pipeline and run
    pipeline = Pipeline(cfg)
    
    try:
        results = pipeline.run_pipeline(
            model_list, input, skip_visualization
        )
        
        click.echo("\nPipeline completed successfully!")
        click.echo(f"Results saved to {output_dir}")
        
    except Exception as e:
        click.echo(f"Error running pipeline: {e}")
        sys.exit(1)

@cli.command()
def list_models():
    """
    List available models in the registry.
    
    Examples:
        flexseq list-models
    """
    from flexseq.models import get_available_models
    
    models = get_available_models()
    
    click.echo("Available models:")
    for model in models:
        click.echo(f"  - {model}")

@cli.command()
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
def list_temperatures(config):
    """
    List available temperatures in the configuration.
    
    Examples:
        flexseq list-temperatures
    """
    # Load configuration
    cfg = load_config(config)
    
    temperatures = get_available_temperatures(cfg)
    
    click.echo("Available temperatures:")
    for temp in temperatures:
        click.echo(f"  - {temp}")

if __name__ == "__main__":
    cli()
### Model Files ###
---------------------------------------------------------
===== FILE: flexseq/models/__init__.py =====
"""
Model registry for the FlexSeq ML pipeline.

This module provides a registry for model classes and utility functions
for model discovery and management.
"""

from importlib import import_module
from pathlib import Path
from typing import Dict, Type, List, Optional

from .base import BaseModel

# Global model registry
MODEL_REGISTRY: Dict[str, Type[BaseModel]] = {}

def register_model(name: str):
    """
    Decorator to register a model class in the global registry.
    
    Args:
        name: Name to register the model under
        
    Returns:
        Decorator function
    """
    def decorator(cls):
        MODEL_REGISTRY[name] = cls
        return cls
    return decorator

def get_model_class(model_name: str) -> Type[BaseModel]:
    """
    Get a model class by name.
    
    Args:
        model_name: Name of the model to get
        
    Returns:
        Model class
        
    Raises:
        ValueError: If model_name is not found in registry
    """
    if model_name not in MODEL_REGISTRY:
        raise ValueError(f"Model '{model_name}' not found in registry. " 
                         f"Available models: {', '.join(MODEL_REGISTRY.keys())}")
    
    return MODEL_REGISTRY[model_name]

def get_available_models() -> List[str]:
    """
    Get list of available model names.
    
    Returns:
        List of registered model names
    """
    return list(MODEL_REGISTRY.keys())

# Auto-discover models using importlib
models_dir = Path(__file__).parent
for model_file in models_dir.glob('*.py'):
    if model_file.stem not in ('__init__', 'base'):
        try:
            import_module(f'flexseq.models.{model_file.stem}')
        except ImportError as e:
            import logging
            logging.getLogger(__name__).warning(f"Error importing model module {model_file.stem}: {e}")
===== FILE: flexseq/models/base.py =====
"""
Base model class for FlexSeq ML pipeline.

This module defines the BaseModel abstract class that all
protein flexibility prediction models must implement.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple

import numpy as np
import pandas as pd

class BaseModel(ABC):
    """
    Base class for all FlexSeq ML models.
    All models must implement these methods.
    """
    
    @abstractmethod
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]) -> 'BaseModel':
        """
        Train the model on input data.
        
        Args:
            X: Feature matrix
            y: Target values
            
        Returns:
            Self, for method chaining
        """
        pass
    
    @abstractmethod
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate predictions for input data.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predictions
        """
        pass
    
    @abstractmethod
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        pass
    
    @classmethod
    @abstractmethod
    def load(cls, path: str) -> 'BaseModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded model instance
        """
        pass
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        # Default implementation returns predictions with zeros for std dev
        # Models that support uncertainty should override this
        predictions = self.predict(X)
        std_deviation = np.zeros_like(predictions)
        return predictions, std_deviation
    
    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """
        Get feature importance values if available.
        
        Returns:
            Dictionary mapping feature names to importance values,
            or None if feature importance is not available
        """
        return None
    
    def get_params(self) -> Dict[str, Any]:
        """
        Get model parameters.
        
        Returns:
            Dictionary of model parameters
        """
        return {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('_') and key != 'model'
        }
    
    def get_model_name(self) -> str:
        """
        Get model name.
        
        Returns:
            String representing model name
        """
        return self.__class__.__name__
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
            
        Raises:
            NotImplementedError: If the model doesn't support hyperparameter optimization
        """
        raise NotImplementedError("This model doesn't support hyperparameter optimization")
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return None
===== FILE: flexseq/models/random_forest.py =====
"""
Random Forest model implementation for the FlexSeq ML pipeline.

This module provides a RandomForestModel for protein flexibility prediction
with support for uncertainty estimation and hyperparameter optimization.
"""

import os
import logging
from typing import Dict, Any, Optional, Union, List, Tuple

import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

from flexseq.models import register_model
from flexseq.models.base import BaseModel
from flexseq.utils.helpers import ProgressCallback

logger = logging.getLogger(__name__)

@register_model("random_forest")
class RandomForestModel(BaseModel):
    """
    Random Forest model for protein flexibility prediction.
    
    This model uses an ensemble of decision trees to capture non-linear
    relationships between protein features and flexibility.
    """
    
    def __init__(
        self, 
        n_estimators: int = 100, 
        max_depth: Optional[int] = None,
        min_samples_split: int = 2,
        min_samples_leaf: int = 1,
        max_features: Union[str, float, int] = 0.7,
        bootstrap: bool = True,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Random Forest model.
        
        Args:
            n_estimators: Number of trees in the forest
            max_depth: Maximum depth of each tree (None = unlimited)
            min_samples_split: Minimum samples required to split an internal node
            min_samples_leaf: Minimum samples required to be at a leaf node
            max_features: Number of features to consider for best split
            bootstrap: Whether to use bootstrap samples
            random_state: Random seed for reproducibility
            **kwargs: Additional parameters passed to RandomForestRegressor
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.random_state = random_state
        self.model_params = kwargs
        self.model = None
        self.feature_names_ = None
        self.best_params_ = None
        
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'RandomForestModel':
        """
        Train the Random Forest model.
        
        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names
            
        Returns:
            Self, for method chaining
        """
        # Store feature names if available
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        elif feature_names is not None:
            self.feature_names_ = feature_names
        
        # Check if randomized search is enabled in the model parameters
        use_randomized_search = self.model_params.pop('use_randomized_search', False)
        
        if use_randomized_search:
            # Get hyperparameter search config
            n_iter = self.model_params.pop('n_iter', 20)
            cv = self.model_params.pop('cv', 3)
            param_distributions = self.model_params.pop('param_distributions', None)
            
            # Use default param distributions if not provided
            if param_distributions is None:
                param_distributions = {
                    'n_estimators': [50, 100, 200, 300],
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['auto', 'sqrt', 'log2', 0.7],
                    'bootstrap': [True, False]
                }
            
            # Base estimator with fixed random_state
            base_rf = RandomForestRegressor(random_state=self.random_state, **self.model_params)
            
            with ProgressCallback(total=1, desc="Setting up RandomizedSearchCV") as pbar:
                logger.info(f"Setting up RandomizedSearchCV with {n_iter} iterations and {cv} folds")
                
                # Create the randomized search
                search = RandomizedSearchCV(
                    base_rf,
                    param_distributions=param_distributions,
                    n_iter=n_iter,
                    cv=cv,
                    scoring='neg_mean_squared_error',
                    n_jobs=-1,
                    random_state=self.random_state,
                    verbose=0,
                    return_train_score=True
                )
                pbar.update()
            
            # Fit the randomized search
            with ProgressCallback(total=1, desc="Training with RandomizedSearchCV") as pbar:
                search.fit(X, y)
                self.model = search.best_estimator_
                self.best_params_ = search.best_params_
                pbar.update()
                
            logger.info(f"Best hyperparameters: {self.best_params_}")
        else:
            # Create and train a standard RandomForestRegressor
            with ProgressCallback(total=1, desc="Training Random Forest") as pbar:
                self.model = RandomForestRegressor(
                    n_estimators=self.n_estimators,
                    max_depth=self.max_depth,
                    min_samples_split=self.min_samples_split,
                    min_samples_leaf=self.min_samples_leaf,
                    max_features=self.max_features,
                    bootstrap=self.bootstrap,
                    random_state=self.random_state,
                    n_jobs=-1,
                    **self.model_params
                )
                
                self.model.fit(X, y)
                pbar.update()
        
        return self
        
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate RMSF predictions.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predicted RMSF values
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        return self.model.predict(X)
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate RMSF predictions with standard deviation (uncertainty).
        
        Uses the variance of predictions across the ensemble of trees
        as a measure of prediction uncertainty.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_dev) arrays
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Make predictions from all trees
        predictions = np.array([tree.predict(X) for tree in self.model.estimators_])
        
        # Calculate mean and standard deviation
        mean_prediction = np.mean(predictions, axis=0)
        std_prediction = np.std(predictions, axis=0)
        
        return mean_prediction, std_prediction
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Random Forest ignores the method and n_trials parameters, using RandomizedSearchCV instead
        if method != "random":
            logger.warning(f"RandomForest only supports 'random' method for optimization, ignoring '{method}'")
            
        with ProgressCallback(total=1, desc="Hyperparameter optimization") as pbar:
            search = RandomizedSearchCV(
                RandomForestRegressor(random_state=self.random_state),
                param_distributions=param_grid,
                n_iter=n_trials,
                cv=cv,
                scoring='neg_mean_squared_error',
                n_jobs=-1,
                random_state=self.random_state,
                verbose=0,
                return_train_score=True
            )
            
            search.fit(X, y)
            pbar.update()
            
        # Update model with the best estimator
        self.model = search.best_estimator_
        self.best_params_ = search.best_params_
        
        logger.info(f"Best hyperparameters: {self.best_params_}")
        
        return self.best_params_
        
    def save(self, path: str) -> None:
        """
        Save model to disk using joblib.
        
        Args:
            path: Path to save location
        """
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
            
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save model state
        state = {
            'model': self.model,
            'feature_names': self.feature_names_,
            'best_params': self.best_params_,
            'params': {
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth,
                'min_samples_split': self.min_samples_split,
                'min_samples_leaf': self.min_samples_leaf,
                'max_features': self.max_features,
                'bootstrap': self.bootstrap,
                'random_state': self.random_state,
                'model_params': self.model_params
            }
        }
        
        joblib.dump(state, path)
        logger.info(f"Model saved to {path}")
        
    @classmethod
    def load(cls, path: str) -> 'RandomForestModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded RandomForestModel instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")
        
        try:
            state = joblib.load(path)
            
            # Create new instance with saved parameters
            params = state['params']
            instance = cls(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', None),
                min_samples_split=params.get('min_samples_split', 2),
                min_samples_leaf=params.get('min_samples_leaf', 1),
                max_features=params.get('max_features', 0.7),
                bootstrap=params.get('bootstrap', True),
                random_state=params.get('random_state', 42),
                **params.get('model_params', {})
            )
            
            # Restore model and feature names
            instance.model = state['model']
            instance.feature_names_ = state.get('feature_names', None)
            instance.best_params_ = state.get('best_params', None)
            
            return instance
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
        
    def get_feature_importance(self, X_val=None, y_val=None) -> Dict[str, float]:
        """
        Get feature importance values using permutation importance.
        
        Args:
            X_val: Optional validation features for permutation importance
            y_val: Optional validation targets for permutation importance
            
        Returns:
            Dictionary mapping feature names to importance values
        """
        if self.model is None:
            return {}
        
        # If validation data is provided, use permutation importance
        if X_val is not None and y_val is not None and len(X_val) > 0:
            try:
                from sklearn.inspection import permutation_importance
                
                # Calculate permutation importance
                r = permutation_importance(
                    self.model, X_val, y_val, 
                    n_repeats=10, 
                    random_state=self.random_state
                )
                
                # Use mean importance as the feature importance
                importance_values = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                    return dict(zip(self.feature_names_, importance_values))
                else:
                    return {f"feature_{i}": imp for i, imp in enumerate(importance_values)}
                    
            except Exception as e:
                logger.warning(f"Could not compute permutation importance: {e}")
                # Fall back to built-in feature importance
        
        # Use built-in feature importance as fallback
        if hasattr(self.model, 'feature_importances_'):
            importance_values = self.model.feature_importances_
            
            if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                return dict(zip(self.feature_names_, importance_values))
            else:
                return {f"feature_{i}": importance for i, importance in enumerate(importance_values)}
        
        return {}
===== FILE: flexseq/models/neural_network.py =====
"""
Neural Network model implementation for the FlexSeq ML pipeline.

This module provides a PyTorch-based neural network model
for protein flexibility prediction, with support for
hyperparameter optimization.
"""

import os
import logging
import json
from typing import Dict, Any, Optional, Union, List, Tuple

import numpy as np
import pandas as pd
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import KFold

from flexseq.models import register_model
from flexseq.models.base import BaseModel
from flexseq.utils.helpers import ProgressCallback, progress_bar

logger = logging.getLogger(__name__)

class FlexibilityNN(nn.Module):
    """
    Neural network architecture for protein flexibility prediction.
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_layers: List[int] = [64, 32],
        activation: str = "relu",
        dropout: float = 0.2
    ):
        """
        Initialize the neural network architecture.
        
        Args:
            input_dim: Number of input features
            hidden_layers: List of hidden layer sizes
            activation: Activation function to use
            dropout: Dropout rate
        """
        super(FlexibilityNN, self).__init__()
        
        # Define activation function
        if activation.lower() == "relu":
            act_fn = nn.ReLU()
        elif activation.lower() == "leaky_relu":
            act_fn = nn.LeakyReLU()
        elif activation.lower() == "tanh":
            act_fn = nn.Tanh()
        elif activation.lower() == "sigmoid":
            act_fn = nn.Sigmoid()
        else:
            act_fn = nn.ReLU()
            logger.warning(f"Unknown activation function '{activation}', using ReLU")
        
        # Create layers
        layers = []
        prev_dim = input_dim
        
        # Hidden layers
        for i, dim in enumerate(hidden_layers):
            layers.append(nn.Linear(prev_dim, dim))
            layers.append(act_fn)
            layers.append(nn.Dropout(dropout))
            prev_dim = dim
        
        # Output layer (single value for RMSF prediction)
        layers.append(nn.Linear(prev_dim, 1))
        
        # Create sequential model
        self.model = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the network.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor
        """
        return self.model(x).squeeze()

@register_model("neural_network")
class NeuralNetworkModel(BaseModel):
    """
    Neural Network model for protein flexibility prediction.
    
    This model uses a feed-forward neural network to learn complex
    relationships between protein features and flexibility.
    """
    
    def __init__(
        self,
        architecture: Dict[str, Any] = None,
        training: Dict[str, Any] = None,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Neural Network model.
        
        Args:
            architecture: Dictionary of architecture parameters
            training: Dictionary of training parameters
            random_state: Random seed for reproducibility
            **kwargs: Additional parameters
        """
        # Set default architecture if not provided
        if architecture is None:
            architecture = {
                "hidden_layers": [64, 32],
                "activation": "relu",
                "dropout": 0.2
            }
        
        # Set default training parameters if not provided
        if training is None:
            training = {
                "optimizer": "adam",
                "learning_rate": 0.001,
                "batch_size": 32,
                "epochs": 100,
                "early_stopping": True,
                "patience": 10
            }
        
        self.architecture = architecture
        self.training = training
        self.random_state = random_state
        self.model = None
        self.feature_names_ = None
        self.scaler = None
        self.history = None
        
        # Set random seeds for reproducibility
        torch.manual_seed(random_state)
        np.random.seed(random_state)
        
        # Device configuration
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
    
    def _create_model(self, input_dim: int) -> FlexibilityNN:
        """
        Create the neural network model.
        
        Args:
            input_dim: Number of input features
            
        Returns:
            Initialized FlexibilityNN model
        """
        model = FlexibilityNN(
            input_dim=input_dim,
            hidden_layers=self.architecture.get("hidden_layers", [64, 32]),
            activation=self.architecture.get("activation", "relu"),
            dropout=self.architecture.get("dropout", 0.2)
        )
        return model.to(self.device)
    
    def _get_optimizer(self, model: FlexibilityNN) -> optim.Optimizer:
        """
        Get the appropriate optimizer.
        
        Args:
            model: The neural network model
            
        Returns:
            Configured optimizer
        """
        optimizer_name = self.training.get("optimizer", "adam").lower()
        lr = self.training.get("learning_rate", 0.001)
        
        if optimizer_name == "adam":
            return optim.Adam(model.parameters(), lr=lr)
        elif optimizer_name == "sgd":
            return optim.SGD(model.parameters(), lr=lr)
        elif optimizer_name == "rmsprop":
            return optim.RMSprop(model.parameters(), lr=lr)
        else:
            logger.warning(f"Unknown optimizer '{optimizer_name}', using Adam")
            return optim.Adam(model.parameters(), lr=lr)
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'NeuralNetworkModel':
        """
        Train the Neural Network model.
        
        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names
            
        Returns:
            Self, for method chaining
        """
        # Store feature names if available
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        elif feature_names is not None:
            self.feature_names_ = feature_names
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Convert target to numpy array if needed
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Scale features using StandardScaler
        from sklearn.preprocessing import StandardScaler
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_array)
        
        # Create PyTorch tensors
        X_tensor = torch.FloatTensor(X_scaled)
        y_tensor = torch.FloatTensor(y_array)
        
        # Create validation split (20% of training data)
        from sklearn.model_selection import train_test_split
        train_indices, val_indices = train_test_split(
            np.arange(len(X_scaled)), 
            test_size=0.2, 
            random_state=self.random_state
        )
        
        # Create dataset and dataloader for training
        train_dataset = TensorDataset(X_tensor[train_indices], y_tensor[train_indices])
        batch_size = self.training.get("batch_size", 32)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # Initialize model
        input_dim = X_array.shape[1]
        self.model = self._create_model(input_dim)
        
        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = self._get_optimizer(self.model)
        
        # Training parameters
        epochs = self.training.get("epochs", 100)
        early_stopping = self.training.get("early_stopping", True)
        patience = self.training.get("patience", 10)
        
        # Initialize training history
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_r2': [],
            'val_r2': [],
            'learning_rate': []
        }
        
        # Training loop
        best_loss = float('inf')
        patience_counter = 0
        
        # Use progress bar for epochs
        epoch_pbar = progress_bar(range(epochs), desc="Training NN")
        
        for epoch in epoch_pbar:
            # Training phase
            self.model.train()
            train_running_loss = 0.0
            train_preds = []
            train_targets = []
            
            # Track batch progress
            batch_pbar = progress_bar(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)
            
            for batch_X, batch_y in batch_pbar:
                # Move tensors to the configured device
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                # Forward pass
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                
                # Backward pass and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                train_running_loss += loss.item() * batch_X.size(0)
                
                # Collect predictions and targets for R² calculation
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())
                
                # Update batch progress bar
                batch_pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            # Compute average epoch loss for training
            train_epoch_loss = train_running_loss / len(train_dataset)
            
            # Concatenate predictions and targets for full training set R²
            train_preds = np.concatenate(train_preds)
            train_targets = np.concatenate(train_targets)
            
            # Calculate R² for training set
            from sklearn.metrics import r2_score
            train_r2 = r2_score(train_targets, train_preds)
            
            # Validation phase
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                val_X = X_tensor[val_indices].to(self.device)
                val_y = y_tensor[val_indices].to(self.device)
                val_outputs = self.model(val_X)
                val_batch_loss = criterion(val_outputs, val_y)
                val_loss = val_batch_loss.item()
                
                # Calculate R² for validation set
                val_preds = val_outputs.cpu().numpy()
                val_targets = val_y.cpu().numpy()
                val_r2 = r2_score(val_targets, val_preds)
            
            # Store metrics in history
            self.history['train_loss'].append(train_epoch_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_r2'].append(train_r2)
            self.history['val_r2'].append(val_r2)
            self.history['learning_rate'].append(self.training.get("learning_rate", 0.001))
            
            # Update epoch progress bar
            epoch_pbar.set_postfix(
                train_loss=f"{train_epoch_loss:.4f}",
                val_loss=f"{val_loss:.4f}",
                val_r2=f"{val_r2:.4f}"
            )
            
            # Early stopping check
            if early_stopping:
                if val_loss < best_loss:
                    best_loss = val_loss
                    patience_counter = 0
                    # Save best model state
                    best_state = {
                        'model_state': self.model.state_dict(),
                        'input_dim': input_dim
                    }
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logger.info(f"Early stopping at epoch {epoch+1}")
                        # Restore best model state
                        self.model.load_state_dict(best_state['model_state'])
                        break
        
        return self
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate RMSF predictions.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predicted RMSF values
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to evaluation mode
        self.model.eval()
        
        # Make predictions
        with torch.no_grad():
            predictions = self.model(X_tensor).cpu().numpy()
        
        return predictions
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates using MC Dropout.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to training mode to enable dropout for MC Dropout
        self.model.train()
        
        # Perform multiple forward passes with dropout enabled
        n_samples = 30  # Number of MC samples
        samples = []
        
        with torch.no_grad():  # No gradients needed
            for _ in range(n_samples):
                predictions = self.model(X_tensor).cpu().numpy()
                samples.append(predictions)
        
        # Calculate mean and standard deviation across samples
        samples = np.stack(samples, axis=0)
        mean_prediction = np.mean(samples, axis=0)
        std_prediction = np.std(samples, axis=0)
        
        return mean_prediction, std_prediction
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        if method.lower() == "bayesian":
            # Try to use optuna for Bayesian optimization
            try:
                import optuna
                logger.info("Using Optuna for Bayesian hyperparameter optimization")
                return self._bayesian_optimization(X, y, param_grid, n_trials, cv)
            except ImportError:
                logger.warning("Optuna not available, falling back to random search")
                method = "random"
        
        if method.lower() == "random":
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        elif method.lower() == "grid":
            return self._grid_optimization(X, y, param_grid, cv)
        
        else:
            logger.warning(f"Unknown optimization method '{method}', using random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _random_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform random search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Create parameter combinations
        from itertools import product
        import random
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        # Limit to n_trials
        if len(param_combinations) > n_trials:
            random.seed(self.random_state)
            param_combinations = random.sample(param_combinations, n_trials)
            
        logger.info(f"Performing random search with {len(param_combinations)} parameter combinations")
        
        # Train and evaluate each combination
        results = []
        
        for i, params in enumerate(progress_bar(param_combinations, desc="Parameter combinations")):
            # Extract architecture and training params
            arch_params = {
                'hidden_layers': params.get('hidden_layers', self.architecture.get('hidden_layers', [64, 32])),
                'activation': params.get('activation', self.architecture.get('activation', 'relu')),
                'dropout': params.get('dropout', self.architecture.get('dropout', 0.2))
            }
            
            train_params = {
                'optimizer': params.get('optimizer', self.training.get('optimizer', 'adam')),
                'learning_rate': params.get('learning_rate', self.training.get('learning_rate', 0.001)),
                'batch_size': params.get('batch_size', self.training.get('batch_size', 32)),
                'epochs': params.get('epochs', self.training.get('epochs', 100)),
                'early_stopping': params.get('early_stopping', self.training.get('early_stopping', True)),
                'patience': params.get('patience', self.training.get('patience', 10))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=arch_params,
                    training=train_params,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            # Store results
            mean_score = np.mean(cv_scores)
            results.append((mean_score, arch_params, train_params))
            logger.debug(f"Parameters: {params}, Score: {mean_score:.4f}")
        
        # Find best parameters
        results.sort(reverse=True)  # Higher score is better
        best_score, best_arch, best_train = results[0]
        
        logger.info(f"Best score: {best_score:.4f}")
        logger.info(f"Best architecture parameters: {best_arch}")
        logger.info(f"Best training parameters: {best_train}")
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def _grid_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform grid search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Grid search is same as random search but with all combinations
        from itertools import product
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        logger.info(f"Performing grid search with {len(param_combinations)} parameter combinations")
        
        # Use random optimization with all combinations
        n_trials = len(param_combinations)
        return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _bayesian_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform Bayesian hyperparameter optimization using Optuna.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        try:
            import optuna
        except ImportError:
            logger.warning("Optuna not available, falling back to random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Define objective function for Optuna
        def objective(trial):
            # Suggest hyperparameters
            architecture = {
                'hidden_layers': trial.suggest_categorical('hidden_layers', param_grid.get(
                    'hidden_layers', [[64, 32], [128, 64], [32, 16]])),
                'activation': trial.suggest_categorical('activation', param_grid.get(
                    'activation', ['relu', 'leaky_relu'])),
                'dropout': trial.suggest_float('dropout', *param_grid.get(
                    'dropout', [0.1, 0.5]), step=0.1)
            }
            
            training = {
                'optimizer': trial.suggest_categorical('optimizer', param_grid.get(
                    'optimizer', ['adam', 'rmsprop'])),
                'learning_rate': trial.suggest_float('learning_rate', *param_grid.get(
                    'learning_rate', [0.0001, 0.01]), log=True),
                'batch_size': trial.suggest_categorical('batch_size', param_grid.get(
                    'batch_size', [16, 32, 64])),
                'early_stopping': True,
                'patience': trial.suggest_int('patience', *param_grid.get(
                    'patience', [5, 15])),
                'epochs': trial.suggest_int('epochs', *param_grid.get(
                    'epochs', [50, 100]))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=architecture,
                    training=training,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            return np.mean(cv_scores)
        
        # Create and run Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        # Get best parameters
        best_params = study.best_params
        logger.info(f"Best score: {study.best_value:.4f}")
        logger.info(f"Best parameters: {best_params}")
        
        # Construct best parameters dictionary
        best_arch = {
            'hidden_layers': best_params.get('hidden_layers', self.architecture.get('hidden_layers', [64, 32])),
            'activation': best_params.get('activation', self.architecture.get('activation', 'relu')),
            'dropout': best_params.get('dropout', self.architecture.get('dropout', 0.2))
        }
        
        best_train = {
            'optimizer': best_params.get('optimizer', self.training.get('optimizer', 'adam')),
            'learning_rate': best_params.get('learning_rate', self.training.get('learning_rate', 0.001)),
            'batch_size': best_params.get('batch_size', self.training.get('batch_size', 32)),
            'early_stopping': True,
            'patience': best_params.get('patience', self.training.get('patience', 10)),
            'epochs': best_params.get('epochs', self.training.get('epochs', 100))
        }
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save model state
        state = {
            'model_state': self.model.state_dict(),
            'architecture': self.architecture,
            'training': self.training,
            'random_state': self.random_state,
            'feature_names': self.feature_names_,
            'scaler': self.scaler,
            'history': self.history,
            'input_dim': self.model.model[0].in_features  # Get input dimension from first layer
        }
        
        # Save state
        torch.save(state, path)
        logger.info(f"Model saved to {path}")
        
        # Save training history separately as CSV
        if self.history:
            history_path = os.path.splitext(path)[0] + "_history.csv"
            history_df = pd.DataFrame(self.history)
            history_df.to_csv(history_path, index=False)
            logger.info(f"Training history saved to {history_path}")
    
    @classmethod
    def load(cls, path: str) -> 'NeuralNetworkModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded NeuralNetworkModel instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")
        
        try:
            # Load state dictionary
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            state = torch.load(path, map_location=device)
            
            # Create new instance with saved parameters
            instance = cls(
                architecture=state['architecture'],
                training=state['training'],
                random_state=state['random_state']
            )
            
            # Restore feature names, scaler, and history
            instance.feature_names_ = state['feature_names']
            instance.scaler = state['scaler']
            instance.history = state.get('history')
            
            # Create and restore model
            input_dim = state['input_dim']
            instance.model = instance._create_model(input_dim)
            instance.model.load_state_dict(state['model_state'])
            instance.model.eval()
            
            # Load training history if available
            history_path = os.path.splitext(path)[0] + "_history.csv"
            if os.path.exists(history_path):
                try:
                    history_df = pd.read_csv(history_path)
                    instance.history = {col: history_df[col].tolist() for col in history_df.columns}
                except Exception as e:
                    logger.warning(f"Could not load training history: {e}")
            
            return instance
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def get_feature_importance(self, X_val=None, y_val=None) -> Optional[Dict[str, float]]:
        """
        Get feature importance values using permutation importance.
        
        Args:
            X_val: Optional validation features for permutation importance
            y_val: Optional validation targets for permutation importance
            
        Returns:
            Dictionary mapping feature names to importance values or None
        """
        if self.model is None:
            return None
        
        # If validation data is provided, use permutation importance
        if X_val is not None and y_val is not None and len(X_val) > 0:
            try:
                from sklearn.inspection import permutation_importance
                
                # Set model to evaluation mode
                self.model.eval()
                
                # Define a prediction function for permutation importance
                def predict_fn(X_test):
                    X_tensor = torch.FloatTensor(self.scaler.transform(X_test)).to(self.device)
                    with torch.no_grad():
                        return self.model(X_tensor).cpu().numpy()
                
                # Calculate permutation importance
                r = permutation_importance(
                    predict_fn, X_val, y_val, 
                    n_repeats=10, 
                    random_state=self.random_state
                )
                
                # Use mean importance as the feature importance
                importance_values = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                    return dict(zip(self.feature_names_, importance_values))
                else:
                    return {f"feature_{i}": imp for i, imp in enumerate(importance_values)}
                    
            except Exception as e:
                logger.warning(f"Could not compute permutation importance: {e}")
                # Fall back to weight-based importance
        
        # Fall back to weight-based importance
        try:
            # Get weights from the first layer
            first_layer = self.model.model[0]
            weights = first_layer.weight.data.cpu().numpy()
            
            # Use absolute values of weights as importance
            importance = np.mean(np.abs(weights), axis=0)
            
            # Map to feature names if available
            if self.feature_names_ is not None and len(self.feature_names_) == len(importance):
                return dict(zip(self.feature_names_, importance))
            else:
                return {f"feature_{i}": imp for i, imp in enumerate(importance)}
                
        except Exception as e:
            logger.warning(f"Could not compute feature importance: {e}")
            return None
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return self.history
### Data Handling Files ###
---------------------------------------------------------
===== FILE: flexseq/data/__init__.py =====
"""
Data handling modules for the FlexSeq ML pipeline.

This package contains functions for loading, processing, and
manipulating protein data across multiple temperatures.
"""

# Import key functions for easier access
from flexseq.data.loader import load_file, list_data_files, get_temperature_files
from flexseq.data.processor import (
    load_and_process_data, 
    clean_data, 
    prepare_data_for_model
)
===== FILE: flexseq/data/loader.py =====
"""
Data loading utilities for the FlexSeq ML pipeline.

This module provides functions for loading protein data from various formats,
with special support for temperature-specific files.
"""

import os
import logging
import re
from typing import List, Dict, Any, Optional, Union, Tuple
from functools import lru_cache

import pandas as pd
import numpy as np
import glob


logger = logging.getLogger(__name__)

def list_data_files(data_dir: str, file_pattern: str) -> List[str]:
    """
    List data files matching a pattern in a directory.
    
    Args:
        data_dir: Directory to search
        file_pattern: File pattern to match
        
    Returns:
        List of file paths
    """
    
    # Get absolute path
    data_dir = os.path.abspath(data_dir)
    
    # Find matching files
    pattern = os.path.join(data_dir, file_pattern)
    matching_files = glob.glob(pattern)
    
    if not matching_files:
        logger.warning(f"No files found matching pattern {pattern}")
    
    return matching_files

def get_temperature_files(data_dir: str, file_pattern: str = "temperature_*.csv") -> Dict[Union[int, str], str]:
    """
    Get a dictionary mapping temperature values to file paths.
    
    Args:
        data_dir: Directory to search
        file_pattern: File pattern to match
        
    Returns:
        Dictionary mapping temperature values to file paths
    """
    # Get all matching files
    files = list_data_files(data_dir, file_pattern)
    
    # Extract temperatures from filenames
    temperature_files = {}
    
    for file_path in files:
        filename = os.path.basename(file_path)
        
        # Try to extract temperature from filename
        # Pattern: temperature_{temp}_train.csv
        match = re.match(r"temperature_(\d+|average)_.*\.csv", filename)
        
        if match:
            temp_str = match.group(1)
            
            # Convert to int if numeric, keep as string for "average"
            temperature = int(temp_str) if temp_str.isdigit() else temp_str
            
            temperature_files[temperature] = file_path
    
    if not temperature_files:
        logger.warning(f"No temperature files found in {data_dir}")
    
    return temperature_files

def detect_file_format(file_path: str) -> str:
    """
    Detect file format based on extension and content.
    
    Args:
        file_path: Path to data file
        
    Returns:
        Format string ('csv', 'tsv', 'pickle', etc.)
    """
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    
    if ext == '.csv':
        return 'csv'
    elif ext == '.tsv':
        return 'tsv'
    elif ext in ['.pkl', '.pickle']:
        return 'pickle'
    elif ext == '.json':
        return 'json'
    elif ext == '.parquet':
        return 'parquet'
    elif ext == '.h5':
        return 'hdf5'
    else:
        # Try to detect CSV/TSV by reading first line
        try:
            with open(file_path, 'r') as f:
                first_line = f.readline()
                if '\t' in first_line:
                    return 'tsv'
                elif ',' in first_line:
                    return 'csv'
        except:
            pass
        
        # Default to CSV if can't determine
        logger.warning(f"Could not determine format for {file_path}, defaulting to CSV")
        return 'csv'

@lru_cache(maxsize=16)
def load_file(file_path: str, **kwargs) -> pd.DataFrame:
    """
    Load data from a file with format auto-detection.
    
    Args:
        file_path: Path to data file
        **kwargs: Additional arguments to pass to pandas
        
    Returns:
        Loaded DataFrame
        
    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file format is not supported or loading fails
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found: {file_path}")
    
    # Detect format
    file_format = detect_file_format(file_path)
    
    try:
        # Load based on format
        if file_format == 'csv':
            return pd.read_csv(file_path, **kwargs)
        elif file_format == 'tsv':
            return pd.read_csv(file_path, sep='\t', **kwargs)
        elif file_format == 'pickle':
            return pd.read_pickle(file_path, **kwargs)
        elif file_format == 'json':
            return pd.read_json(file_path, **kwargs)
        elif file_format == 'parquet':
            return pd.read_parquet(file_path, **kwargs)
        elif file_format == 'hdf5':
            return pd.read_hdf(file_path, **kwargs)
        else:
            raise ValueError(f"Unsupported file format: {file_format}")
    except Exception as e:
        logger.error(f"Error loading file {file_path}: {e}")
        raise ValueError(f"Failed to load file {file_path}: {e}")

def merge_data_files(file_paths: List[str], **kwargs) -> pd.DataFrame:
    """
    Merge multiple data files into a single DataFrame.
    
    Args:
        file_paths: List of paths to data files
        **kwargs: Additional arguments to pass to pandas
        
    Returns:
        Merged DataFrame
    """
    if not file_paths:
        raise ValueError("No files provided for merging")
    
    # Load and concatenate files
    dfs = []
    
    for file_path in file_paths:
        try:
            df = load_file(file_path, **kwargs)
            dfs.append(df)
        except Exception as e:
            logger.warning(f"Skipping file {file_path} due to error: {e}")
    
    if not dfs:
        raise ValueError("No data files could be loaded")
    
    return pd.concat(dfs, ignore_index=True)

def validate_data_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
    """
    Validate that DataFrame contains all required columns.
    
    Args:
        df: DataFrame to validate
        required_columns: List of required column names
        
    Returns:
        True if all required columns are present, False otherwise
    """
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.warning(f"Missing required columns: {missing_columns}")
        return False
    
    return True

def load_temperature_data(
    config: Dict[str, Any],
    temperature: Optional[Union[int, str]] = None
) -> pd.DataFrame:
    """
    Load data for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Optional temperature to load (if None, use config["temperature"]["current"])
        
    Returns:
        DataFrame for the specified temperature
        
    Raises:
        FileNotFoundError: If data file doesn't exist
        ValueError: If temperature is not in available temperatures
    """
    # Get temperature from config if not provided
    if temperature is None:
        temperature = config["temperature"]["current"]
    
    # Validate temperature
    available_temps = config["temperature"]["available"]
    if str(temperature) not in [str(t) for t in available_temps]:
        raise ValueError(f"Temperature {temperature} is not in the list of available temperatures: {available_temps}")
    
    # Get data directory and file pattern
    data_dir = config["paths"]["data_dir"]
    file_pattern = config["dataset"]["file_pattern"]
    
    # Replace {temperature} placeholder in file pattern
    file_pattern = file_pattern.replace("{temperature}", str(temperature))
    
    # Find matching file
    matching_files = list_data_files(data_dir, file_pattern)
    
    if not matching_files:
        raise FileNotFoundError(f"No data file found for temperature {temperature} with pattern {file_pattern}")
    
    # Use the first matching file
    file_path = matching_files[0]
    
    # Load data
    df = load_file(file_path)
    
    return df

def load_all_temperature_data(config: Dict[str, Any]) -> Dict[Union[int, str], pd.DataFrame]:
    """
    Load data for all available temperatures.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Dictionary mapping temperature values to DataFrames
    """
    # Get available temperatures
    available_temps = config["temperature"]["available"]
    
    # Load data for each temperature
    temperature_data = {}
    
    for temp in available_temps:
        try:
            df = load_temperature_data(config, temp)
            temperature_data[temp] = df
        except Exception as e:
            logger.warning(f"Error loading data for temperature {temp}: {e}")
    
    if not temperature_data:
        raise ValueError("No temperature data could be loaded")
    
    return temperature_data

def summarize_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Generate summary statistics for a dataset.
    
    Args:
        df: Input DataFrame
        
    Returns:
        Dictionary of summary statistics
    """
    summary = {
        "num_rows": len(df),
        "num_columns": len(df.columns),
        "columns": list(df.columns),
        "memory_usage": None,
        "domains": None,
        "residues_per_domain": None,
        "column_types": {},
        "missing_values": {},
    }
    
    # Memory usage
    try:
        memory_bytes = df.memory_usage(deep=True).sum()
        
        if memory_bytes < 1024:
            summary["memory_usage"] = f"{memory_bytes} bytes"
        elif memory_bytes < 1024**2:
            summary["memory_usage"] = f"{memory_bytes / 1024:.2f} KB"
        elif memory_bytes < 1024**3:
            summary["memory_usage"] = f"{memory_bytes / (1024**2):.2f} MB"
        else:
            summary["memory_usage"] = f"{memory_bytes / (1024**3):.2f} GB"
    except:
        pass
    
    # Domain statistics if domain_id is present
    if "domain_id" in df.columns:
        domains = df["domain_id"].unique()
        summary["domains"] = {
            "count": len(domains),
            "examples": list(domains[:5])
        }
        
        # Residues per domain
        residue_counts = df.groupby("domain_id").size()
        summary["residues_per_domain"] = {
            "min": residue_counts.min(),
            "max": residue_counts.max(),
            "mean": residue_counts.mean(),
            "median": residue_counts.median()
        }
    
    # Column types and missing values
    for col in df.columns:
        summary["column_types"][col] = str(df[col].dtype)
        missing = df[col].isna().sum()
        if missing > 0:
            summary["missing_values"][col] = {
                "count": missing,
                "percentage": (missing / len(df)) * 100
            }
    
    # Check for temperature-specific RMSF columns
    rmsf_columns = [col for col in df.columns if col.startswith("rmsf_")]
    if rmsf_columns:
        summary["rmsf_columns"] = rmsf_columns
    
    # Check for OmniFlex specific columns
    omniflex_columns = ["esm_rmsf", "voxel_rmsf"]
    found_omniflex_columns = [col for col in omniflex_columns if col in df.columns]
    if found_omniflex_columns:
        summary["omniflex_columns"] = found_omniflex_columns
    
    return summary

def log_data_summary(summary: Dict[str, Any]) -> None:
    """
    Log a summary of dataset statistics.
    
    Args:
        summary: Dictionary of summary statistics
    """
    logger.info("=== Dataset Summary ===")
    logger.info(f"Rows: {summary['num_rows']}, Columns: {summary['num_columns']}")
    
    if "memory_usage" in summary and summary["memory_usage"]:
        logger.info(f"Memory usage: {summary['memory_usage']}")
    
    if "domains" in summary and summary["domains"]:
        logger.info(f"Domains: {summary['domains']['count']} unique domains")
        logger.info(f"Examples: {', '.join(summary['domains']['examples'])}")
    
    if "residues_per_domain" in summary and summary["residues_per_domain"]:
        stats = summary["residues_per_domain"]
        logger.info(f"Residues per domain: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")
    
    if "missing_values" in summary and summary["missing_values"]:
        missing = summary["missing_values"]
        if missing:
            logger.info("Columns with missing values:")
            for col, stats in missing.items():
                logger.info(f"  {col}: {stats['count']} missing ({stats['percentage']:.1f}%)")
        else:
            logger.info("No missing values detected")
    
    if "rmsf_columns" in summary:
        logger.info(f"RMSF columns: {', '.join(summary['rmsf_columns'])}")
    
    if "omniflex_columns" in summary:
        logger.info(f"OmniFlex columns: {', '.join(summary['omniflex_columns'])}")
    
    logger.info("========================")
===== FILE: flexseq/data/processor.py =====
"""
Data processing for the FlexSeq ML pipeline.

This module provides functions for preprocessing protein data,
including feature engineering, window-based feature generation,
and handling missing values.
"""

import os
import logging
from functools import lru_cache
from typing import Dict, List, Tuple, Optional, Any, Union, Set

import numpy as np
import pandas as pd

from flexseq.data.loader import load_file, load_temperature_data

logger = logging.getLogger(__name__)

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean input data by handling missing values with appropriate defaults.
    
    Args:
        df: Input DataFrame with protein data
        
    Returns:
        Cleaned DataFrame with no missing values
    """
    cleaned_df = df.copy()
    
    # Fill missing secondary structure with coil (most common)
    if 'dssp' in cleaned_df.columns:
        cleaned_df['dssp'] = cleaned_df['dssp'].fillna('C')
        
    # Fill missing accessibility with moderate value
    if 'relative_accessibility' in cleaned_df.columns:
        cleaned_df['relative_accessibility'] = pd.to_numeric(
            cleaned_df['relative_accessibility'], errors='coerce'
        ).fillna(0.5)
        
    # Fill missing core_exterior with default "core"
    if 'core_exterior' in cleaned_df.columns:
        cleaned_df['core_exterior'] = cleaned_df['core_exterior'].fillna('core')
        
    # Fill missing phi/psi angles with neutral values
    if 'phi' in cleaned_df.columns:
        cleaned_df['phi'] = pd.to_numeric(cleaned_df['phi'], errors='coerce').fillna(0.0)
    if 'psi' in cleaned_df.columns:
        cleaned_df['psi'] = pd.to_numeric(cleaned_df['psi'], errors='coerce').fillna(0.0)
        
    # Add normalized residue position if missing
    if 'normalized_resid' not in cleaned_df.columns and 'resid' in cleaned_df.columns:
        cleaned_df['normalized_resid'] = cleaned_df.groupby('domain_id')['resid'].transform(
            lambda x: (x - x.min()) / max(x.max() - x.min(), 1)
        )
        
    # Add missing encoded features if needed
    if 'resname' in cleaned_df.columns and 'resname_encoded' not in cleaned_df.columns:
        # Simple ordinal encoding for amino acids
        aa_map = {
            'ALA': 1, 'ARG': 2, 'ASN': 3, 'ASP': 4, 'CYS': 5,
            'GLN': 6, 'GLU': 7, 'GLY': 8, 'HSD': 9, 'HSE': 10,
            'HSP': 11, 'HIS': 12, 'ILE': 13, 'LEU': 14, 'LYS': 15,
            'MET': 16, 'PHE': 17, 'SER': 18, 'THR': 19, 'TRP': 20,
            'TYR': 21, 'VAL': 22, 'UNK': 0
        }
        cleaned_df['resname_encoded'] = cleaned_df['resname'].map(
            lambda x: aa_map.get(x, 0) if x else 0
        )
        
    if 'core_exterior' in cleaned_df.columns and 'core_exterior_encoded' not in cleaned_df.columns:
        # Binary encoding for core/exterior
        cleaned_df['core_exterior_encoded'] = cleaned_df['core_exterior'].map(
            lambda x: 0 if x == 'core' else 1
        )
        
    if 'dssp' in cleaned_df.columns and 'secondary_structure_encoded' not in cleaned_df.columns:
        # Encode secondary structure
        ss_map = {
            'H': 0,  # Alpha helix
            'G': 0,  # 3/10 helix (grouped with alpha)
            'I': 0,  # Pi helix (grouped with alpha)
            'E': 1,  # Beta sheet
            'B': 1,  # Beta bridge (grouped with sheet)
            'T': 2,  # Turn
            'S': 2,  # Bend (grouped with turn)
            'C': 2,  # Coil
            '-': 2,  # Unknown (default to coil)
        }
        cleaned_df['secondary_structure_encoded'] = cleaned_df['dssp'].map(
            lambda x: ss_map.get(x, 2) if x else 2
        )
        
    # Add normalized phi/psi angles if needed
    if 'phi' in cleaned_df.columns and 'phi_norm' not in cleaned_df.columns:
        # Normalize angles to [-1, 1] range
        cleaned_df['phi_norm'] = cleaned_df['phi'].map(
            lambda x: (x % 360) / 180 - 1 if pd.notnull(x) else 0
        )
        
    if 'psi' in cleaned_df.columns and 'psi_norm' not in cleaned_df.columns:
        cleaned_df['psi_norm'] = cleaned_df['psi'].map(
            lambda x: (x % 360) / 180 - 1 if pd.notnull(x) else 0
        )
    
    # Handle OmniFlex specific features
    # Fill missing esm_rmsf with mean value
    if 'esm_rmsf' in cleaned_df.columns:
        cleaned_df['esm_rmsf'] = pd.to_numeric(cleaned_df['esm_rmsf'], errors='coerce')
        mean_esm = cleaned_df['esm_rmsf'].mean()
        cleaned_df['esm_rmsf'] = cleaned_df['esm_rmsf'].fillna(mean_esm)
    
    # Fill missing voxel_rmsf with mean value
    if 'voxel_rmsf' in cleaned_df.columns:
        cleaned_df['voxel_rmsf'] = pd.to_numeric(cleaned_df['voxel_rmsf'], errors='coerce')
        mean_voxel = cleaned_df['voxel_rmsf'].mean()
        cleaned_df['voxel_rmsf'] = cleaned_df['voxel_rmsf'].fillna(mean_voxel)
    
    return cleaned_df

def filter_domains(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Filter data based on domain inclusion/exclusion rules.
    
    Args:
        df: Input DataFrame with protein data
        config: Configuration dictionary with domain filtering rules
        
    Returns:
        Filtered DataFrame
    """
    domain_config = config.get("dataset", {}).get("domains", {})
    
    # Start with a copy of the input data
    filtered_df = df.copy()
    
    # Filter by domain inclusion if specified
    include_domains = domain_config.get("include", [])
    if include_domains:
        filtered_df = filtered_df[filtered_df['domain_id'].isin(include_domains)]
        
    # Filter by domain exclusion if specified
    exclude_domains = domain_config.get("exclude", [])
    if exclude_domains:
        filtered_df = filtered_df[~filtered_df['domain_id'].isin(exclude_domains)]
        
    # Filter by protein size if specified
    min_size = domain_config.get("min_protein_size", 0)
    if min_size > 0:
        # Get domain sizes
        domain_sizes = filtered_df.groupby('domain_id').size()
        valid_domains = domain_sizes[domain_sizes >= min_size].index
        filtered_df = filtered_df[filtered_df['domain_id'].isin(valid_domains)]
        
    max_size = domain_config.get("max_protein_size")
    if max_size is not None:
        # Get domain sizes
        domain_sizes = filtered_df.groupby('domain_id').size()
        valid_domains = domain_sizes[domain_sizes <= max_size].index
        filtered_df = filtered_df[filtered_df['domain_id'].isin(valid_domains)]
        
    return filtered_df

def create_window_features(
    df: pd.DataFrame, 
    window_size: int, 
    feature_cols: List[str]
) -> pd.DataFrame:
    """
    Create window-based features for each domain, respecting domain boundaries.
    
    Args:
        df: Input DataFrame with protein data
        window_size: Number of residues on each side to include in window
        feature_cols: List of feature column names to use for window features
        
    Returns:
        DataFrame with added window features
    """
    result_df = df.copy()
    
    # Process each domain separately to respect domain boundaries
    for domain_id, domain_df in df.groupby('domain_id'):
        # Sort by residue ID to ensure correct window creation
        domain_df = domain_df.sort_values('resid')
        
        # Create window features for each feature column
        for feature in feature_cols:
            if feature not in domain_df.columns:
                continue
                
            feature_values = domain_df[feature].values
            
            # Create columns for each window position
            for offset in range(-window_size, window_size + 1):
                if offset == 0:
                    continue  # Skip current position (already exists)
                    
                col_name = f"{feature}_offset_{offset}"
                
                # Create offset values with appropriate padding
                offset_values = np.full_like(feature_values, np.nan)
                
                if offset < 0:
                    # Negative offset (previous residues)
                    offset_values[-offset:] = feature_values[:offset]
                else:
                    # Positive offset (next residues)
                    offset_values[:-offset] = feature_values[offset:]
                    
                # Update the result dataframe
                result_df.loc[domain_df.index, col_name] = offset_values
    
    # Fill missing window values with defaults
    for col in result_df.columns:
        if '_offset_' in col:
            # Extract base feature name
            base_feature = col.split('_offset_')[0]
            
            # For categorical features use mode, for numeric use median
            if base_feature in ['resname_encoded', 'core_exterior_encoded', 'secondary_structure_encoded']:
                # Use mode (most common value) for categorical
                mode_val = result_df[base_feature].mode()[0]
                result_df[col] = result_df[col].fillna(mode_val)
            else:
                # Use median for numeric features
                median_val = result_df[base_feature].median()
                result_df[col] = result_df[col].fillna(median_val)
    
    return result_df

def process_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Process features with appropriate error handling and fallbacks.
    
    Args:
        df: Input DataFrame with raw protein data
        config: Configuration dictionary
        
    Returns:
        DataFrame with processed features
    """
    try:
        # Start with basic feature processing
        processed_df = clean_data(df)
        
        # Add protein size if not present
        if "protein_size" not in processed_df.columns:
            processed_df["protein_size"] = processed_df.groupby("domain_id")["resid"].transform("count")
        
        # Ensure target column is numeric
        rmsf_col = config["dataset"]["target"]
        if rmsf_col in processed_df.columns:
            processed_df[rmsf_col] = pd.to_numeric(processed_df[rmsf_col], errors='coerce').fillna(0.0)
        else:
            logger.warning(f"Target column '{rmsf_col}' not found in data")
        
        # Filter features based on config
        feature_config = config["dataset"]["features"]
        use_features = feature_config.get("use_features", {})
        
        active_features = []
        for feature, enabled in use_features.items():
            if enabled and feature in processed_df.columns:
                active_features.append(feature)
            elif enabled and feature not in processed_df.columns:
                logger.warning(f"Feature '{feature}' is enabled but not found in data")
        
        # Add window features if enabled
        window_config = feature_config.get("window", {})
        if window_config.get("enabled", False):
            window_size = window_config.get("size", 3)
            processed_df = create_window_features(processed_df, window_size, active_features)
        
        return processed_df
    
    except Exception as e:
        # Log error and return original dataframe if processing fails
        logger.error(f"Feature processing failed: {e}")
        return df

def prepare_data_for_model(
    df: pd.DataFrame, 
    config: Dict[str, Any],
    include_target: bool = True
) -> Tuple[np.ndarray, Optional[np.ndarray], List[str]]:
    """
    Prepare final data matrices for model training/prediction.
    
    Args:
        df: Processed DataFrame with features
        config: Configuration dictionary
        include_target: Whether to include target variable in output
        
    Returns:
        Tuple of (X, y, feature_names) where y is None if include_target is False
    """
    # Get active features
    feature_config = config["dataset"]["features"]
    use_features = feature_config.get("use_features", {})
    
    active_features = []
    for feature, enabled in use_features.items():
        if enabled and feature in df.columns:
            active_features.append(feature)
    
    # Add window features if enabled
    window_config = feature_config.get("window", {})
    if window_config.get("enabled", False):
        window_cols = [col for col in df.columns if "_offset_" in col]
        active_features.extend(window_cols)
    
    # Ensure all required features exist
    missing_features = [f for f in active_features if f not in df.columns]
    if missing_features:
        logger.warning(f"Missing features: {missing_features}")
        active_features = [f for f in active_features if f in df.columns]
    
    # Prepare feature matrix
    X = df[active_features].values
    
    # Prepare target vector if needed
    y = None
    if include_target:
        target_col = config["dataset"]["target"]
        if target_col in df.columns:
            y = df[target_col].values
        else:
            raise ValueError(f"Target column '{target_col}' not found in data")
    
    return X, y, active_features

@lru_cache(maxsize=4)
def _cached_load_data(data_path: str) -> pd.DataFrame:
    """Load data from file with caching."""
    if data_path.endswith('.csv'):
        return pd.read_csv(data_path)
    elif data_path.endswith('.tsv'):
        return pd.read_csv(data_path, sep='\t')
    elif data_path.endswith('.pkl') or data_path.endswith('.pickle'):
        return pd.read_pickle(data_path)
    else:
        raise ValueError(f"Unsupported file format: {data_path}")

def load_and_process_data(
    data_path: Optional[str] = None,
    config: Dict[str, Any] = None,
    temperature: Optional[Union[int, str]] = None
) -> pd.DataFrame:
    """
    Load and process data from file or using temperature configuration.
    
    Args:
        data_path: Path to data file (if None, use temperature config)
        config: Configuration dictionary
        temperature: Optional temperature value to override config
        
    Returns:
        Processed DataFrame
        
    Raises:
        FileNotFoundError: If data file not found
        ValueError: If required columns are missing
    """
    if data_path:
        # Check if file exists
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Data file not found: {data_path}")
        
        # Load data with caching
        try:
            df = _cached_load_data(data_path)
        except Exception as e:
            raise ValueError(f"Error loading data from {data_path}: {e}")
    else:
        # Use temperature-based loading
        if config is None:
            raise ValueError("Config is required when data_path is not provided")
        
        df = load_temperature_data(config, temperature)
    
    if config:
        # Validate required columns
        required_cols = config["dataset"]["features"]["required"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            # Try to handle templated column names (e.g., rmsf_{temperature})
            templated_cols = []
            for col in missing_cols:
                if "{temperature}" in col:
                    temp = temperature or config["temperature"]["current"]
                    templated_col = col.replace("{temperature}", str(temp))
                    if templated_col in df.columns:
                        templated_cols.append((col, templated_col))
            
            for orig_col, templated_col in templated_cols:
                missing_cols.remove(orig_col)
            
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")
        
        # Process features
        processed_df = process_features(df, config)
        
        # Filter domains
        filtered_df = filter_domains(processed_df, config)
        
        return filtered_df
    else:
        # Just clean the data if no config provided
        return clean_data(df)

def split_data(
    df: pd.DataFrame,
    config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Split data into train, validation, and test sets.
    Respects domain boundaries when stratify_by_domain is True.
    
    Args:
        df: Processed DataFrame with features
        config: Configuration dictionary
        
    Returns:
        Tuple of (train_df, val_df, test_df)
    """
    from sklearn.model_selection import train_test_split
    
    split_config = config["dataset"]["split"]
    test_size = split_config.get("test_size", 0.2)
    val_size = split_config.get("validation_size", 0.15)
    random_state = split_config.get("random_state", 42)
    stratify_by_domain = split_config.get("stratify_by_domain", True)
    
    if stratify_by_domain:
        # Get unique domains
        domains = df['domain_id'].unique()
        
        # Split domains into train/test
        test_ratio = test_size
        domains_train, domains_test = train_test_split(
            domains, test_size=test_ratio, random_state=random_state
        )
        
        # Split train domains into train/val
        val_ratio = val_size / (1 - test_ratio)
        domains_train_final, domains_val = train_test_split(
            domains_train, test_size=val_ratio, random_state=random_state
        )
        
        # Create dataframes based on domain splits
        train_df = df[df['domain_id'].isin(domains_train_final)].copy()
        val_df = df[df['domain_id'].isin(domains_val)].copy()
        test_df = df[df['domain_id'].isin(domains_test)].copy()
    else:
        # Regular sample-based splitting
        # First split train/test
        train_val_df, test_df = train_test_split(
            df, test_size=test_size, random_state=random_state
        )
        
        # Then split train/val
        val_ratio = val_size / (1 - test_size)
        train_df, val_df = train_test_split(
            train_val_df, test_size=val_ratio, random_state=random_state
        )
    
    return train_df, val_df, test_df
### Temperature Handling Files ###
---------------------------------------------------------
===== FILE: flexseq/temperature/__init__.py =====
"""
Temperature handling modules for the FlexSeq ML pipeline.

This package contains functions for managing and comparing protein
flexibility predictions across multiple temperatures.
"""

from flexseq.temperature.comparison import (
    compare_temperature_predictions,
    calculate_temperature_correlations,
    generate_temperature_metrics
)
===== FILE: flexseq/temperature/comparison.py =====
"""
Temperature comparison functionality for the FlexSeq ML pipeline.

This module provides functions for comparing protein flexibility
predictions across multiple temperatures.
"""

import os
import logging
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import pandas as pd
from scipy import stats

from flexseq.data.loader import load_temperature_data

logger = logging.getLogger(__name__)

def compare_temperature_predictions(
    predictions: Dict[Union[int, str], pd.DataFrame],
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Compare predictions across temperatures.
    
    Args:
        predictions: Dictionary mapping temperatures to prediction DataFrames
        config: Configuration dictionary
        
    Returns:
        DataFrame with combined predictions for all temperatures
    """
    if not predictions:
        raise ValueError("No predictions provided for comparison")
    
    # Extract and prepare dataframes for merging
    merged_dfs = []
    
    for temp, df in predictions.items():
        # Create a subset with domain_id, resid, resname, and RMSF predictions
        subset = df[['domain_id', 'resid', 'resname']].copy()
        
        # Get target column and prediction column
        target_col = f"rmsf_{temp}" if temp != "average" else "rmsf_average"
        pred_col = f"{target_col}_predicted"
        
        if target_col in df.columns:
            subset[f"actual_{temp}"] = df[target_col]
        
        if pred_col in df.columns:
            subset[f"predicted_{temp}"] = df[pred_col]
        
        # Add uncertainty column if available
        uncertainty_col = f"{target_col}_uncertainty"
        if uncertainty_col in df.columns:
            subset[f"uncertainty_{temp}"] = df[uncertainty_col]
        
        # Add temperature indicator
        subset['temperature'] = temp
        
        merged_dfs.append(subset)
    
    # Combine all temperature predictions
    if not merged_dfs:
        raise ValueError("No valid prediction dataframes found")
    
    # Find common keys for all dataframes
    result = merged_dfs[0]
    
    for df in merged_dfs[1:]:
        # Use suffixes to avoid column name conflicts
        result = pd.merge(
            result, df, 
            on=['domain_id', 'resid', 'resname'], 
            suffixes=('', '_drop')
        )
        
        # Remove duplicate columns
        drop_cols = [col for col in result.columns if col.endswith('_drop')]
        result = result.drop(columns=drop_cols)
    
    return result

def calculate_temperature_correlations(
    combined_df: pd.DataFrame,
    temperatures: List[Union[int, str]],
    use_actual: bool = True
) -> pd.DataFrame:
    """
    Calculate correlations between RMSF values at different temperatures.
    
    Args:
        combined_df: DataFrame with combined predictions
        temperatures: List of temperatures to compare
        use_actual: Whether to use actual values (True) or predictions (False)
        
    Returns:
        DataFrame with correlation matrix
    """
    # Initialize correlation matrix
    n_temps = len(temperatures)
    corr_matrix = np.zeros((n_temps, n_temps))
    
    # Determine column prefix
    prefix = "actual_" if use_actual else "predicted_"
    
    # Fill correlation matrix
    for i, temp1 in enumerate(temperatures):
        col1 = f"{prefix}{temp1}"
        
        # Check if column exists
        if col1 not in combined_df.columns:
            logger.warning(f"Column {col1} not found, skipping correlations for {temp1}")
            continue
        
        for j, temp2 in enumerate(temperatures):
            col2 = f"{prefix}{temp2}"
            
            # Check if column exists
            if col2 not in combined_df.columns:
                logger.warning(f"Column {col2} not found, skipping correlations for {temp2}")
                continue
            
            # Calculate correlation, handling missing values
            valid_mask = ~(combined_df[col1].isna() | combined_df[col2].isna())
            if valid_mask.sum() > 1:
                pearson_r, _ = stats.pearsonr(
                    combined_df.loc[valid_mask, col1],
                    combined_df.loc[valid_mask, col2]
                )
                corr_matrix[i, j] = pearson_r
            else:
                corr_matrix[i, j] = np.nan
    
    # Create correlation DataFrame
    corr_df = pd.DataFrame(
        corr_matrix,
        index=[str(t) for t in temperatures],
        columns=[str(t) for t in temperatures]
    )
    
    return corr_df

def generate_temperature_metrics(
    predictions: Dict[Union[int, str], Dict[str, Dict[str, float]]],
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Generate metrics comparing model performance across temperatures.
    
    Args:
        predictions: Nested dict mapping temperature -> model_name -> metrics
        config: Configuration dictionary
        
    Returns:
        DataFrame with metrics for each model and temperature
    """
    # Get metrics of interest
    metrics_list = config["temperature"]["comparison"]["metrics"]
    
    # Create a list to store the results
    results = []
    
    for temp, models in predictions.items():
        for model_name, metrics in models.items():
            # Create a row for each temperature/model combination
            row = {
                'temperature': temp,
                'model': model_name
            }
            
            # Add metrics
            for metric in metrics_list:
                if metric in metrics:
                    row[metric] = metrics[metric]
            
            results.append(row)
    
    # Convert to DataFrame
    result_df = pd.DataFrame(results)
    
    return result_df

def analyze_temperature_effects(
    combined_df: pd.DataFrame,
    temperatures: List[Union[int, str]],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Analyze how protein flexibility changes with temperature.
    
    Args:
        combined_df: DataFrame with combined predictions
        temperatures: List of temperatures to analyze
        config: Configuration dictionary
        
    Returns:
        Dictionary with analysis results
    """
    # Only use numeric temperatures for trend analysis
    numeric_temps = [t for t in temperatures if isinstance(t, int) or (isinstance(t, str) and t.isdigit())]
    
    if len(numeric_temps) < 2:
        logger.warning("Not enough numeric temperatures for trend analysis")
        return {}
    
    # Convert string temperatures to int
    numeric_temps = [int(t) if isinstance(t, str) else t for t in numeric_temps]
    
    # Sort temperatures
    numeric_temps.sort()
    
    # Initialize results dictionary
    results = {
        'linear_coefficients': {},
        'r_squared': {},
        'domain_trends': [],
        'residue_outliers': []
    }
    
    # Analyze per-residue trends
    for _, residue_group in combined_df.groupby(['domain_id', 'resid']):
        domain_id = residue_group['domain_id'].iloc[0]
        resid = residue_group['resid'].iloc[0]
        resname = residue_group['resname'].iloc[0]
        
        # Get flexibility values across temperatures
        flex_values = []
        for temp in numeric_temps:
            col = f"actual_{temp}"
            if col in residue_group.columns and not pd.isna(residue_group[col].iloc[0]):
                flex_values.append(residue_group[col].iloc[0])
            else:
                # If missing, use predicted value if available
                pred_col = f"predicted_{temp}"
                if pred_col in residue_group.columns and not pd.isna(residue_group[pred_col].iloc[0]):
                    flex_values.append(residue_group[pred_col].iloc[0])
                else:
                    flex_values.append(np.nan)
        
        # Skip if too many missing values
        if sum(~np.isnan(flex_values)) < 2:
            continue
        
        # Fit linear model
        valid_indices = ~np.isnan(flex_values)
        x = np.array(numeric_temps)[valid_indices]
        y = np.array(flex_values)[valid_indices]
        
        if len(x) < 2:
            continue
        
        # Linear regression
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        
        # Add result to domain trends
        results['domain_trends'].append({
            'domain_id': domain_id,
            'resid': resid,
            'resname': resname,
            'slope': slope,
            'intercept': intercept,
            'r_squared': r_value**2,
            'p_value': p_value
        })
        
        # Check if this is an outlier (high slope or negative slope)
        if slope < 0 or slope > np.nanmean([r['slope'] for r in results['domain_trends']]) + 2 * np.nanstd([r['slope'] for r in results['domain_trends']]):
            results['residue_outliers'].append({
                'domain_id': domain_id,
                'resid': resid,
                'resname': resname,
                'slope': slope,
                'r_squared': r_value**2,
                'behavior': 'negative_trend' if slope < 0 else 'high_increase'
            })
    
    # Calculate domain-level statistics
    domains = combined_df['domain_id'].unique()
    domain_stats = []
    
    for domain in domains:
        domain_mask = combined_df['domain_id'] == domain
        domain_trends = [r for r in results['domain_trends'] if r['domain_id'] == domain]
        
        if not domain_trends:
            continue
        
        # Get average slope and r_squared for the domain
        slopes = [r['slope'] for r in domain_trends]
        r_squared = [r['r_squared'] for r in domain_trends]
        
        domain_stats.append({
            'domain_id': domain,
            'avg_slope': np.nanmean(slopes),
            'std_slope': np.nanstd(slopes),
            'avg_r_squared': np.nanmean(r_squared),
            'num_residues': len(domain_trends),
            'outliers': len([r for r in results['residue_outliers'] if r['domain_id'] == domain])
        })
    
    # Add domain stats to results
    results['domain_stats'] = domain_stats
    
    # Calculate amino acid-specific responses
    aa_responses = []
    
    for resname in np.unique(combined_df['resname']):
        resname_trends = [r for r in results['domain_trends'] if r['resname'] == resname]
        
        if not resname_trends:
            continue
        
        # Get average slope and r_squared for this amino acid
        slopes = [r['slope'] for r in resname_trends]
        r_squared = [r['r_squared'] for r in resname_trends]
        
        aa_responses.append({
            'resname': resname,
            'avg_slope': np.nanmean(slopes),
            'std_slope': np.nanstd(slopes),
            'avg_r_squared': np.nanmean(r_squared),
            'num_residues': len(resname_trends)
        })
    
    # Add amino acid responses to results
    results['aa_responses'] = aa_responses
    
    return results

def prepare_temperature_comparison_data(
    config: Dict[str, Any],
    model_name: str,
    output_dir: str
) -> Dict[str, pd.DataFrame]:
    """
    Prepare data for temperature comparison visualization.
    
    Args:
        config: Configuration dictionary
        model_name: Model name to use for prediction
        output_dir: Output directory to save comparison data
        
    Returns:
        Dictionary mapping data_name -> DataFrame
    """
    # Get available temperatures
    temperatures = config["temperature"]["available"]
    
    # Load predictions for each temperature
    predictions = {}
    model_metrics = {}
    
    for temp in temperatures:
        # Get temperature-specific output directory
        temp_output_dir = os.path.join(config["paths"]["output_dir"], f"outputs_{temp}")
        
        # Load all results
        results_path = os.path.join(temp_output_dir, "all_results.csv")
        
        if not os.path.exists(results_path):
            logger.warning(f"Results file not found for temperature {temp}: {results_path}")
            continue
        
        # Load predictions
        predictions[temp] = pd.read_csv(results_path)
        
        # Load metrics
        metrics_path = os.path.join(temp_output_dir, "evaluation_results.csv")
        
        if os.path.exists(metrics_path):
            metrics_df = pd.read_csv(metrics_path, index_col=0)
            
            if model_name in metrics_df.index:
                model_metrics[temp] = metrics_df.loc[model_name].to_dict()
            else:
                logger.warning(f"Model {model_name} not found in metrics for temperature {temp}")
    
    # Create combined predictions
    combined_preds = compare_temperature_predictions(predictions, config)
    
    # Save combined predictions
    combined_path = os.path.join(output_dir, "combined_predictions.csv")
    os.makedirs(os.path.dirname(combined_path), exist_ok=True)
    combined_preds.to_csv(combined_path, index=False)
    
    # Calculate correlations
    actual_corr = calculate_temperature_correlations(combined_preds, temperatures, use_actual=True)
    predicted_corr = calculate_temperature_correlations(combined_preds, temperatures, use_actual=False)
    
    # Save correlations
    actual_corr_path = os.path.join(output_dir, "actual_correlations.csv")
    predicted_corr_path = os.path.join(output_dir, "predicted_correlations.csv")
    
    actual_corr.to_csv(actual_corr_path)
    predicted_corr.to_csv(predicted_corr_path)
    
    # Generate metrics comparison
    metrics_df = generate_temperature_metrics({'all_models': model_metrics}, config)
    metrics_path = os.path.join(output_dir, "temperature_metrics.csv")
    metrics_df.to_csv(metrics_path, index=False)
    
    # Analyze temperature effects
    effects = analyze_temperature_effects(combined_preds, temperatures, config)
    
    # Save temperature effects
    for key, data in effects.items():
        if isinstance(data, list) and data:
            df = pd.DataFrame(data)
            df_path = os.path.join(output_dir, f"{key}.csv")
            df.to_csv(df_path, index=False)
    
    # Prepare histogram data
    histogram_data = []
    
    for temp in temperatures:
        if temp in predictions:
            df = predictions[temp]
            
            # Get RMSF column
            target_col = f"rmsf_{temp}" if temp != "average" else "rmsf_average"
            pred_col = f"{target_col}_predicted"
            
            if target_col in df.columns and pred_col in df.columns:
                # Calculate histogram
                actual_values = df[target_col].dropna()
                predicted_values = df[pred_col].dropna()
                
                if len(actual_values) > 0 and len(predicted_values) > 0:
                    # Get histogram data
                    actual_hist, actual_bins = np.histogram(actual_values, bins=20)
                    predicted_hist, predicted_bins = np.histogram(predicted_values, bins=20)
                    
                    # Add to results
                    for i in range(len(actual_hist)):
                        histogram_data.append({
                            'temperature': temp,
                            'type': 'actual',
                            'bin_start': actual_bins[i],
                            'bin_end': actual_bins[i+1],
                            'count': actual_hist[i]
                        })
                    
                    for i in range(len(predicted_hist)):
                        histogram_data.append({
                            'temperature': temp,
                            'type': 'predicted',
                            'bin_start': predicted_bins[i],
                            'bin_end': predicted_bins[i+1],
                            'count': predicted_hist[i]
                        })
    
    # Save histogram data
    if histogram_data:
        histogram_df = pd.DataFrame(histogram_data)
        histogram_path = os.path.join(output_dir, "histogram_data.csv")
        histogram_df.to_csv(histogram_path, index=False)
    
    return {
        'combined_predictions': combined_preds,
        'actual_correlations': actual_corr,
        'predicted_correlations': predicted_corr,
        'temperature_metrics': metrics_df,
        'histogram_data': pd.DataFrame(histogram_data) if histogram_data else None
    }
### Utility Files ###
---------------------------------------------------------
===== FILE: flexseq/utils/__init__.py =====
"""
Utility modules for the FlexSeq ML pipeline.

This package contains utility functions for metrics, visualization, and
general helpers used throughout the pipeline.
"""

# Import key functions for easier access
from flexseq.utils.metrics import evaluate_predictions, cross_validate_model
from flexseq.utils.helpers import timer, ensure_dir, progress_bar, ProgressCallback
===== FILE: flexseq/utils/helpers.py =====
"""
Helper functions for the FlexSeq ML pipeline.

This module provides utility functions used throughout the pipeline.
"""

import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional, Union, Callable, Iterable, TypeVar
from functools import wraps
from time import time

import numpy as np
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

T = TypeVar('T')

def timer(func):
    """
    Decorator for measuring function execution time.
    
    Args:
        func: Function to wrap
        
    Returns:
        Wrapped function
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time()
        result = func(*args, **kwargs)
        end_time = time()
        logger.info(f"Function '{func.__name__}' executed in {end_time - start_time:.2f} seconds")
        return result
    return wrapper

def ensure_dir(directory: str) -> str:
    """
    Ensure a directory exists, creating it if necessary.
    
    Args:
        directory: Directory path
        
    Returns:
        Directory path
    """
    os.makedirs(directory, exist_ok=True)
    return directory

def estimate_memory_usage(df: pd.DataFrame) -> Tuple[float, str]:
    """
    Estimate memory usage of a DataFrame.
    
    Args:
        df: Input DataFrame
        
    Returns:
        Tuple of (size, unit) where size is a number and unit is a string
    """
    memory_bytes = df.memory_usage(deep=True).sum()
    
    if memory_bytes < 1024:
        return memory_bytes, "bytes"
    elif memory_bytes < 1024**2:
        return memory_bytes / 1024, "KB"
    elif memory_bytes < 1024**3:
        return memory_bytes / (1024**2), "MB"
    else:
        return memory_bytes / (1024**3), "GB"

def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:
    """
    Split a DataFrame into chunks of specified size.
    
    Args:
        df: Input DataFrame
        chunk_size: Number of rows per chunk
        
    Returns:
        List of DataFrame chunks
    """
    return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]

def safe_open(file_path: str, mode: str = 'r'):
    """
    Safely open a file with proper directory creation.
    
    Args:
        file_path: Path to file
        mode: File opening mode
        
    Returns:
        File object
    """
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
    return open(file_path, mode)

def truncate_filename(filename: str, max_length: int = 255) -> str:
    """
    Truncate a filename to ensure it doesn't exceed maximum path length.
    
    Args:
        filename: Original filename
        max_length: Maximum allowed length
        
    Returns:
        Truncated filename
    """
    if len(filename) <= max_length:
        return filename
    
    name, ext = os.path.splitext(filename)
    return name[:max_length - len(ext)] + ext

def safe_parse_float(value: Any) -> Optional[float]:
    """
    Safely parse a value to float, returning None if not possible.
    
    Args:
        value: Value to convert
        
    Returns:
        Float value or None if conversion fails
    """
    if pd.isna(value):
        return None
    
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

def get_amino_acid_properties() -> Dict[str, Dict[str, Any]]:
    """
    Get dictionary of amino acid properties.
    
    Returns:
        Dictionary mapping amino acid codes to property dictionaries
    """
    properties = {
        # Hydrophobic residues
        'ALA': {'hydropathy': 1.8, 'volume': 88.6, 'charge': 0, 'group': 'hydrophobic'},
        'VAL': {'hydropathy': 4.2, 'volume': 140.0, 'charge': 0, 'group': 'hydrophobic'},
        'LEU': {'hydropathy': 3.8, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'ILE': {'hydropathy': 4.5, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'MET': {'hydropathy': 1.9, 'volume': 162.9, 'charge': 0, 'group': 'hydrophobic'},
        'PHE': {'hydropathy': 2.8, 'volume': 189.9, 'charge': 0, 'group': 'hydrophobic'},
        'TRP': {'hydropathy': -0.9, 'volume': 227.8, 'charge': 0, 'group': 'hydrophobic'},
        'PRO': {'hydropathy': -1.6, 'volume': 112.7, 'charge': 0, 'group': 'special'},
        'GLY': {'hydropathy': -0.4, 'volume': 60.1, 'charge': 0, 'group': 'special'},
        
        # Polar residues
        'SER': {'hydropathy': -0.8, 'volume': 89.0, 'charge': 0, 'group': 'polar'},
        'THR': {'hydropathy': -0.7, 'volume': 116.1, 'charge': 0, 'group': 'polar'},
        'CYS': {'hydropathy': 2.5, 'volume': 108.5, 'charge': 0, 'group': 'polar'},
        'TYR': {'hydropathy': -1.3, 'volume': 193.6, 'charge': 0, 'group': 'polar'},
        'ASN': {'hydropathy': -3.5, 'volume': 111.1, 'charge': 0, 'group': 'polar'},
        'GLN': {'hydropathy': -3.5, 'volume': 143.8, 'charge': 0, 'group': 'polar'},
        
        # Charged residues
        'LYS': {'hydropathy': -3.9, 'volume': 168.6, 'charge': 1, 'group': 'positive'},
        'ARG': {'hydropathy': -4.5, 'volume': 173.4, 'charge': 1, 'group': 'positive'},
        'HIS': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'ASP': {'hydropathy': -3.5, 'volume': 111.1, 'charge': -1, 'group': 'negative'},
        'GLU': {'hydropathy': -3.5, 'volume': 138.4, 'charge': -1, 'group': 'negative'},
        
        # Non-standard
        'HSE': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSD': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSP': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 1, 'group': 'positive'},
        'UNK': {'hydropathy': 0.0, 'volume': 0.0, 'charge': 0, 'group': 'unknown'}
    }
    
    return properties

def is_glycine_or_proline(residue: str) -> bool:
    """
    Check if residue is Glycine or Proline, which significantly affect flexibility.
    
    Args:
        residue: Residue name
        
    Returns:
        True if residue is GLY or PRO
    """
    return residue in ["GLY", "PRO"]

def analyze_hydrogen_bonds(
    secondary_structure: str, 
    residue_name: str
) -> float:
    """
    Analyze potential hydrogen bonding based on secondary structure and residue type.
    Returns a relative measure of hydrogen bond stabilization (higher means more stable).
    
    Args:
        secondary_structure: DSSP code
        residue_name: Residue name
        
    Returns:
        Relative stability score (0-1)
    """
    # Secondary structure types have different H-bond patterns
    ss_stability = {
        'H': 1.0,  # Alpha helix - most stable H-bond pattern
        'G': 0.8,  # 3-10 helix
        'I': 0.9,  # Pi helix
        'E': 0.9,  # Beta sheet - very stable
        'B': 0.7,  # Beta bridge
        'T': 0.4,  # Turn
        'S': 0.3,  # Bend
        'C': 0.1   # Coil - least stable
    }
    
    # Some residues have different H-bond propensities
    residue_factors = {
        'SER': 1.2,  # Can form side-chain H-bonds
        'THR': 1.2,  # Can form side-chain H-bonds
        'ASN': 1.2,  # Can form side-chain H-bonds
        'GLN': 1.2,  # Can form side-chain H-bonds
        'TYR': 1.1,  # Can form side-chain H-bonds
        'PRO': 0.7,  # Disrupts H-bond patterns
        'GLY': 0.9   # More flexible backbone
    }
    
    # Get base stability from secondary structure
    stability = ss_stability.get(secondary_structure, 0.1)
    
    # Apply residue-specific factor
    factor = residue_factors.get(residue_name, 1.0)
    
    return min(1.0, stability * factor)

def calculate_sequence_complexity(sequence: List[str], window_size: int = 5) -> List[float]:
    """
    Calculate sequence complexity using Shannon entropy in a sliding window.
    Higher values indicate more diverse/complex sequence regions.
    
    Args:
        sequence: List of amino acid types
        window_size: Size of sliding window
        
    Returns:
        List of complexity values for each position
    """
    # Convert to single-letter amino acid codes if needed
    if all(len(aa) == 3 for aa in sequence if aa):
        three_to_one = {
            'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',
            'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',
            'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',
            'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V',
            'HSD': 'H', 'HSE': 'H', 'HSP': 'H', 'UNK': 'X'
        }
        sequence = [three_to_one.get(aa, 'X') for aa in sequence]
    
    # Compute complexity for each position
    complexity = []
    half_window = window_size // 2
    padded_seq = ['X'] * half_window + list(sequence) + ['X'] * half_window
    
    for i in range(half_window, len(padded_seq) - half_window):
        window = padded_seq[i - half_window:i + half_window + 1]
        
        # Calculate Shannon entropy
        aa_counts = {}
        for aa in window:
            if aa in aa_counts:
                aa_counts[aa] += 1
            else:
                aa_counts[aa] = 1
        
        entropy = 0
        for count in aa_counts.values():
            p = count / window_size
            entropy -= p * np.log2(p)
        
        # Normalize by maximum possible entropy (all different amino acids)
        max_entropy = np.log2(min(20, window_size))
        if max_entropy > 0:
            normalized_entropy = entropy / max_entropy
        else:
            normalized_entropy = 0
        
        complexity.append(normalized_entropy)
    
    return complexity

def progress_bar(
    iterable: Iterable[T],
    desc: str = None,
    total: Optional[int] = None,
    disable: bool = False,
    leave: bool = True,
    **kwargs
) -> Iterable[T]:
    """
    Create a progress bar for an iterable.
    
    Args:
        iterable: Iterable to track progress of
        desc: Description for the progress bar
        total: Total number of items (inferred if not provided)
        disable: Whether to disable the progress bar
        leave: Whether to leave the progress bar after completion
        **kwargs: Additional arguments to pass to tqdm
        
    Returns:
        Wrapped iterable with progress tracking
    """
    # Disable progress bar if verbose logging is not enabled
    log_level = logging.getLogger().getEffectiveLevel()
    if log_level > logging.INFO:
        disable = True
        
    return tqdm(
        iterable,
        desc=desc,
        total=total,
        disable=disable,
        leave=leave,
        ncols=100,  # Fixed width
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
        **kwargs
    )

class ProgressCallback:
    """
    Callback class to track progress of operations that don't use iterables.
    """
    
    def __init__(
        self, 
        total: int, 
        desc: str = None,
        disable: bool = False,
        leave: bool = True,
        **kwargs
    ):
        """
        Initialize progress callback.
        
        Args:
            total: Total number of steps
            desc: Description for the progress bar
            disable: Whether to disable the progress bar
            leave: Whether to leave the progress bar after completion
            **kwargs: Additional arguments to pass to tqdm
        """
        # Disable progress bar if verbose logging is not enabled
        log_level = logging.getLogger().getEffectiveLevel()
        if log_level > logging.INFO:
            disable = True
            
        self.pbar = tqdm(
            total=total,
            desc=desc,
            disable=disable,
            leave=leave,
            ncols=100,  # Fixed width
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            **kwargs
        )
    
    def update(self, n: int = 1):
        """Update the progress bar by n steps."""
        self.pbar.update(n)
    
    def set_description(self, desc: str):
        """Set the description of the progress bar."""
        self.pbar.set_description(desc)
    
    def set_postfix(self, **kwargs):
        """Set the postfix of the progress bar."""
        self.pbar.set_postfix(**kwargs)
    
    def close(self):
        """Close the progress bar."""
        self.pbar.close()
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()

def format_time(seconds: float) -> str:
    """
    Format time in seconds to a human-readable string.
    
    Args:
        seconds: Time in seconds
        
    Returns:
        Formatted time string
    """
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} hours"

def get_temperature_color(temperature: Union[int, str]) -> str:
    """
    Get a color code for a temperature value.
    Colors range from blue (cold) to red (hot).
    
    Args:
        temperature: Temperature value
        
    Returns:
        Hex color code
    """
    # Handle special case for "average"
    if temperature == "average" or not isinstance(temperature, (int, float)):
        return "#7F7F7F"  # Gray
    
    # Temperature ranges for FlexSeq
    min_temp = 320
    max_temp = 450
    
    # Clamp temperature to range
    temp = max(min_temp, min(temperature, max_temp))
    
    # Normalize to 0-1 range
    normalized = (temp - min_temp) / (max_temp - min_temp)
    
    # Generate color (blue to red)
    r = int(255 * normalized)
    b = int(255 * (1 - normalized))
    g = int(100 * (1 - abs(2 * normalized - 1)))
    
    return f"#{r:02x}{g:02x}{b:02x}"

def make_model_color_map(model_names: List[str]) -> Dict[str, str]:
    """
    Create a consistent color mapping for models.
    
    Args:
        model_names: List of model names
        
    Returns:
        Dictionary mapping model names to color codes
    """
    # Standard colors for models
    standard_colors = [
        "#1f77b4",  # Blue
        "#ff7f0e",  # Orange
        "#2ca02c",  # Green
        "#d62728",  # Red
        "#9467bd",  # Purple
        "#8c564b",  # Brown
        "#e377c2",  # Pink
        "#7f7f7f",  # Gray
        "#bcbd22",  # Olive
        "#17becf"   # Cyan
    ]
    
    # Create mapping
    color_map = {}
    for i, model in enumerate(model_names):
        color_map[model] = standard_colors[i % len(standard_colors)]
    
    return color_map
===== FILE: flexseq/utils/metrics.py =====
"""
Evaluation metrics for the FlexSeq ML pipeline.

This module provides functions for evaluating model performance
and cross-validation.
"""

import logging
from typing import Dict, List, Any, Union, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    explained_variance_score,
    max_error,
    median_absolute_error
)
from sklearn.model_selection import KFold, cross_val_score
from scipy.stats import pearsonr, spearmanr

logger = logging.getLogger(__name__)

def evaluate_predictions(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    config: Dict[str, Any],
    X: Optional[np.ndarray] = None,
    n_features: Optional[int] = None
) -> Dict[str, float]:
    """
    Evaluate predictions using multiple metrics.
    
    Args:
        y_true: True target values
        y_pred: Predicted values
        config: Configuration dictionary with metrics settings
        X: Optional feature matrix for advanced metrics
        n_features: Optional number of features for adjusted R2
        
    Returns:
        Dictionary of metric names and values
    """
    results = {}
    metrics_config = config["evaluation"]["metrics"]
    
    # Root Mean Squared Error
    if metrics_config.get("rmse", True):
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results["rmse"] = rmse
    
    # Mean Absolute Error
    if metrics_config.get("mae", True):
        mae = mean_absolute_error(y_true, y_pred)
        results["mae"] = mae
    
    # R-squared
    if metrics_config.get("r2", True):
        r2 = r2_score(y_true, y_pred)
        results["r2"] = r2
    
    # Pearson Correlation
    if metrics_config.get("pearson_correlation", True):
        pearson_corr, p_value = pearsonr(y_true, y_pred)
        results["pearson_correlation"] = pearson_corr
        results["pearson_p_value"] = p_value
    
    # Spearman Correlation
    if metrics_config.get("spearman_correlation", True):
        spearman_corr, p_value = spearmanr(y_true, y_pred)
        results["spearman_correlation"] = spearman_corr
        results["spearman_p_value"] = p_value
    
    # Explained Variance Score
    if metrics_config.get("explained_variance", False):
        ev = explained_variance_score(y_true, y_pred)
        results["explained_variance"] = ev
    
    # Max Error
    if metrics_config.get("max_error", False):
        me = max_error(y_true, y_pred)
        results["max_error"] = me
    
    # Median Absolute Error
    if metrics_config.get("median_absolute_error", False):
        medae = median_absolute_error(y_true, y_pred)
        results["median_absolute_error"] = medae
    
    # Adjusted R2
    if metrics_config.get("adjusted_r2", False) and n_features is not None:
        n = len(y_true)
        r2 = results.get("r2", r2_score(y_true, y_pred))
        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)
        results["adjusted_r2"] = adj_r2
    
    # Root Mean Square Absolute Error
    if metrics_config.get("root_mean_square_absolute_error", False):
        rmsae = np.sqrt(np.mean(np.abs(y_true - y_pred)**2))
        results["root_mean_square_absolute_error"] = rmsae
    
    # Q2 (Cross-validated R2)
    if metrics_config.get("q2", False) and X is not None:
        from sklearn.linear_model import LinearRegression
        cv_r2 = cross_val_score(
            LinearRegression(), X, y_true, 
            cv=5, scoring='r2'
        ).mean()
        results["q2"] = cv_r2
    
    # Temperature-specific metrics (for OmniFlex mode)
    if config["mode"]["active"] == "omniflex":
        # Calculate additional metrics for temperature analysis
        temp = config["temperature"]["current"]
        if str(temp).isdigit():
            # Temperature coefficient (how well the model captures temperature effects)
            # This is a placeholder - in a real implementation, this would use
            # actual temperature coefficients from molecular dynamics
            temp_coef = np.corrcoef(y_pred, np.array(y_true) * int(temp)/400)[0, 1]
            results["temperature_coefficient"] = temp_coef
    
    return results

def cross_validate_model(
    model_class: Any,
    model_params: Dict[str, Any],
    data: pd.DataFrame,
    config: Dict[str, Any],
    n_folds: int = 5,
    return_predictions: bool = False
) -> Dict[str, float]:
    """
    Perform cross-validation for a model.
    
    Args:
        model_class: Model class to instantiate
        model_params: Parameters for model initialization
        data: DataFrame with features and target
        config: Configuration dictionary
        n_folds: Number of cross-validation folds
        return_predictions: Whether to return predictions
        
    Returns:
        Dictionary with cross-validation results
    """
    from flexseq.data.processor import prepare_data_for_model
    from flexseq.utils.helpers import progress_bar
    
    # Get target column
    target_col = config["dataset"]["target"]
    
    # Initialize metrics storage
    metrics = {
        "rmse": [],
        "mae": [],
        "r2": [],
        "pearson_correlation": []
    }
    
    # Add storage for predictions if requested
    all_predictions = []
    all_true_values = []
    
    # Create cross-validation folds
    stratify_by_domain = config["dataset"]["split"].get("stratify_by_domain", True)
    random_state = config["system"]["random_state"]
    
    if stratify_by_domain:
        # Domain-aware cross-validation
        unique_domains = data["domain_id"].unique()
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
        
        fold_domains = []
        for train_idx, test_idx in kf.split(unique_domains):
            # Get train and test domains
            train_domains = unique_domains[train_idx]
            test_domains = unique_domains[test_idx]
            fold_domains.append((train_domains, test_domains))
        
        for i, (train_domains, test_domains) in enumerate(
            progress_bar(fold_domains, desc=f"Cross-validation ({n_folds} folds)")
        ):
            # Split data by domains
            train_data = data[data["domain_id"].isin(train_domains)]
            test_data = data[data["domain_id"].isin(test_domains)]
            
            # Prepare data for model
            X_train, y_train, feature_names = prepare_data_for_model(train_data, config)
            X_test, y_test, _ = prepare_data_for_model(test_data, config)
            
            # Create and train model
            model = model_class(**model_params)
            model.fit(X_train, y_train)
            
            # Generate predictions
            y_pred = model.predict(X_test)
            
            # Evaluate predictions with feature count for adjusted R2
            n_features = X_train.shape[1]
            fold_metrics = evaluate_predictions(
                y_test, y_pred, config, X_test, n_features
            )
            
            # Store results
            for metric, value in fold_metrics.items():
                if metric in metrics:
                    metrics[metric].append(value)
                else:
                    metrics[metric] = [value]
            
            # Store predictions if requested
            if return_predictions:
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)
                
            # Get uncertainty if available
            if hasattr(model, 'predict_with_std'):
                try:
                    _, y_std = model.predict_with_std(X_test)
                    if "uncertainty_std" not in metrics:
                        metrics["uncertainty_std"] = []
                    metrics["uncertainty_std"].append(np.mean(y_std))
                except Exception as e:
                    logger.warning(f"Could not calculate uncertainty: {e}")
    else:
        # Regular cross-validation
        X, y, feature_names = prepare_data_for_model(data, config)
        
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
        
        fold_indices = []
        for train_idx, test_idx in kf.split(X):
            fold_indices.append((train_idx, test_idx))
        
        for i, (train_idx, test_idx) in enumerate(
            progress_bar(fold_indices, desc=f"Cross-validation ({n_folds} folds)")
        ):
            # Split data
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            
            # Create and train model
            model = model_class(**model_params)
            model.fit(X_train, y_train)
            
            # Generate predictions
            y_pred = model.predict(X_test)
            
            # Evaluate predictions with feature count for adjusted R2
            n_features = X_train.shape[1]
            fold_metrics = evaluate_predictions(
                y_test, y_pred, config, X_test, n_features
            )
            
            # Store results
            for metric, value in fold_metrics.items():
                if metric in metrics:
                    metrics[metric].append(value)
                else:
                    metrics[metric] = [value]
            
            # Store predictions if requested
            if return_predictions:
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)
                
            # Get uncertainty if available
            if hasattr(model, 'predict_with_std'):
                try:
                    _, y_std = model.predict_with_std(X_test)
                    if "uncertainty_std" not in metrics:
                        metrics["uncertainty_std"] = []
                    metrics["uncertainty_std"].append(np.mean(y_std))
                except Exception as e:
                    logger.warning(f"Could not calculate uncertainty: {e}")
    
    # Calculate statistics
    results = {}
    
    for metric, values in metrics.items():
        if values:
            results[f"mean_{metric}"] = np.mean(values)
            results[f"std_{metric}"] = np.std(values)
    
    # Add predictions if requested
    if return_predictions:
        results["predictions"] = np.array(all_predictions)
        results["true_values"] = np.array(all_true_values)
    
    return results

def calculate_residue_metrics(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str],
    include_uncertainty: bool = False,
    uncertainty_cols: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Calculate residue-level metrics for model predictions.
    
    Args:
        df: DataFrame with true values and predictions
        target_col: Column with true target values
        prediction_cols: Columns with model predictions
        include_uncertainty: Whether to include uncertainty metrics
        uncertainty_cols: Columns with prediction uncertainties
        
    Returns:
        DataFrame with residue-level metrics
    """
    results = []
    
    for (domain_id, resid), residue_df in df.groupby(["domain_id", "resid"]):
        residue_metrics = {
            "domain_id": domain_id,
            "resid": resid,
            "resname": residue_df["resname"].iloc[0] if "resname" in residue_df.columns else None
        }
        
        # Add structural features if available
        for feature in ["secondary_structure_encoded", "core_exterior_encoded"]:
            if feature in residue_df.columns:
                residue_metrics[feature] = residue_df[feature].iloc[0]
        
        # Calculate metrics for each model
        true_value = residue_df[target_col].iloc[0]
        residue_metrics["actual"] = true_value
        
        for pred_col in prediction_cols:
            pred_value = residue_df[pred_col].iloc[0]
            residue_metrics[pred_col] = pred_value
            
            # Calculate error
            error = pred_value - true_value
            abs_error = abs(error)
            
            model_name = pred_col.split("_predicted")[0]
            residue_metrics[f"{model_name}_error"] = error
            residue_metrics[f"{model_name}_abs_error"] = abs_error
            
            # Add uncertainty if available
            if include_uncertainty and uncertainty_cols is not None:
                unc_col = next((col for col in uncertainty_cols if model_name in col), None)
                if unc_col and unc_col in residue_df.columns:
                    unc_value = residue_df[unc_col].iloc[0]
                    residue_metrics[f"{model_name}_uncertainty"] = unc_value
                    
                    # Calculate normalized error (error / uncertainty)
                    if unc_value > 0:
                        normalized_error = abs_error / unc_value
                        residue_metrics[f"{model_name}_normalized_error"] = normalized_error
        
        results.append(residue_metrics)
    
    return pd.DataFrame(results)

def calculate_temperature_scaling_factors(
    df: pd.DataFrame,
    temperatures: List[Union[int, str]]
) -> Dict[str, float]:
    """
    Calculate scaling factors between RMSF values at different temperatures.
    
    Args:
        df: DataFrame with RMSF values at multiple temperatures
        temperatures: List of temperatures to analyze
        
    Returns:
        Dictionary mapping temperature pairs to scaling factors
    """
    # Convert string temperatures to int if numeric
    temp_list = []
    for temp in temperatures:
        if isinstance(temp, str) and temp.isdigit():
            temp_list.append(int(temp))
        elif isinstance(temp, int):
            temp_list.append(temp)
    
    # Sort temperatures
    temp_list.sort()
    
    # Calculate scaling factors
    scaling_factors = {}
    
    for i, temp1 in enumerate(temp_list):
        col1 = f"rmsf_{temp1}"
        if col1 not in df.columns:
            continue
            
        for j, temp2 in enumerate(temp_list[i+1:], i+1):
            col2 = f"rmsf_{temp2}"
            if col2 not in df.columns:
                continue
                
            # Calculate average ratio
            valid_mask = (df[col1] > 0) & (df[col2] > 0)
            ratios = df.loc[valid_mask, col2] / df.loc[valid_mask, col1]
            
            # Store average
            key = f"{temp1}_to_{temp2}"
            scaling_factors[key] = ratios.mean()
    
    return scaling_factors

def calculate_uncertainty_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_std: np.ndarray
) -> Dict[str, float]:
    """
    Calculate metrics for uncertainty quantification.
    
    Args:
        y_true: True target values
        y_pred: Predicted values
        y_std: Standard deviation of predictions (uncertainty)
        
    Returns:
        Dictionary of uncertainty metrics
    """
    # Calculate absolute errors
    errors = np.abs(y_true - y_pred)
    
    # Calculate percentage of true values within confidence intervals
    within_1std = np.mean(errors <= y_std)
    within_2std = np.mean(errors <= 2 * y_std)
    within_3std = np.mean(errors <= 3 * y_std)
    
    # Ideal percentages: 68% within 1 std, 95% within 2 std, 99.7% within 3 std
    # Calculate calibration error (how far from ideal)
    cal_error_1std = np.abs(within_1std - 0.68)
    cal_error_2std = np.abs(within_2std - 0.95)
    cal_error_3std = np.abs(within_3std - 0.997)
    
    # Average calibration error
    avg_cal_error = (cal_error_1std + cal_error_2std + cal_error_3std) / 3
    
    # Calculate negative log predictive density (NLPD)
    nlpd = np.mean(0.5 * np.log(2 * np.pi * y_std**2) + 
                   0.5 * ((y_true - y_pred)**2) / (y_std**2))
    
    # Calculate uncertainty-error correlation
    # Ideally, higher uncertainty should correlate with higher error
    unc_err_corr = np.corrcoef(y_std, errors)[0, 1]
    
    return {
        "within_1std": within_1std,
        "within_2std": within_2std,
        "within_3std": within_3std,
        "calibration_error_1std": cal_error_1std,
        "calibration_error_2std": cal_error_2std,
        "calibration_error_3std": cal_error_3std,
        "avg_calibration_error": avg_cal_error,
        "nlpd": nlpd,
        "uncertainty_error_correlation": unc_err_corr
    }

def calculate_domain_performance(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str]
) -> pd.DataFrame:
    """
    Calculate performance metrics by domain.
    
    Args:
        df: DataFrame with predictions and actual values
        target_col: Target column name
        prediction_cols: Columns with model predictions
        
    Returns:
        DataFrame with domain performance metrics
    """
    results = []
    
    for domain_id, domain_df in df.groupby("domain_id"):
        row = {"domain_id": domain_id, "num_residues": len(domain_df)}
        
        # Add domain properties if available
        if "core_exterior_encoded" in domain_df.columns:
            row["percent_surface"] = domain_df["core_exterior_encoded"].mean() * 100
        
        if "secondary_structure_encoded" in domain_df.columns:
            # Map values to types
            ss_mapping = {0: "helix", 1: "sheet", 2: "loop"}
            ss_counts = domain_df["secondary_structure_encoded"].value_counts(normalize=True) * 100
            for ss_code, ss_type in ss_mapping.items():
                row[f"percent_{ss_type}"] = ss_counts[ss_code] if ss_code in ss_counts else 0
        
        # Calculate metrics for each model
        for pred_col in prediction_cols:
            model_name = pred_col.split("_predicted")[0]
            
            # Calculate metrics
            y_true = domain_df[target_col].values
            y_pred = domain_df[pred_col].values
            
            row[f"{model_name}_rmse"] = np.sqrt(mean_squared_error(y_true, y_pred))
            row[f"{model_name}_mae"] = mean_absolute_error(y_true, y_pred)
            row[f"{model_name}_r2"] = r2_score(y_true, y_pred)
            
            # Calculate Pearson correlation
            pearson_corr, _ = pearsonr(y_true, y_pred)
            row[f"{model_name}_pearson"] = pearson_corr
        
        results.append(row)
    
    return pd.DataFrame(results)
===== FILE: flexseq/utils/visualization.py =====
"""
Visualization functions for the FlexSeq ML pipeline.

This module provides placeholder functions for generating visualization data
to be used by external visualization tools. The functions primarily save data
in CSV format that can be visualized separately.
"""

import os
import logging
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

from flexseq.utils.helpers import (
    get_temperature_color,
    make_model_color_map,
    ensure_dir
)

logger = logging.getLogger(__name__)

def save_plot(plt, output_path: str, dpi: int = 300) -> None:
    """
    Save a matplotlib plot to disk.
    
    Args:
        plt: Matplotlib pyplot instance
        output_path: Path to save the plot
        dpi: Resolution in dots per inch
    """
    # Create directory if it doesn't exist
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    
    # Save the plot
    plt.tight_layout()
    plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
    plt.close()

def plot_temperature_comparison(
    temperatures: List[Union[int, str]],
    metrics: pd.DataFrame,
    output_path: str
) -> None:
    """
    Generate a plot comparing metrics across temperatures.
    
    Args:
        temperatures: List of temperature values
        metrics: DataFrame with metrics for each temperature
        output_path: Path to save the plot
    """
    # Prepare the data for plotting
    metrics_data = []
    for temp in temperatures:
        if str(temp) in metrics.index.astype(str):
            row = metrics.loc[str(temp)]
            metrics_data.append({
                'temperature': temp,
                'rmse': row.get('rmse', np.nan),
                'r2': row.get('r2', np.nan),
                'pearson_correlation': row.get('pearson_correlation', np.nan)
            })
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Save the data as CSV for external visualization
    metrics_df = pd.DataFrame(metrics_data)
    metrics_df.to_csv(output_path, index=False)
    
    logger.info(f"Temperature comparison data saved to {output_path}")

def plot_amino_acid_performance(
    data: pd.DataFrame,
    temperature: Union[int, str],
    output_path: str
) -> None:
    """
    Generate amino acid-specific performance data.
    
    Args:
        data: DataFrame with predictions and errors by amino acid
        temperature: Temperature value for the data
        output_path: Path to save the data
    """
    # Group data by amino acid
    if 'resname' in data.columns:
        error_cols = [col for col in data.columns if col.endswith('_abs_error')]
        aa_performance = []
        
        for resname, group in data.groupby('resname'):
            row = {'resname': resname, 'count': len(group)}
            
            for error_col in error_cols:
                model_name = error_col.split('_abs_error')[0]
                row[f"{model_name}_mean_error"] = group[error_col].mean()
                row[f"{model_name}_median_error"] = group[error_col].median()
                row[f"{model_name}_std_error"] = group[error_col].std()
            
            aa_performance.append(row)
        
        # Create DataFrame and save to CSV
        aa_df = pd.DataFrame(aa_performance)
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        aa_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid performance data saved to {output_path}")

def plot_feature_importance(
    importances: Dict[str, float],
    feature_names: List[str],
    output_path: str
) -> None:
    """
    Generate feature importance visualization data.
    
    Args:
        importances: Dictionary mapping features to importance values
        feature_names: List of feature names
        output_path: Path to save the data
    """
    # Create DataFrame from importance dictionary
    importance_data = []
    
    for feature, importance in importances.items():
        importance_data.append({
            'feature': feature,
            'importance': importance
        })
    
    importance_df = pd.DataFrame(importance_data)
    importance_df = importance_df.sort_values('importance', ascending=False)
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Save to CSV
    importance_df.to_csv(output_path, index=False)
    logger.info(f"Feature importance data saved to {output_path}")
    
    # Create a simple bar plot of the top 15 features
    plt.figure(figsize=(10, 6))
    top_features = importance_df.head(15)
    sns.barplot(x='importance', y='feature', data=top_features)
    plt.title('Top 15 Feature Importances')
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"Feature importance plot saved to {plot_path}")

def plot_model_metrics_table(
    metrics: Dict[str, Dict[str, float]],
    config: Dict[str, Any]
) -> None:
    """
    Generate a table comparing metrics across models.
    
    Args:
        metrics: Dictionary mapping model names to metrics
        config: Configuration dictionary
    """
    # Create DataFrame from metrics dictionary
    metrics_data = []
    
    for model_name, model_metrics in metrics.items():
        row = {'model': model_name}
        row.update(model_metrics)
        metrics_data.append(row)
    
    metrics_df = pd.DataFrame(metrics_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "model_metrics_table.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    metrics_df.to_csv(output_path, index=False)
    logger.info(f"Model metrics table saved to {output_path}")

def plot_r2_comparison(
    predictions: Dict[str, np.ndarray],
    target_values: np.ndarray,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate R² comparison data across models.
    
    Args:
        predictions: Dictionary mapping model names to predictions
        target_values: True target values
        model_names: List of model names
        config: Configuration dictionary
    """
    from sklearn.metrics import r2_score
    
    # Calculate R² for each model
    r2_data = []
    
    for model_name in model_names:
        if model_name in predictions:
            r2 = r2_score(target_values, predictions[model_name])
            r2_data.append({
                'model': model_name,
                'r2': r2
            })
    
    r2_df = pd.DataFrame(r2_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "comparisons", "r2_comparison.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    r2_df.to_csv(output_path, index=False)
    logger.info(f"R² comparison data saved to {output_path}")
    
    # Create a simple bar plot
    plt.figure(figsize=(8, 5))
    sns.barplot(x='model', y='r2', data=r2_df)
    plt.title('R² Comparison Across Models')
    plt.ylim(0, 1)  # R² is typically between 0 and 1
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"R² comparison plot saved to {plot_path}")

def plot_residue_level_rmsf(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate residue-level RMSF comparison data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    # Sample a single domain for visualization
    domains = df['domain_id'].unique()
    if len(domains) > 0:
        # Select the domain with the most residues for better visualization
        domain_counts = df.groupby('domain_id').size()
        selected_domain = domain_counts.idxmax()
        
        domain_df = df[df['domain_id'] == selected_domain].copy()
        
        # Add predictions from each model
        for model_name in model_names:
            if model_name in predictions:
                domain_df[f"{model_name}_predicted"] = np.nan
                
                # Match predictions to domain rows
                for i, idx in enumerate(df.index):
                    if idx in domain_df.index:
                        domain_df.loc[idx, f"{model_name}_predicted"] = predictions[model_name][i]
        
        # Sort by residue ID
        domain_df = domain_df.sort_values('resid')
        
        # Select columns for output
        output_cols = ['domain_id', 'resid', 'resname', target_col]
        output_cols.extend([f"{model_name}_predicted" for model_name in model_names if model_name in predictions])
        
        if 'secondary_structure_encoded' in domain_df.columns:
            output_cols.append('secondary_structure_encoded')
        
        if 'core_exterior_encoded' in domain_df.columns:
            output_cols.append('core_exterior_encoded')
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"domain_{selected_domain}_rmsf.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        domain_df[output_cols].to_csv(output_path, index=False)
        logger.info(f"Residue-level RMSF data saved to {output_path}")
        
        # Create a simple line plot
        plt.figure(figsize=(12, 6))
        
        # Plot actual values
        plt.plot(domain_df['resid'], domain_df[target_col], 'k-', label='Actual', linewidth=2)
        
        # Plot predictions
        colors = make_model_color_map(model_names)
        for model_name in model_names:
            if model_name in predictions and f"{model_name}_predicted" in domain_df.columns:
                plt.plot(domain_df['resid'], domain_df[f"{model_name}_predicted"], 
                         label=model_name, color=colors.get(model_name), linewidth=1.5)
        
        plt.xlabel('Residue ID')
        plt.ylabel('RMSF')
        plt.title(f'RMSF Profile for Domain {selected_domain}')
        plt.legend()
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Residue-level RMSF plot saved to {plot_path}")

def plot_amino_acid_error_analysis(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid error analysis data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns:
        logger.warning("Residue name information not available for amino acid analysis")
        return
    
    # Calculate errors for each model
    df_with_preds = df.copy()
    
    for model_name in model_names:
        if model_name in predictions:
            df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
            df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
            df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
    
    # Group by amino acid
    aa_errors = []
    
    for resname, group in df_with_preds.groupby('resname'):
        row = {'resname': resname, 'count': len(group)}
        
        for model_name in model_names:
            if model_name in predictions:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
        
        aa_errors.append(row)
    
    aa_error_df = pd.DataFrame(aa_errors)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "residue_analysis", "amino_acid_errors.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    aa_error_df.to_csv(output_path, index=False)
    logger.info(f"Amino acid error analysis saved to {output_path}")
    
    # Create a simple bar plot for the model with the most predictions
    if model_names:
        model_name = model_names[0]  # Use first model
        
        if f"{model_name}_mean_error" in aa_error_df.columns:
            plt.figure(figsize=(10, 6))
            
            # Sort by error
            sorted_df = aa_error_df.sort_values(f"{model_name}_mean_error")
            
            # Plot
            sns.barplot(x='resname', y=f"{model_name}_mean_error", data=sorted_df)
            plt.title(f'Mean Absolute Error by Amino Acid Type ({model_name})')
            plt.xlabel('Amino Acid')
            plt.ylabel('Mean Absolute Error')
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + f"_{model_name}.png"
            save_plot(plt, plot_path)
            logger.info(f"Amino acid error plot saved to {plot_path}")

def plot_amino_acid_error_boxplot(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid error boxplot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns or not model_names:
        return
    
    # Calculate errors for the first model
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Create long-form data for boxplot
        error_data = []
        
        for _, row in df_with_preds.iterrows():
            error_data.append({
                'resname': row['resname'],
                'error': row[f"{model_name}_abs_error"]
            })
        
        error_df = pd.DataFrame(error_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"amino_acid_errors_boxplot_{model_name}.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        error_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid error boxplot data saved to {output_path}")
        
        # Create a boxplot
        plt.figure(figsize=(12, 6))
        
        # Get median errors for sorting
        median_errors = error_df.groupby('resname')['error'].median().sort_values()
        sorted_residues = median_errors.index.tolist()
        
        # Create boxplot with sorted residues
        sns.boxplot(x='resname', y='error', data=error_df, order=sorted_residues)
        plt.title(f'Absolute Error Distribution by Amino Acid Type ({model_name})')
        plt.xlabel('Amino Acid')
        plt.ylabel('Absolute Error')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Amino acid error boxplot saved to {plot_path}")

def plot_amino_acid_scatter_plot(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid scatter plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns or not model_names:
        return
    
    # Use the first model for scatter plot
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        
        # Create scatter plot data
        scatter_data = []
        
        for _, row in df_with_preds.iterrows():
            scatter_data.append({
                'resname': row['resname'],
                'actual': row[target_col],
                'predicted': row[f"{model_name}_predicted"]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"amino_acid_scatter_{model_name}.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid scatter plot data saved to {output_path}")
        
        # Create a scatter plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Create a colormap for amino acids
        unique_residues = sampled_df['resname'].unique()
        cmap = plt.cm.get_cmap('tab20', len(unique_residues))
        residue_to_color = {res: cmap(i) for i, res in enumerate(unique_residues)}
        colors = [residue_to_color[res] for res in sampled_df['resname']]
        
        # Plot
        plt.scatter(sampled_df['actual'], sampled_df['predicted'], c=colors, alpha=0.7, s=30)
        
        # Add diagonal line
        min_val = min(sampled_df['actual'].min(), sampled_df['predicted'].min())
        max_val = max(sampled_df['actual'].max(), sampled_df['predicted'].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--')
        
        plt.xlabel('Actual RMSF')
        plt.ylabel('Predicted RMSF')
        plt.title(f'Actual vs Predicted RMSF by Amino Acid Type ({model_name})')
        
        # Add legend with the most common amino acids (top 10)
        residue_counts = df['resname'].value_counts().head(10)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                          markerfacecolor=residue_to_color[res], markersize=10, label=res) 
                         for res in residue_counts.index]
        plt.legend(handles=legend_elements, title='Amino Acid')
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Amino acid scatter plot saved to {plot_path}")

def plot_error_analysis_by_property(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate error analysis by property data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names:
        return
    
    # Use the first model for analysis
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Properties to analyze
        properties = []
        
        if 'secondary_structure_encoded' in df_with_preds.columns:
            properties.append({
                'name': 'secondary_structure',
                'column': 'secondary_structure_encoded',
                'labels': {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other'}
            })
        
        if 'core_exterior_encoded' in df_with_preds.columns:
            properties.append({
                'name': 'surface_exposure',
                'column': 'core_exterior_encoded',
                'labels': {0: 'Core', 1: 'Surface'}
            })
        
        if 'normalized_resid' in df_with_preds.columns:
            # Create bins for normalized position
            df_with_preds['position_bin'] = pd.cut(
                df_with_preds['normalized_resid'], 
                bins=5, 
                labels=['N-term', 'N-quarter', 'Middle', 'C-quarter', 'C-term']
            )
            
            properties.append({
                'name': 'sequence_position',
                'column': 'position_bin',
                'labels': None  # Use the bin labels
            })
        
        # Analyze each property
        for prop in properties:
            property_data = []
            
            if prop['column'] in df_with_preds.columns:
                groupby_col = prop['column']
                
                for group_val, group in df_with_preds.groupby(groupby_col):
                    # Get label
                    if prop['labels'] is not None:
                        label = prop['labels'].get(group_val, str(group_val))
                    else:
                        label = str(group_val)
                    
                    # Calculate metrics
                    row = {
                        'property': prop['name'],
                        'value': label,
                        'count': len(group),
                        'mean_error': group[f"{model_name}_abs_error"].mean(),
                        'median_error': group[f"{model_name}_abs_error"].median(),
                        'std_error': group[f"{model_name}_abs_error"].std()
                    }
                    
                    property_data.append(row)
                
                # Create DataFrame
                prop_df = pd.DataFrame(property_data)
                
                # Save to CSV
                output_dir = config["paths"]["output_dir"]
                output_path = os.path.join(
                    output_dir, 
                    "residue_analysis", 
                    f"error_by_{prop['name']}_{model_name}.csv"
                )
                
                # Create output directory
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                prop_df.to_csv(output_path, index=False)
                logger.info(f"Error analysis by {prop['name']} saved to {output_path}")
                
                # Create a bar plot
                plt.figure(figsize=(8, 5))
                
                # Sort by value if not a categorical property
                if prop['name'] == 'sequence_position':
                    # Use the defined order for position bins
                    order = ['N-term', 'N-quarter', 'Middle', 'C-quarter', 'C-term']
                    sns.barplot(x='value', y='mean_error', data=prop_df, order=order)
                else:
                    sns.barplot(x='value', y='mean_error', data=prop_df)
                
                plt.title(f'Mean Absolute Error by {prop["name"].replace("_", " ").title()} ({model_name})')
                plt.xlabel(prop['name'].replace('_', ' ').title())
                plt.ylabel('Mean Absolute Error')
                plt.tight_layout()
                
                # Save the plot
                plot_path = os.path.splitext(output_path)[0] + '.png'
                save_plot(plt, plot_path)
                logger.info(f"Error analysis plot for {prop['name']} saved to {plot_path}")

def plot_r2_comparison_scatter(
    predictions: Dict[str, np.ndarray],
    target_values: np.ndarray,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate R² comparison scatter plot data.
    
    Args:
        predictions: Dictionary mapping model names to predictions
        target_values: True target values
        model_names: List of model names
        config: Configuration dictionary
    """
    if len(model_names) < 2:
        return
    
    # Get pairs of models
    model_pairs = []
    
    for i, model1 in enumerate(model_names):
        for model2 in model_names[i+1:]:
            if model1 in predictions and model2 in predictions:
                model_pairs.append((model1, model2))
    
    # Create scatter plot data for each pair
    for model1, model2 in model_pairs:
        scatter_data = []
        
        for i in range(len(target_values)):
            scatter_data.append({
                'actual': target_values[i],
                f"{model1}_predicted": predictions[model1][i],
                f"{model2}_predicted": predictions[model2][i]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "comparisons", 
            f"scatter_{model1}_vs_{model2}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Scatter plot data for {model1} vs {model2} saved to {output_path}")
        
        # Create a scatter plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Plot
        plt.scatter(
            sampled_df[f"{model1}_predicted"], 
            sampled_df[f"{model2}_predicted"], 
            c=sampled_df['actual'], 
            cmap='viridis', 
            alpha=0.7, 
            s=30
        )
        
        # Add diagonal line
        min_val = min(sampled_df[f"{model1}_predicted"].min(), sampled_df[f"{model2}_predicted"].min())
        max_val = max(sampled_df[f"{model1}_predicted"].max(), sampled_df[f"{model2}_predicted"].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--')
        
        plt.xlabel(f'{model1} Predicted')
        plt.ylabel(f'{model2} Predicted')
        plt.title(f'Prediction Comparison: {model1} vs {model2}')
        plt.colorbar(label='Actual RMSF')
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Scatter plot for {model1} vs {model2} saved to {plot_path}")

def plot_scatter_with_density_contours(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate scatter plot with density contours.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Create data for plotting
        scatter_data = []
        
        for i, idx in enumerate(df.index):
            scatter_data.append({
                'actual': df.loc[idx, target_col],
                'predicted': predictions[model_name][i]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "comparisons", 
            f"density_scatter_{model_name}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Density scatter plot data for {model_name} saved to {output_path}")
        
        # Create the plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Create plot with density contours
        sns.kdeplot(
            x='actual',
            y='predicted',
            data=sampled_df,
            fill=True,
            cmap='Blues',
            alpha=0.5,
            levels=10
        )
        
        plt.scatter(
            sampled_df['actual'],
            sampled_df['predicted'],
            alpha=0.3,
            s=20,
            c='darkblue'
        )
        
        # Add diagonal line
        min_val = min(sampled_df['actual'].min(), sampled_df['predicted'].min())
        max_val = max(sampled_df['actual'].max(), sampled_df['predicted'].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--')
        
        plt.xlabel('Actual RMSF')
        plt.ylabel('Predicted RMSF')
        plt.title(f'Actual vs Predicted RMSF with Density Contours ({model_name})')
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Density scatter plot for {model_name} saved to {plot_path}")

def plot_flexibility_vs_dihedral_angles(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate flexibility vs dihedral angles plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'phi_norm' not in df.columns or 'psi_norm' not in df.columns:
        return
    
    # Use first model
    model_name = model_names[0] if model_names else None
    
    # Prepare data for plotting
    plot_data = []
    
    for i, idx in enumerate(df.index):
        row = {
            'phi': df.loc[idx, 'phi_norm'],
            'psi': df.loc[idx, 'psi_norm'],
            'actual': df.loc[idx, target_col]
        }
        
        if model_name and model_name in predictions:
            row['predicted'] = predictions[model_name][i]
        
        plot_data.append(row)
    
    plot_df = pd.DataFrame(plot_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(
        output_dir, 
        "residue_analysis", 
        "flexibility_vs_dihedral_angles.csv"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    plot_df.to_csv(output_path, index=False)
    logger.info(f"Flexibility vs dihedral angles data saved to {output_path}")
    
    # Create the plot
    plt.figure(figsize=(10, 8))
    
    # Sample up to 5000 points for better visibility
    sample_size = min(5000, len(plot_df))
    sampled_df = plot_df.sample(sample_size, random_state=config["system"]["random_state"])
    
    # Create heatmap scatter plot
    plt.scatter(
        sampled_df['phi'],
        sampled_df['psi'],
        c=sampled_df['actual'],
        cmap='viridis',
        alpha=0.7,
        s=30
    )
    
    plt.xlabel('Normalized Phi Angle')
    plt.ylabel('Normalized Psi Angle')
    plt.title('Protein Flexibility in Dihedral Angle Space')
    plt.colorbar(label='RMSF')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"Flexibility vs dihedral angles plot saved to {plot_path}")

def plot_flexibility_sequence_neighborhood(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate flexibility vs sequence neighborhood plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    # Check if we have window features
    window_cols = [col for col in df.columns if '_offset_' in col]
    
    if not window_cols:
        return
    
    # Find a specific domain with good data
    domains = df['domain_id'].unique()
    
    if len(domains) > 0:
        # Select a domain with at least 50 residues
        domain_sizes = df.groupby('domain_id').size()
        valid_domains = domain_sizes[domain_sizes >= 50].index
        
        if len(valid_domains) > 0:
            selected_domain = valid_domains[0]
            domain_df = df[df['domain_id'] == selected_domain].copy()
            
            # Sort by residue ID
            domain_df = domain_df.sort_values('resid')
            
            # Select a window around a highly flexible residue
            flexible_idx = domain_df[target_col].idxmax()
            flexible_resid = domain_df.loc[flexible_idx, 'resid']
            
            # Select residues within 10 positions
            window_size = 10
            min_resid = max(0, flexible_resid - window_size)
            max_resid = flexible_resid + window_size
            
            window_df = domain_df[
                (domain_df['resid'] >= min_resid) & 
                (domain_df['resid'] <= max_resid)
            ].copy()
            
            # Add predictions if available
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                window_df[f"{model_name}_predicted"] = np.nan
                
                # Match predictions to domain rows
                for i, idx in enumerate(df.index):
                    if idx in window_df.index:
                        window_df.loc[idx, f"{model_name}_predicted"] = predictions[model_name][i]
            
            # Save to CSV
            output_dir = config["paths"]["output_dir"]
            output_path = os.path.join(
                output_dir, 
                "residue_analysis", 
                f"sequence_neighborhood_domain_{selected_domain}.csv"
            )
            
            # Create output directory
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # Select relevant columns
            relevant_cols = ['domain_id', 'resid', 'resname', target_col, 'secondary_structure_encoded']
            
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                if f"{model_name}_predicted" in window_df.columns:
                    relevant_cols.append(f"{model_name}_predicted")
            
            window_df[relevant_cols].to_csv(output_path, index=False)
            logger.info(f"Sequence neighborhood data saved to {output_path}")
            
            # Create the plot
            plt.figure(figsize=(10, 6))
            
            # Plot actual values
            plt.plot(
                window_df['resid'], 
                window_df[target_col], 
                'k-', 
                linewidth=2, 
                label='Actual'
            )
            
            # Plot predictions if available
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                if f"{model_name}_predicted" in window_df.columns:
                    plt.plot(
                        window_df['resid'], 
                        window_df[f"{model_name}_predicted"], 
                        'r--', 
                        linewidth=1.5, 
                        label='Predicted'
                    )
            
            # Add secondary structure if available
            if 'secondary_structure_encoded' in window_df.columns:
                # Create secondary structure bars at the bottom
                for i, row in window_df.iterrows():
                    ss = row['secondary_structure_encoded']
                    resid = row['resid']
                    
                    if ss == 0:  # Helix
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='red', ymin=0, ymax=0.05)
                    elif ss == 1:  # Sheet
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='blue', ymin=0, ymax=0.05)
                    else:  # Loop/Other
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='green', ymin=0, ymax=0.05)
                
                # Add legend for secondary structure
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='red', alpha=0.2, label='Helix'),
                    Patch(facecolor='blue', alpha=0.2, label='Sheet'),
                    Patch(facecolor='green', alpha=0.2, label='Loop/Other')
                ]
                
                plt.legend(handles=legend_elements, loc='upper right')
            
            plt.xlabel('Residue ID')
            plt.ylabel('RMSF')
            plt.title(f'Flexibility in Sequence Neighborhood (Domain {selected_domain})')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + '.png'
            save_plot(plt, plot_path)
            logger.info(f"Sequence neighborhood plot saved to {plot_path}")

def plot_error_response_surface(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate error response surface plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names or 'normalized_resid' not in df.columns:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Calculate errors
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Create bins for normalized position and secondary structure
        if 'secondary_structure_encoded' in df_with_preds.columns:
            # Create data for heatmap
            heatmap_data = []
            
            # Create position bins
            df_with_preds['position_bin'] = pd.cut(
                df_with_preds['normalized_resid'], 
                bins=10, 
                labels=range(10)
            )
            
            # Group by position bin and secondary structure
            grouped = df_with_preds.groupby(['position_bin', 'secondary_structure_encoded'])
            
            for (pos_bin, ss), group in grouped:
                heatmap_data.append({
                    'position_bin': pos_bin,
                    'secondary_structure': ss,
                    'mean_error': group[f"{model_name}_abs_error"].mean(),
                    'count': len(group)
                })
            
            heatmap_df = pd.DataFrame(heatmap_data)
            
            # Save to CSV
            output_dir = config["paths"]["output_dir"]
            output_path = os.path.join(
                output_dir, 
                "residue_analysis", 
                f"error_response_surface_{model_name}.csv"
            )
            
            # Create output directory
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            heatmap_df.to_csv(output_path, index=False)
            logger.info(f"Error response surface data saved to {output_path}")
            
            # Create a pivot table for the heatmap
            pivot_df = heatmap_df.pivot(
                index='secondary_structure', 
                columns='position_bin', 
                values='mean_error'
            )
            
            # Create the plot
            plt.figure(figsize=(10, 6))
            
            # Create heatmap
            sns.heatmap(
                pivot_df, 
                cmap='viridis', 
                annot=True, 
                fmt=".2f", 
                linewidths=0.5
            )
            
            # Set labels
            plt.xlabel('Normalized Position Bin')
            plt.ylabel('Secondary Structure (0=Helix, 1=Sheet, 2=Loop)')
            plt.title(f'Error Response Surface: Position vs Structure ({model_name})')
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + '.png'
            save_plot(plt, plot_path)
            logger.info(f"Error response surface plot saved to {plot_path}")

def plot_secondary_structure_error_correlation(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate secondary structure error correlation plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'secondary_structure_encoded' not in df.columns or not model_names:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Calculate errors
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Group by secondary structure
        ss_errors = []
        
        for ss, group in df_with_preds.groupby('secondary_structure_encoded'):
            ss_name = {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other'}.get(ss, str(ss))
            
            # Calculate metrics
            row = {
                'secondary_structure': ss_name,
                'count': len(group),
                'mean_actual': group[target_col].mean(),
                'mean_predicted': group[f"{model_name}_predicted"].mean(),
                'mean_error': group[f"{model_name}_abs_error"].mean(),
                'std_error': group[f"{model_name}_abs_error"].std()
            }
            
            ss_errors.append(row)
        
        ss_error_df = pd.DataFrame(ss_errors)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "residue_analysis", 
            f"secondary_structure_errors_{model_name}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        ss_error_df.to_csv(output_path, index=False)
        logger.info(f"Secondary structure error correlation data saved to {output_path}")
        
        # Create the plot
        plt.figure(figsize=(10, 6))
        
        # Create grouped bar plot
        x = np.arange(len(ss_error_df))
        width = 0.35
        
        plt.bar(
            x - width/2, 
            ss_error_df['mean_actual'], 
            width, 
            label='Actual RMSF'
        )
        
        plt.bar(
            x + width/2, 
            ss_error_df['mean_predicted'], 
            width, 
            label='Predicted RMSF'
        )
        
        plt.xlabel('Secondary Structure')
        plt.ylabel('Mean RMSF')
        plt.title(f'Actual vs Predicted RMSF by Secondary Structure ({model_name})')
        plt.xticks(x, ss_error_df['secondary_structure'])
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Secondary structure comparison plot saved to {plot_path}")

def plot_training_validation_curves(
    train_metrics: Dict[str, List[float]],
    val_metrics: Dict[str, List[float]],
    model_name: str,
    config: Dict[str, Any]
) -> None:
    """
    Generate training and validation curves.
    
    Args:
        train_metrics: Dictionary of training metrics by epoch
        val_metrics: Dictionary of validation metrics by epoch
        model_name: Name of the model
        config: Configuration dictionary
    """
    if not train_metrics or not val_metrics:
        return
    
    # Convert to DataFrame
    epochs = len(train_metrics.get('train_loss', []))
    
    if epochs == 0:
        return
    
    curve_data = []
    
    for i in range(epochs):
        row = {'epoch': i}
        
        for metric, values in train_metrics.items():
            if i < len(values):
                metric_name = metric.replace('train_', '')
                row[f"train_{metric_name}"] = values[i]
        
        for metric, values in val_metrics.items():
            if i < len(values):
                metric_name = metric.replace('val_', '')
                row[f"val_{metric_name}"] = values[i]
        
        curve_data.append(row)
    
    curve_df = pd.DataFrame(curve_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(
        output_dir, 
        "training_performance", 
        f"{model_name}_training_curves.csv"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    curve_df.to_csv(output_path, index=False)
    logger.info(f"Training and validation curves data saved to {output_path}")
    
    # Create the plots
    metrics_to_plot = []
    
    if 'train_loss' in train_metrics and 'val_loss' in val_metrics:
        metrics_to_plot.append(('loss', 'Loss'))
    
    if 'train_r2' in train_metrics and 'val_r2' in val_metrics:
        metrics_to_plot.append(('r2', 'R²'))
    
    for metric, title in metrics_to_plot:
        plt.figure(figsize=(10, 6))
        
        plt.plot(
            curve_df['epoch'], 
            curve_df[f"train_{metric}"], 
            'b-', 
            label=f'Training {title}'
        )
        
        plt.plot(
            curve_df['epoch'], 
            curve_df[f"val_{metric}"], 
            'r-', 
            label=f'Validation {title}'
        )
        
        plt.xlabel('Epoch')
        plt.ylabel(title)
        plt.title(f'{title} Curves for {model_name}')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.join(
            output_dir, 
            "training_performance", 
            f"{model_name}_{metric}_curves.png"
        )
        
        save_plot(plt, plot_path)
        logger.info(f"{title} curves plot saved to {plot_path}")

def plot_neural_network_learning_dynamics(
    train_metrics: Dict[str, List[float]],
    val_metrics: Dict[str, List[float]],
    model_name: str,
    config: Dict[str, Any]
) -> None:
    """
    Generate neural network learning dynamics visualization.
    
    Args:
        train_metrics: Dictionary of training metrics by epoch
        val_metrics: Dictionary of validation metrics by epoch
        model_name: Name of the model
        config: Configuration dictionary
    """
    if not train_metrics or not val_metrics:
        return
    
    # Convert to DataFrame
    epochs = len(train_metrics.get('train_loss', []))
    
    if epochs == 0:
        return
    
    # Create a combined plot with multiple metrics
    plt.figure(figsize=(12, 8))
    
    # Create subplots
    if 'train_loss' in train_metrics and 'val_loss' in val_metrics:
        ax1 = plt.subplot(2, 1, 1)
        
        # Plot loss
        ax1.plot(
            range(epochs), 
            train_metrics['train_loss'], 
            'b-', 
            label='Training Loss'
        )
        
        ax1.plot(
            range(epochs), 
            val_metrics['val_loss'], 
            'r-', 
            label='Validation Loss'
        )
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title(f'Loss Curves for {model_name}')
        ax1.legend()
        ax1.grid(alpha=0.3)
    
    if 'train_r2' in train_metrics and 'val_r2' in val_metrics:
        ax2 = plt.subplot(2, 1, 2)
        
        # Plot R²
        ax2.plot(
            range(epochs), 
            train_metrics['train_r2'], 
            'g-', 
            label='Training R²'
        )
        
        ax2.plot(
            range(epochs), 
            val_metrics['val_r2'], 
            'y-', 
            label='Validation R²'
        )
        
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('R²')
        ax2.set_title(f'R² Curves for {model_name}')
        ax2.legend()
        ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    
    # Save the plot
    output_dir = config["paths"]["output_dir"]
    plot_path = os.path.join(
        output_dir, 
        "training_performance", 
        f"{model_name}_learning_dynamics.png"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    
    save_plot(plt, plot_path)
    logger.info(f"Learning dynamics plot saved to {plot_path}")
==========================================================
End of FlexSeq Context Document
==========================================================
