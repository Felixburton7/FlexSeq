==========================================================
                FlexSeq: Protein Flexibility ML Pipeline
==========================================================

==========================================================
Example Input Data Format
==========================================================
FlexSeq expects temperature-specific CSV files in the data directory.
Example file name format: 'temperature_320_train.csv'

Expected columns in the CSV:
- domain_id: Protein domain identifier (e.g., '1a0aA00')
- resid: Residue ID (integer position in the protein chain)
- resname: Amino acid type (e.g., ALA, LYS, etc.)
- rmsf_{temperature}: Target RMSF value at the specified temperature
- protein_size: Total number of residues in the protein
- normalized_resid: Position normalized to 0-1 range
- core_exterior: Location classification ('interior' or 'surface')
- relative_accessibility: Solvent accessibility measure (0-1)
- dssp: Secondary structure annotation (H=helix, E=sheet, C=coil, etc.)
- phi, psi: Backbone dihedral angles
- *_encoded: Various numerically encoded categorical features
- phi_norm, psi_norm: Normalized dihedral angles to [-1, 1] range

For OmniFlex mode, additional columns:
- esm_rmsf: Predictions from ESM embeddings
- voxel_rmsf: Predictions from 3D voxel representation

==========================================================
Usage Examples
==========================================================
# Training a model at a specific temperature
flexseq train --temperature 320

# Training models on all available temperatures
flexseq train-all-temps

# Evaluate a trained model
flexseq evaluate --model random_forest --temperature 320

# Generate predictions using the best model
flexseq predict --input new_proteins.csv --temperature 320

# Compare results across temperatures
flexseq compare-temperatures

# Use OmniFlex mode with advanced features
flexseq train --mode omniflex --temperature 320

Project Working Directory: /home/s_felix/flexseq

Project Tree Structure:
---------------------------------------------------------
.
./flexseq
./flexseq/data
./flexseq.egg-info
./flexseq/models
./flexseq/temperature
./flexseq/utils

File Listing (excluding cache, data, output, and test directories):
---------------------------------------------------------
./data
./default_config.yaml
./flexseq/cli.py
./flexseq/config.py
./flexseq/data/__init__.py
./flexseq/data/loader.py
./flexseq/data/processor.py
./flexseq/data/__pycache__
./flexseq/__init__.py
./flexseq/models/base.py
./flexseq/models/__init__.py
./flexseq/models/neural_network.py
./flexseq/models/__pycache__
./flexseq/models/random_forest.py
./flexseq/pipeline.py
./flexseq/__pycache__
./flexseq/temperature/comparison.py
./flexseq/temperature/__init__.py
./flexseq/temperature/__pycache__
./flexseq/utils/helpers.py
./flexseq/utils/__init__.py
./flexseq/utils/metrics.py
./flexseq/utils/__pycache__
./flexseq/utils/temperature_comparison_visualizer.py
./flexseq/utils/visualization.py
./.git
./maxed_config.yaml
./models
./output
./pyproject.toml
./setup.py
./test

==========================================================
Default Configuration (default_config.yaml)
==========================================================
# # FlexSeq Configuration


# FlexSeq Configuration with Maxed Specifications

# Paths
paths:
  data_dir: ./data               # Data directory
  output_dir: ./output            # Output directory
  models_dir: ./models            # Saved models directory

# Mode configuration
mode:
  active: "omniflex"              # Using advanced mode
  omniflex:
    use_esm: true                 # Use ESM embeddings feature
    use_voxel: true               # Enable 3D voxel feature

# Temperature configuration
temperature:
  current: 320                    # Current temperature to process
  available: [320, 348, 379, 413, 450, "average"]
  comparison:
    enabled: true                 # Generate temperature comparisons
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]

# Dataset configuration
dataset:
  # Data loading
  file_pattern: "temperature_{temperature}_train.csv"
  
  # Domain filtering
  domains:
    include: []                   # Empty means include all domains
    exclude: []                   # Domains to exclude
    min_protein_size: 0           # Minimum protein size
    max_protein_size: null        # Maximum protein size (null = no limit)
  
  # Feature configuration
  features:
    # Required columns that must exist in data
    required:
      - domain_id                 # Domain identifier
      - resid                     # Residue ID
      - resname                   # Residue name
      - rmsf_{temperature}        # Target variable
    
    # Input features with toggles - all enabled
    use_features:
      protein_size: true          # Size of protein
      normalized_resid: true      # Position in sequence
      relative_accessibility: true # Solvent accessibility
      core_exterior_encoded: true # Core or exterior
      secondary_structure_encoded: true # Secondary structure
      phi_norm: true              # Normalized phi angle
      psi_norm: true              # Normalized psi angle
      resname_encoded: true       # Encoded residue name
      esm_rmsf: true              # ESM embeddings prediction (OmniFlex only)
      voxel_rmsf: false            # 3D voxel prediction (OmniFlex only)
    
    # Enhanced feature engineering
    window:
      enabled: true               # Use window-based features
      size: 10                     # Increased window size for better context

  # Target variable
  target: rmsf_{temperature}      # Templated with current temperature
  
  # Data splitting - standard split is fine
  split:
    test_size: 0.2                # Test set size
    validation_size: 0.15         # Validation set size
    stratify_by_domain: true      # Keep domains together
    random_state: 42              # Random seed

# Evaluation settings
evaluation:
  comparison_set: "test"          # Which set to use: "validation" or "test"
  metrics:
    rmse: true                    # Root Mean Squared Error
    mae: true                     # Mean Absolute Error
    r2: true                      # R-squared
    pearson_correlation: true     # Pearson correlation
    spearman_correlation: true    # Spearman rank correlation
    root_mean_square_absolute_error: true  # Root Mean Square Absolute Error

# Model configurations
models:
  # Shared settings
  common:
    cross_validation:
      enabled: true               # Enable cross-validation for better validation
      folds: 5                    # Number of folds if enabled
    save_best: true               # Save best model
  
  # Neural Network - enhanced architecture and training
  neural_network:
    enabled: true                 # Run this model
    architecture:
      hidden_layers: [256, 128, 64]  # Larger network
      activation: relu            # Activation function
      dropout: 0.3                # Increased dropout for better generalization
    training:
      optimizer: adam             # Optimizer
      learning_rate: 0.001        # Learning rate
      batch_size: 64              # Increased batch size
      epochs: 20                 # Increased max epochs
      early_stopping: true        # Use early stopping
      patience: 5                # Increased patience
    hyperparameter_optimization:
      enabled: false               # Enable hyperparameter optimization
      method: "random"          # Better optimization method
      trials: 5                  # More trials
      parameters:                 # Enhanced parameter space
        hidden_layers:
          - [64, 32]
          - [128, 64]
          - [256, 128]
          - [512, 256, 128]
          - [256, 128, 64, 32]
          - [128, 128, 64]
        learning_rate: [0.01, 0.005, 0.001, 0.0005, 0.0001]
        batch_size: [32, 64, 128]
        dropout: [0.1, 0.2, 0.3, 0.4, 0.5]
        activation: ["relu", "leaky_relu", "tanh"]
  
  # Random Forest - enhanced model
  random_forest:
    enabled: true                 # Run this model
    n_estimators: 500             # Increased number of trees
    max_depth: null               # Max tree depth
    min_samples_split: 2          # Min samples to split
    min_samples_leaf: 1           # Min samples in leaf
    max_features: 0.7             # Feature fraction
    bootstrap: true               # Use bootstrapping
    randomized_search:
      enabled: true               # Enable RandomizedSearchCV
      n_iter: 3                  # More parameter combinations to try
      cv: 2                       # Increased cross-validation folds
      param_distributions:        # Enhanced parameter distributions to search
        n_estimators: [100, 200, 300, 500, 800]
        max_depth: [null, 15, 30, 50, 100]
        min_samples_split: [2, 3, 5, 8, 10]
        min_samples_leaf: [1, 2, 3, 4, 5]
        max_features: ["auto", "sqrt", "log2", 0.5, 0.7, 0.9]
        bootstrap: [true, false]

# Analysis and visualization
analysis:
  feature_importance: 
    enabled: true                 # Analyze feature importance
    method: "permutation"         # Use permutation importance
    n_repeats: 20                 # Increased permutation repetitions
    use_validation_data: true     # Use validation data for importance calculation
  
  temperature_comparison:
    enabled: true                 # Compare results across temperatures
    metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]
    plots:
      histogram: true             # Generate histogram plots
      correlation: true           # Generate correlation plots
      performance: true           # Generate performance comparison plots

# System settings
system:
  n_jobs: -1                      # Use all available cores
  random_state: 42                # Global random seed
  log_level: INFO                 # Logging level
  gpu_enabled: auto               # Auto-detect GPU

# # Paths
# paths:
#   data_dir: ./data                # Data directory
#   output_dir: ./output            # Output directory
#   models_dir: ./models            # Saved models directory

# # Mode configuration
# mode:
#   active: "omniflex"               # "flexseq" or "omniflex"
#   omniflex:
#     use_esm: true                 # Use ESM embeddings feature
#     use_voxel: false               # Use 3D voxel feature

# # Temperature configuration
# temperature:
#   current: 348                    # Current temperature to process
#   available: [320, 348, 379, 413, 450, "average"]
#   comparison:
#     enabled: true                 # Generate temperature comparisons
#     metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]

# # Dataset configuration
# dataset:
#   # Data loading
#   file_pattern: "temperature_{temperature}_train.csv"
  
#   # Domain filtering
#   domains:
#     include: []                   # Empty means include all domains
#     exclude: []                   # Domains to exclude
#     min_protein_size: 0           # Minimum protein size
#     max_protein_size: null        # Maximum protein size (null = no limit)
  
#   # Feature configuration
#   features:
#     # Required columns that must exist in data
#     required:
#       - domain_id                 # Domain identifier
#       - resid                     # Residue ID
#       - resname                   # Residue name
#       - rmsf_{temperature}        # Target variable
    
#     # Input features with toggles
#     use_features:
#       protein_size: true          # Size of protein
#       normalized_resid: true      # Position in sequence
#       relative_accessibility: true # Solvent accessibility
#       core_exterior_encoded: true # Core or exterior
#       secondary_structure_encoded: true # Secondary structure
#       phi_norm: true              # Normalized phi angle
#       psi_norm: true              # Normalized psi angle
#       resname_encoded: true       # Encoded residue name
#       esm_rmsf: false             # ESM embeddings prediction (OmniFlex only)
#       voxel_rmsf: false           # 3D voxel prediction (OmniFlex only)
    
#     # Feature engineering
#     window:
#       enabled: true               # Use window-based features
#       size: 3                     # Window size (residues on each side)
  
#   # Target variable
#   target: rmsf_{temperature}      # Templated with current temperature
  
#   # Data splitting
#   split:
#     test_size: 0.2                # Test set size
#     validation_size: 0.15         # Validation set size
#     stratify_by_domain: true      # Keep domains together
#     random_state: 42              # Random seed

# # Evaluation settings
# evaluation:
#   comparison_set: "test"          # Which set to use: "validation" or "test"
#   metrics:
#     rmse: true                    # Root Mean Squared Error
#     mae: true                     # Mean Absolute Error
#     r2: true                      # R-squared
#     pearson_correlation: true     # Pearson correlation
#     spearman_correlation: true    # Spearman rank correlation
#     root_mean_square_absolute_error: true  # Root Mean Square Absolute Error

# # Model configurations
# models:
#   # Shared settings
#   common:
#     cross_validation:
#       enabled: false              # Whether to use CV
#       folds: 5                    # Number of folds if enabled
#     save_best: true               # Save best model
  
#   # Neural Network
#   neural_network:
#     enabled: true                 # Run this model
#     architecture:
#       hidden_layers: [64, 32]     # Layer sizes
#       activation: relu            # Activation function
#       dropout: 0.2                # Dropout rate
#     training:
#       optimizer: adam             # Optimizer
#       learning_rate: 0.001        # Learning rate
#       batch_size: 32              # Batch size
#       epochs: 100                 # Max epochs
#       early_stopping: true        # Use early stopping
#       patience: 10                # Early stopping patience
#     hyperparameter_optimization:
#       enabled: true              # Enable hyperparameter optimization
#       method: "random"          # "grid", "random", or "bayesian"
#       trials: 10                  # Number of trials
#       parameters:                 # Parameters to optimize
#         hidden_layers:
#           - [32, 16]
#           - [64, 32]
#           - [128, 64]
#           - [64, 32, 16]
#         learning_rate: [0.01, 0.001, 0.0001]
#         batch_size: [16, 32, 64]
#         dropout: [0.1, 0.2, 0.3, 0.5]
#         activation: ["relu", "leaky_relu"]
  
#   # Random Forest
#   random_forest:
#     enabled: true                 # Run this model
#     n_estimators: 100             # Number of trees
#     max_depth: null               # Max tree depth
#     min_samples_split: 2          # Min samples to split
#     min_samples_leaf: 1           # Min samples in leaf
#     max_features: 0.7             # Feature fraction
#     bootstrap: true               # Use bootstrapping
#     randomized_search:
#       enabled: true              # Enable RandomizedSearchCV
#       n_iter: 20                  # Number of parameter combinations to try
#       cv: 3                       # Number of cross-validation folds
#       param_distributions:        # Parameter distributions to search
#         n_estimators: [50, 100, 200, 300]
#         max_depth: [null, 10, 20, 30]
#         min_samples_split: [2, 5, 10]
#         min_samples_leaf: [1, 2, 4]
#         max_features: ["auto", "sqrt", "log2", 0.7]
#         bootstrap: [true, false]

# # Analysis and visualization
# # Analysis and visualization
# analysis:
#   feature_importance: 
#     enabled: true                 # Analyze feature importance
#     method: "permutation"         # Use permutation importance
#     n_repeats: 10                 # Number of permutation repetitions
#     use_validation_data: true     # Use validation data for importance calculation
  
#   temperature_comparison:
#     enabled: true                 # Compare results across temperatures
#     metrics: ["rmse", "r2", "pearson_correlation", "root_mean_square_absolute_error"]
#     plots:
#       histogram: true             # Generate histogram plots
#       correlation: true           # Generate correlation plots
#       performance: true           # Generate performance comparison plots

# # System settings
# system:
#   n_jobs: -1                      # Number of parallel jobs
#   random_state: 42                # Global random seed
#   log_level: INFO                 # Logging level
#   gpu_enabled: auto               # Auto-detect GPU
==========================================================
FlexSeq Package Files
==========================================================
### Main Package Files ###
---------------------------------------------------------
===== FILE: pyproject.toml =====
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "flexseq"
version = "0.1.0"
description = "ML pipeline for protein flexibility prediction with multi-temperature analysis"
readme = "README.md"
authors = [
  { name = "Felix Burton", email = "felixburton2002@gmail.com" }
]
requires-python = ">=3.8"
classifiers = [
  "Programming Language :: Python :: 3",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Topic :: Scientific/Engineering :: Bio-Informatics",
  "Topic :: Scientific/Engineering :: Artificial Intelligence"
]
dependencies = [
  "numpy>=1.20.0",
  "pandas>=1.3.0",
  "scikit-learn>=1.0.0",
  "torch>=1.9.0",
  "pyyaml>=6.0",
  "click>=8.0.0",
  "matplotlib>=3.4.0",
  "seaborn>=0.11.0",
  "joblib>=1.0.0",
  "tqdm>=4.64.0",
  "optuna>=3.0.0"
]

[project.scripts]
flexseq = "flexseq.cli:cli"

[project.urls]
Homepage = "https://github.com/Felixburton7/flexseq"
"Bug Tracker" = "https://github.com/Felixburton7/flexseq/issues"
===== FILE: setup.py =====
import os
from setuptools import setup, find_packages

# Read the content of README.md
this_directory = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(this_directory, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

setup(
    name="flexseq",
    version="0.1.0",
    description="ML pipeline for protein flexibility prediction with multi-temperature analysis",
    long_description=long_description,
    long_description_content_type="text/markdown",
    author="Felix Burton",
    author_email="felixburton2002@gmail.comcom",
    url="https://github.com/Felixburton7/flexseq",
    packages=find_packages(),
    include_package_data=True,
    install_requires=[
        "tqdm>=4.64.0",
        "numpy>=1.20.0",
        "pandas>=1.3.0",
        "scikit-learn>=1.0.0",
        "torch>=1.9.0",
        "pyyaml>=6.0",
        "click>=8.0.0",
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "joblib>=1.0.0",
        "optuna>=3.0.0",
    ],
    entry_points={
        "console_scripts": [
            "flexseq=flexseq.cli:cli",
        ],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Bio-Informatics",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires=">=3.8",
)
===== FILE: README.md =====
Okay, here is the revised `README.md` file, incorporating the accurate content from the project context and documentation into the desired style.

```markdown
# FlexSeq: Protein Flexibility Prediction Pipeline 🧬🔍

<div align="center">

<img src="https://via.placeholder.com/150x150/4B0082/FFFFFF?text=FlexSeq" alt="FlexSeq Logo" width="150"/>

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen?style=for-the-badge)](CONTRIBUTING.md)
[![GitHub Repo](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Felixburton7/flexseq)

**A comprehensive machine learning pipeline for predicting protein flexibility (RMSF) across multiple temperatures using sequence and structural features.**

[📊 Key Features](#key-features) •
[🔧 Installation](#installation) •
[🚀 Quick Start](#quick-start) •
[🔄 Pipeline Overview](#pipeline-overview) •
[📥 Input Data](#input-data) •
[📤 Output Data](#output-data) •
[🤖 Models](#models) •
[📈 Analysis & Visualization](#analysis--visualization) •
[⚙️ Configuration](#configuration) •
[💻 Command-Line Interface](#command-line-interface) •
[📚 Documentation](#documentation) •
[🤝 Contributing](#contributing)

</div>

## 🌟 Overview

FlexSeq is a machine learning pipeline meticulously designed for predicting protein flexibility, quantified as Root Mean Square Fluctuation (RMSF), based on protein sequence and structural features. A core capability of FlexSeq is its robust support for analyzing and comparing flexibility across a range of user-defined temperatures (e.g., 320K, 348K, 379K, 413K, 450K, and an averaged dataset), enabling the study of temperature-dependent dynamic behavior.

The pipeline offers two distinct operational modes, configurable via the `mode.active` setting:

-   **🔬 FlexSeq Mode**: The standard operational mode, utilizing a rich set of features derived directly from protein sequence and basic structural properties (e.g., protein size, residue position, solvent accessibility, secondary structure classification from DSSP, backbone dihedral angles φ/ψ).
-   **🔭 OmniFlex Mode**: An enhanced prediction mode that leverages the standard features *plus* pre-computed RMSF predictions derived from external, powerful models like ESM (Evolutionary Scale Modeling) embeddings (`esm_rmsf`) and potentially 3D voxel representations (`voxel_rmsf`), aiming for improved predictive accuracy.

FlexSeq employs a modular and configurable architecture, built upon Python libraries like Pandas, Scikit-learn, PyTorch, and Optuna. It features:
*   Configurable machine learning models (Random Forest and Neural Network).
*   Automated feature engineering, including sequence-window features and numerical encoding.
*   Comprehensive model evaluation using a suite of standard regression metrics.
*   Integrated hyperparameter optimization using Randomized Search (RF) or Optuna (NN).
*   Tools for systematic comparison of results across different temperatures.
*   Uncertainty estimation capabilities for model predictions.
*   A flexible and user-friendly Command-Line Interface (CLI) powered by Click.

## 📊 Key Features

<table>
<thead>
  <tr bgcolor="#6236FF">
    <th width="200"><span style="color:white">Feature</span></th>
    <th><span style="color:white">Description</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>🌡️ **Multi-Temperature Analysis**</td>
    <td>Train models, evaluate performance, and compare RMSF predictions across a user-defined list of temperatures (e.g., `[320, 348, 379, 413, 450, "average"]`).</td>
  </tr>
  <tr>
    <td>🤖 **Multiple ML Models**</td>
    <td>Includes Random Forest (Scikit-learn) and Feed-Forward Neural Network (PyTorch) implementations. The architecture allows for easy addition of new models inheriting from `BaseModel`.</td>
  </tr>
  <tr>
    <td>⚙️ **Feature Engineering**</td>
    <td>Automatic encoding of categorical features (residue name, core/exterior, secondary structure), normalization of angles (φ/ψ), calculation of normalized residue position, and optional generation of window-based features using neighboring residue information.</td>
  </tr>
  <tr>
    <td>🔬 **OmniFlex Mode**</td>
    <td>Optionally incorporates external predictions (`esm_rmsf`, `voxel_rmsf`) as input features for potentially enhanced performance. Enabled via configuration.</td>
  </tr>
  <tr>
    <td>⚠️ **Uncertainty Quantification**</td>
    <td>Models provide uncertainty estimates: standard deviation across trees for Random Forest, Monte Carlo Dropout sampling for Neural Network.</td>
  </tr>
  <tr>
    <td>📏 **Comprehensive Evaluation**</td>
    <td>Utilizes multiple metrics including RMSE, MAE, R², Pearson correlation, Spearman correlation, and Root Mean Square Absolute Error (RMSAE). Metrics are configurable.</td>
  </tr>
  <tr>
    <td>📊 **Analysis & Visualization**</td>
    <td>Generates detailed output CSV files for evaluation metrics, domain-level performance, residue-level errors (by AA type, position, structure), feature importance, and cross-temperature comparisons, suitable for external visualization tools. Also generates basic plots (e.g., feature importance).</td>
  </tr>
   <tr>
    <td>🧩 **Domain Stratification**</td>
    <td>Supports data splitting (`train`/`validation`/`test`) that ensures all residues from a given protein domain are kept within the same split, preventing data leakage between sets.</td>
  </tr>
  <tr>
    <td>🎯 **Hyperparameter Optimization**</td>
    <td>Automated tuning for both models using Scikit-learn's `RandomizedSearchCV` for Random Forest and Optuna (supporting random search and Bayesian optimization) for Neural Network. Configuration allows defining search spaces and trials.</td>
  </tr>
  <tr>
    <td>💻 **Command-Line Interface**</td>
    <td>Provides the `flexseq` command with subcommands (`train`, `evaluate`, `predict`, `run`, `compare-temperatures`, etc.) for easy pipeline execution and control.</td>
  </tr>
   <tr>
    <td>⚙️ **Configuration System**</td>
    <td>Highly flexible configuration via YAML (`default_config.yaml`), environment variables (prefixed `FLEXSEQ_`), and direct CLI parameter overrides (`--param`). Supports temperature templating in paths and column names.</td>
  </tr>
</tbody>
</table>

## 🔄 Pipeline Overview

The FlexSeq pipeline follows a structured workflow managed by the `Pipeline` class (`flexseq/pipeline.py`), driven by the configuration settings.

**Conceptual Workflow Diagram:**

```mermaid
graph TD
    A[Input: Temp-Specific CSV Data<br>(e.g., temperature_320_train.csv)] --> B(Load & Process Data);
    B --> |`data.loader`, `data.processor`| C[Clean Data & Feature Engineering<br>(Encoding, Normalization, Windowing)];
    C --> |`config.dataset.domains`| D(Filter Domains);
    D --> |`data.processor.split_data`| E{Split Data<br>(Train/Val/Test Set<br>Stratify by Domain?)};

    subgraph "Model Training Pipeline"
    direction LR
    E -- Train Set --> F[Select Enabled Models<br>(RF, NN)];
    F --> G{Hyperparameter Optimization?<br>(`config.models.*.optimization`)}
    G -- Yes --> H[Optimize via CV<br>(Optuna/RandomizedSearch)];
    G -- No --> I[Train Model<br>(`model.fit`)];
    H --> I;
    I --> J[Save Trained Model<br>(`.pkl`/`.pt`)];
    end

    subgraph "Model Evaluation Pipeline"
    direction LR
    J --> K[Load Trained Model<br>(from `./models/models_{T}/`)];
    E -- Evaluation Set (Test/Val) --> L[Prepare Eval Data];
    K --> M[Predict on Eval Set<br>(`model.predict`/`predict_with_std`)];
    L --> M;
    M --> N[Calculate Metrics<br>(`utils.metrics.evaluate_predictions`)];
    N --> O[Save Metrics & Detailed Results<br>(to `./output/outputs_{T}/`)];
    end

    subgraph "Prediction Pipeline"
    direction LR
    P[Input: New Data CSV] --> Q(Load & Process New Data);
    J --> R[Load Trained Model];
    Q --> S[Predict on New Data<br>(`model.predict`/`predict_with_std`)];
    R --> S;
    S --> T[Save Predictions CSV];
    end

    subgraph "Analysis & Comparison"
    direction LR
    O -- Per-Temp Results --> U[Temperature Comparison<br>(`temperature.comparison`)];
    O -- Per-Temp Results --> V[Analysis & Visualization Data<br>(Feature Importance, Residue Errors, etc.)];
    U --> W[Save Comparison Data<br>(to `./output/outputs_comparison/`)]
    V --> X[Save Analysis Data CSVs & Basic Plots<br>(to `./output/outputs_{T}/`)];
    end

    Z[Configuration File<br>(`default_config.yaml`, Overrides)]-.-> B;
    Z-.-> C;
    Z-.-> D;
    Z-.-> E;
    Z-.-> F;
    Z-.-> G;
    Z-.-> L;
    Z-.-> N;
    Z-.-> U;
    Z-.-> V;

    style A fill:#FFDAB9,stroke:#FFA07A
    style P fill:#FFDAB9,stroke:#FFA07A
    style B fill:#ADD8E6,stroke:#87CEEB
    style C fill:#ADD8E6,stroke:#87CEEB
    style D fill:#ADD8E6,stroke:#87CEEB
    style E fill:#ADD8E6,stroke:#87CEEB
    style F fill:#90EE90,stroke:#3CB371
    style G fill:#FFFFE0,stroke:#F0E68C
    style H fill:#FFEC8B,stroke:#CDAD00
    style I fill:#90EE90,stroke:#3CB371
    style J fill:#C1FFC1,stroke:#00CD00
    style K fill:#FFFFE0,stroke:#F0E68C
    style L fill:#ADD8E6,stroke:#87CEEB
    style M fill:#FFB6C1,stroke:#FF69B4
    style N fill:#FFB6C1,stroke:#FF69B4
    style O fill:#DDA0DD,stroke:#BA55D3
    style Q fill:#ADD8E6,stroke:#87CEEB
    style R fill:#FFFFE0,stroke:#F0E68C
    style S fill:#FFB6C1,stroke:#FF69B4
    style T fill:#DDA0DD,stroke:#BA55D3
    style U fill:#E6E6FA,stroke:#9370DB
    style V fill:#E6E6FA,stroke:#9370DB
    style W fill:#D8BFD8,stroke:#9A32CD
    style X fill:#D8BFD8,stroke:#9A32CD
    style Z fill:#F5F5DC,stroke:#A0522D
```

### 🧩 Logical Flow of Operation (CLI Perspective)

```mermaid
flowchart TD
    start([🏁 Start `flexseq <command>`]) --> config[📝 Load Configuration<br>(YAML + Env Var + CLI Params)];
    config --> op{⚙️ Operation Type?};

    op -->|train| train_flow
    op -->|evaluate| eval_flow
    op -->|predict| predict_flow
    op -->|run| run_flow
    op -->|train-all-temps| train_all_flow
    op -->|compare-temperatures| compare_flow

    subgraph train_flow [Train Flow]
        direction LR
        tr_start(Train) --> tr_mode{Mode?};
        tr_mode -- FlexSeq --> tr_std_feats(Standard Features);
        tr_mode -- OmniFlex --> tr_adv_feats(Advanced Features);
        tr_std_feats --> tr_temp(Select Temperature);
        tr_adv_feats --> tr_temp;
        tr_temp --> tr_data(Load & Process Data);
        tr_data --> tr_split(Split Data);
        tr_split --> tr_models(Select Models);
        tr_models --> tr_hp_check{Optimize HParams?};
        tr_hp_check -- Yes --> tr_hp_opt(Hyperparameter Opt.);
        tr_hp_check -- No --> tr_train(Train Models);
        tr_hp_opt --> tr_train;
        tr_train --> tr_save(Save Models);
        tr_save --> tr_eval(Evaluate on Validation);
        tr_eval --> tr_end(End Train);
    end

    subgraph eval_flow [Evaluate Flow]
        direction LR
        ev_start(Evaluate) --> ev_mode{Mode?};
        ev_mode --> ev_temp(Select Temperature);
        ev_temp --> ev_load_data(Load & Process Data);
        ev_load_data --> ev_split(Split Data);
        ev_split -- Eval Set --> ev_load_models(Load Models);
        ev_load_models --> ev_predict(Generate Predictions);
        ev_predict --> ev_metrics(Calculate Metrics);
        ev_metrics --> ev_save(Save Results);
        ev_save --> ev_end(End Evaluate);
    end

    subgraph predict_flow [Predict Flow]
        direction LR
        pr_start(Predict) --> pr_mode{Mode?};
        pr_mode --> pr_temp(Select Temperature);
        pr_temp --> pr_input(Load & Process Input Data);
        pr_input --> pr_load_model(Load Best/Specified Model);
        pr_load_model --> pr_predict(Generate Predictions);
        pr_predict --> pr_save(Save Predictions);
        pr_save --> pr_end(End Predict);
    end

    subgraph run_flow [Run Flow]
        direction LR
        run_start(Run) --> run_train(Execute Train Flow);
        run_train --> run_eval(Execute Evaluate Flow);
        run_eval --> run_analyze(Analyze & Gen Viz Data);
        run_analyze --> run_end(End Run);
    end

     subgraph train_all_flow [Train All Temps Flow]
        direction LR
        tat_start(Train All) --> tat_loop{For each Temp in Config};
        tat_loop -- Loop --> tat_train(Execute Train Flow for Temp);
        tat_train -- Done --> tat_loop;
        tat_loop -- Finished --> tat_end(End Train All);
     end

     subgraph compare_flow [Compare Temps Flow]
        direction LR
        ct_start(Compare) --> ct_load(Load Results from All Temps);
        ct_load --> ct_analyze(Compare Metrics & Predictions);
        ct_analyze --> ct_save(Save Comparison Data);
        ct_save --> ct_end(End Compare);
     end

    tr_end --> finish([🏁 Finish])
    ev_end --> finish
    pr_end --> finish
    run_end --> finish
    tat_end --> finish
    ct_end --> finish

    style start fill:#f9f9f9,stroke:#333,stroke-width:2px
    style finish fill:#f9f9f9,stroke:#333,stroke-width:2px
    style config fill:#ffcc99,stroke:#ff9933,stroke-width:2px
    style op fill:#FFDAAB,stroke:#FF9933,stroke-width:2px
```

## 🔧 Installation

### Prerequisites
*   Python 3.8 or higher
*   pip (Python package installer)

### Install from Source (Recommended)
```bash
# 1. Clone the repository
git clone https://github.com/Felixburton7/flexseq.git
cd flexseq

# 2. Install the package in editable mode
pip install -e .
```
*This installs the package such that changes to the source code are immediately reflected.*

### Dependencies
Core dependencies are managed by `setuptools` via `pyproject.toml` and `setup.py`. Key dependencies include:
`numpy`, `pandas`, `scikit-learn`, `torch`, `pyyaml`, `click`, `matplotlib`, `seaborn`, `joblib`, `tqdm`, `optuna`.

## 🚀 Quick Start

*(Run commands from the root directory: `/home/s_felix/flexseq`)*

### Basic Usage
```bash
# Train Random Forest model at 320K using default config
flexseq train --temperature 320 --model random_forest

# Evaluate the trained Random Forest model at 320K
flexseq evaluate --temperature 320 --model random_forest
# Check output in ./output/outputs_320/evaluation_results.csv

# Predict RMSF for new proteins at 320K using the best model
# (Assumes new_proteins.csv is in ./data and formatted correctly)
flexseq predict --input ./data/new_proteins.csv --temperature 320 --output ./output/new_proteins_pred_320.csv
```

### Advanced Usage
```bash
# Train Neural Network using OmniFlex mode at 348K
# (Requires esm_rmsf column in temperature_348_train.csv)
flexseq train --mode omniflex --temperature 348 --model neural_network

# Train enabled models on all available temperatures
flexseq train-all-temps

# Run the full pipeline (train, evaluate, analyze) for 379K
flexseq run --temperature 379

# Generate data comparing Random Forest results across all temperatures
flexseq compare-temperatures --model random_forest
# Check output in ./output/outputs_comparison/
```

## 📥 Input Data

FlexSeq expects temperature-specific CSV files in the data directory (`./data` by default).

*   **File Naming:** Defined by `dataset.file_pattern` in config (e.g., `temperature_320_train.csv`).
*   **Required Columns for Training:** `domain_id`, `resid`, `resname`, `rmsf_{temperature}`.
*   **Optional/Recommended Columns:** `protein_size`, `normalized_resid`, `core_exterior`, `relative_accessibility`, `dssp`, `phi`, `psi`.
*   **OmniFlex Mode Columns:** `esm_rmsf` (required if `use_esm: true`), `voxel_rmsf` (required if `use_voxel: true`).

| Column                      | Description                                             | Type    | Example        | Notes                                       |
| :-------------------------- | :------------------------------------------------------ | :------ | :------------- | :------------------------------------------ |
| `domain_id`                 | Protein domain identifier                             | string  | `1a0aA00`      | Used for grouping and stratified splitting |
| `resid`                     | Residue ID (position in chain)                        | int     | `42`           |                                             |
| `resname`                   | 3-letter amino acid code                                | string  | `ALA`          |                                             |
| `rmsf_{temperature}`        | **Target:** RMSF value at specified temperature       | float   | `0.835`        | e.g., `rmsf_320` for T=320K                 |
| `protein_size`              | *Feature:* Total # residues in protein/domain         | int     | `153`          | Calculated if missing                       |
| `normalized_resid`          | *Feature:* Residue pos. normalized to 0-1             | float   | `0.274`        | Calculated if missing                       |
| `core_exterior`             | *Source:* Location ('interior' or 'surface')          | string  | `surface`      | Encoded to `core_exterior_encoded`        |
| `relative_accessibility`    | *Feature:* Solvent accessibility measure              | float   | `0.65`         | Typically 0-1                               |
| `dssp`                      | *Source:* Secondary structure (DSSP codes)            | string  | `H`, `E`, `C`  | Encoded to `secondary_structure_encoded`    |
| `phi`, `psi`                | *Source:* Backbone dihedral angles (degrees)          | float   | `-65.3`, `120.7` | Normalized to `phi_norm`, `psi_norm`        |
| `resname_encoded`           | *Feature:* Numerical encoding of `resname`            | int     | `1`            | Generated if `resname` present              |
| `core_exterior_encoded`     | *Feature:* Binary encoding (0=core, 1=surface)        | int     | `1`            | Generated if `core_exterior` present        |
| `secondary_structure_encoded`| *Feature:* Numerical encoding of `dssp` (0=H, 1=E, 2=Loop)| int| `0`            | Generated if `dssp` present                 |
| `phi_norm`, `psi_norm`      | *Feature:* Normalized angles [-1, 1]                  | float   | `-0.36`, `0.67` | Generated if `phi`/`psi` present           |
| `esm_rmsf`                  | *Feature (OmniFlex):* Prediction from ESM           | float   | `0.75`         | Required if `use_esm: true`               |
| `voxel_rmsf`                | *Feature (OmniFlex):* Prediction from Voxels        | float   | `0.81`         | Required if `use_voxel: true`             |

*The pipeline attempts data cleaning and feature generation (`flexseq/data/processor.py`). Missing optional source columns will prevent generation of derived features.*

## 📤 Output Data

Output files are saved to the configured `paths.output_dir` (default: `./output`), often within temperature-specific subdirectories (`outputs_{T}`).

| Output Type                 | Description                                             | Format | Default Path (`T`=temperature)                      |
| :-------------------------- | :------------------------------------------------------ | :----- | :-------------------------------------------------- |
| 💾 **Trained Models**       | Saved state of trained models                           | `.pkl` | `./models/models_{T}/{model_name}.pkl`              |
| 📊 **Evaluation Metrics**   | Summary of performance metrics for each model           | CSV    | `./output/outputs_{T}/evaluation_results.csv`     |
| 📈 **Detailed Results**     | Eval data + predictions + errors + uncertainty        | CSV    | `./output/outputs_{T}/all_results.csv`            |
| 🧩 **Domain Metrics**        | Performance metrics aggregated per domain               | CSV    | `./output/outputs_{T}/domain_metrics.csv`         |
| 🔮 **Predictions**          | Predictions on new input data                           | CSV    | `./output/{input_base}_predictions_{T}.csv`       |
| ⭐ **Feature Importance**   | Importance scores for each feature per model            | CSV, PNG| `./output/outputs_{T}/feature_importance/`        |
| 🧬 **Residue Analysis**     | Data for error analysis by AA, position, structure    | CSV, PNG| `./output/outputs_{T}/residue_analysis/`          |
| 🌡️ **Temp Comparison**     | Combined results and metrics across temperatures        | CSV    | `./output/outputs_comparison/`                    |
| 📉 **Training History (NN)**| Epoch-wise loss/metrics for Neural Network              | CSV, PNG| `./output/outputs_{T}/training_performance/`      |
| 📊 **Visualization Data**   | Pre-formatted data for generating plots externally      | CSV    | `./output/outputs_{T}/visualization_data/`        |

## 🤖 Models

FlexSeq implements Random Forest and Neural Network models, configurable in the `models` section of the config YAML.

| Model             | Implementation             | Key Config Parameters (`models.{name}.*`)                                                                                                | Uncertainty Method               | Hyperparameter Optimization      |
| :---------------- | :------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------- | :----------------------------- |
| 🌲 **Random Forest** | `RandomForestModel`        | `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, `bootstrap`, `randomized_search` (uses Scikit-learn's `RandomizedSearchCV`) | Variance across tree predictions | `RandomizedSearchCV` (built-in)|
| 🧠 **Neural Network**| `NeuralNetworkModel`       | `architecture` (`hidden_layers`, `activation`, `dropout`), `training` (`optimizer`, `learning_rate`, `batch_size`, `epochs`, `early_stopping`), `hyperparameter_optimization` (uses Optuna) | Monte Carlo Dropout sampling   | Optuna (Bayesian/Random/Grid)  |

*   See `flexseq/models/base.py` for the base class definition.
*   Model parameters and optimization settings are highly configurable (see `default_config.yaml`).

## 📈 Analysis & Visualization

The pipeline focuses on generating structured CSV data to facilitate detailed analysis and external visualization, complementing the basic plots it generates directly.

**Generated Data/Plots Enable Analysis Of:**
*   **Overall Performance:** R², RMSE, MAE comparisons between models and across temperatures.
*   **Prediction Accuracy:** Scatter plots of Actual vs. Predicted RMSF, optionally with density contours or colored by residue properties.
*   **Error Analysis:** Distribution of errors (absolute, relative) grouped by amino acid type, secondary structure, sequence position, or surface exposure.
*   **Feature Contributions:** Ranked list/bar chart of feature importances (using permutation importance).
*   **Temperature Dependence:** Correlation of RMSF values between temperatures, trends of metrics vs. temperature, linear regression of RMSF change per residue.
*   **Model Behavior:** RMSF profiles along the sequence for specific domains, training/validation curves for Neural Networks.

*Refer to `flexseq/utils/visualization.py` for the functions generating plot data and basic plots.*

## ⚙️ Configuration

Pipeline behavior is controlled via YAML configuration files.

*   **Default:** `default_config.yaml` (packaged with the library).
*   **User:** Specify a custom YAML via `flexseq <command> --config path/to/my_config.yaml`.
*   **Overrides:**
    *   **Environment Variables:** Prefix with `FLEXSEQ_`, use underscores for nesting (e.g., `FLEXSEQ_MODELS_RANDOM_FOREST_N_ESTIMATORS=500`).
    *   **CLI Parameters:** Use `--param key=value` (e.g., `--param dataset.split.test_size=0.25`). CLI overrides take highest precedence.

**Example Snippet (`default_config.yaml`):**
```yaml
# FlexSeq Configuration

paths:
  data_dir: ./data                # Data directory
  output_dir: ./output            # Output directory
  models_dir: ./models            # Saved models directory

mode:
  active: "omniflex"              # Using advanced mode ("flexseq" or "omniflex")
  omniflex:
    use_esm: true                 # Use ESM embeddings feature
    use_voxel: false              # Enable 3D voxel feature (if available)

temperature:
  current: 348                    # Current temperature to process
  available: [320, 348, 379, 413, 450, "average"] # Available datasets

dataset:
  file_pattern: "temperature_{temperature}_train.csv" # How to find data files
  features:
    use_features:                 # Features to use as model input
      protein_size: true
      normalized_resid: true
      relative_accessibility: true
      core_exterior_encoded: true
      secondary_structure_encoded: true
      phi_norm: true
      psi_norm: true
      resname_encoded: true
      esm_rmsf: true              # OmniFlex only
      voxel_rmsf: false           # OmniFlex only
    window:                       # Window feature settings
      enabled: true
      size: 5                     # Window = size*2 + 1 residues
  target: rmsf_{temperature}      # Target variable column name
  split:
    test_size: 0.2
    validation_size: 0.15
    stratify_by_domain: true      # Keep domains together during split

models:
  random_forest:
    enabled: true
    n_estimators: 500
    max_features: 0.7
    randomized_search:            # Hyperparameter optimization settings
      enabled: true
      n_iter: 50
      cv: 5
      param_distributions:        # Search space
        n_estimators: [100, 200, 300, 500, 800]
        # ... other parameters
  neural_network:
    enabled: true
    architecture:
      hidden_layers: [256, 128, 64]
    # ... other NN parameters and optimization settings
```
*Refer to the full `default_config.yaml` for all options.*

## 💻 Command-Line Interface

The `flexseq` command provides structured access to pipeline functions.

| Command                 | Description                                              | Example                                              |
| :---------------------- | :------------------------------------------------------- | :--------------------------------------------------- |
| `train`                 | Train models for a specific temperature.                 | `flexseq train --temp 320 --model random_forest`     |
| `evaluate`              | Evaluate trained models.                                 | `flexseq evaluate --temp 320 --model random_forest`  |
| `predict`               | Generate predictions for new input data.                 | `flexseq predict --input new.csv --temp 320`         |
| `run`                   | Execute the full train, evaluate, analyze pipeline.      | `flexseq run --temp 348 --mode omniflex`             |
| `train-all-temps`       | Train models for all temperatures in `temperature.available`. | `flexseq train-all-temps`                            |
| `compare-temperatures`  | Generate data comparing results across temperatures.     | `flexseq compare-temperatures --model random_forest` |
| `preprocess`            | Only load, clean, and process data; save output.       | `flexseq preprocess --input raw.csv --out proc.csv`  |
| `list-models`           | List registered model names.                             | `flexseq list-models`                                |
| `list-temperatures`     | List temperatures defined in the configuration.          | `flexseq list-temperatures`                          |

**Common Options:** `--temperature` (`--temp`), `--model`, `--config`, `--param`, `--mode`, `--input`, `--output`. Use `flexseq <command> --help` for details.

## 📚 Documentation

*(Placeholder links - adapt if full documentation exists)*
*   [Installation Guide](https://flexseq.readthedocs.io/en/latest/installation.html)
*   [User Guide](https://flexseq.readthedocs.io/en/latest/user_guide.html)
*   [API Reference](https://flexseq.readthedocs.io/en/latest/api.html)
*   [Examples](https://flexseq.readthedocs.io/en/latest/examples.html)

## 📝 Citation

If you use FlexSeq in your research, please cite the repository:
```bibtex
@software{burton2023flexseq,
  author = {Burton, Felix},
  title = {FlexSeq: Protein Flexibility Prediction Pipeline},
  year = {2023},
  url = {https://github.com/Felixburton7/flexseq}
}
```

## 🤝 Contributing

Contributions are welcome! Please follow standard GitHub practices (fork, feature branch, pull request) or refer to the `CONTRIBUTING.md` file if present.

## 📄 License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## 👏 Acknowledgements

*   Developed by Felix Burton ([@Felixburton7](https://github.com/Felixburton7)).
*   Utilizes numerous open-source libraries including Scikit-learn, PyTorch, Pandas, NumPy, Matplotlib, Seaborn, Click, PyYAML, TQDM, Joblib, and Optuna.
```
### Core Module Files ###
---------------------------------------------------------
===== FILE: flexseq/__init__.py =====
"""
FlexSeq ML Pipeline for protein flexibility prediction.

This package provides a complete machine learning pipeline for predicting
protein flexibility (RMSF values) from sequence and structural features
across multiple temperatures.
"""

__version__ = "0.1.0"

from flexseq.pipeline import Pipeline
===== FILE: flexseq/config.py =====
"""
Configuration handling for the FlexSeq ML pipeline.

This module provides functions for loading, validating, and managing
configuration settings throughout the pipeline, with special support
for temperature-specific configurations.
"""

import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
import yaml
import re

logger = logging.getLogger(__name__)

def deep_merge(base_dict: Dict, overlay_dict: Dict) -> Dict:
    """
    Recursively merge two dictionaries, with values from overlay_dict taking precedence.
    
    Args:
        base_dict: Base dictionary to merge into
        overlay_dict: Dictionary with values that should override base_dict
        
    Returns:
        Dict containing merged configuration
    """
    result = base_dict.copy()
    
    for key, value in overlay_dict.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
            
    return result

def get_env_var_config() -> Dict[str, Any]:
    """
    Get configuration from environment variables.
    Environment variables should be prefixed with FLEXSEQ_ and use
    underscore separators for nested keys.
    
    Examples:
        FLEXSEQ_PATHS_DATA_DIR=/path/to/data
        FLEXSEQ_MODELS_RANDOM_FOREST_N_ESTIMATORS=200
        FLEXSEQ_TEMPERATURE_CURRENT=320
        
    Returns:
        Dict containing configuration from environment variables
    """
    config = {}
    
    for key, value in os.environ.items():
        if not key.startswith("FLEXSEQ_"):
            continue
            
        # Remove prefix and convert to lowercase
        key = key[8:].lower()
        
        # Split into parts and create nested dict
        parts = key.split("_")
        current = config
        
        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]
            
        # Set value, converting to appropriate type
        value_part = parts[-1]
        
        # Try to convert to appropriate type
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                if "." in value:
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                # Keep as string if not convertible
                pass
                
        current[value_part] = value
        
    return config

def parse_param_overrides(params: List[str]) -> Dict[str, Any]:
    """
    Parse parameter overrides from CLI arguments.
    
    Args:
        params: List of parameter overrides in format "key=value"
        
    Returns:
        Dict containing parameter overrides
    """
    if not params:
        return {}
        
    override_dict = {}
    
    for param in params:
        if "=" not in param:
            logger.warning(f"Ignoring invalid parameter override: {param}")
            continue
            
        key, value = param.split("=", 1)
        
        # Convert value to appropriate type
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                if "." in value:
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                # Keep as string if not convertible
                pass
                
        # Split key into parts and create nested dict
        parts = key.split(".")
        current = override_dict
        
        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]
            
        current[parts[-1]] = value
        
    return override_dict

def template_config_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> Dict[str, Any]:
    """
    Apply temperature-specific templating to configuration.
    
    Replaces {temperature} placeholders in strings with the specified temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value to use in templating
        
    Returns:
        Dictionary with templated configuration
    """
    # Create deep copy to avoid modifying the original
    import copy
    result = copy.deepcopy(config)
    
    def replace_in_dict(d):
        for key, value in d.items():
            if isinstance(value, dict):
                replace_in_dict(value)
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        replace_in_dict(item)
                    elif isinstance(item, str):
                        d[key][i] = item.replace("{temperature}", str(temperature))
            elif isinstance(value, str):
                d[key] = value.replace("{temperature}", str(temperature))
                
    replace_in_dict(result)
    
    return result

def load_config(
    config_path: Optional[str] = None,
    param_overrides: Optional[List[str]] = None,
    use_env_vars: bool = True,
    temperature: Optional[Union[int, str]] = None
) -> Dict[str, Any]:
    """
    Load configuration from default and user-provided sources.
    
    Args:
        config_path: Optional path to user config file
        param_overrides: Optional list of parameter overrides
        use_env_vars: Whether to use environment variables
        temperature: Optional temperature value for templating
        
    Returns:
        Dict containing merged configuration
        
    Raises:
        FileNotFoundError: If config_path is provided but file doesn't exist
        ValueError: If configuration is invalid
    """
    # Determine default config path
    default_path = os.path.join(os.path.dirname(__file__), "..", "default_config.yaml")
    if not os.path.exists(default_path):
        package_dir = os.path.dirname(os.path.abspath(__file__))
        default_path = os.path.join(package_dir, "..", "default_config.yaml")
        
    # Load default config
    if not os.path.exists(default_path):
        raise FileNotFoundError(f"Default config not found at {default_path}")
        
    with open(default_path, 'r') as f:
        config = yaml.safe_load(f)
        
    # Overlay user config if provided
    if config_path:
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"User config not found at {config_path}")
            
        with open(config_path, 'r') as f:
            user_config = yaml.safe_load(f)
            config = deep_merge(config, user_config)
            
    # Apply environment variable overrides
    if use_env_vars:
        env_config = get_env_var_config()
        config = deep_merge(config, env_config)
        
    # Apply CLI parameter overrides
    if param_overrides:
        override_config = parse_param_overrides(param_overrides)
        config = deep_merge(config, override_config)
    
    # Apply temperature override if provided
    if temperature is not None:
        config["temperature"]["current"] = temperature
    
    # Get current temperature from config
    current_temp = config["temperature"]["current"]
    
    # Apply temperature templating
    config = template_config_for_temperature(config, current_temp)
    
    # Handle OmniFlex mode settings
    if config["mode"]["active"].lower() == "omniflex":
        # Enable ESM and voxel features if in OmniFlex mode
        if config["mode"]["omniflex"]["use_esm"]:
            config["dataset"]["features"]["use_features"]["esm_rmsf"] = True
        
        if config["mode"]["omniflex"]["use_voxel"]:
            config["dataset"]["features"]["use_features"]["voxel_rmsf"] = True
    
    # Validate config (basic validation)
    validate_config(config)
    
    # Set system-wide logging level
    log_level = config.get("system", {}).get("log_level", "INFO")
    numeric_level = getattr(logging, log_level.upper(), None)
    if isinstance(numeric_level, int):
        logging.getLogger().setLevel(numeric_level)
    
    return config

def validate_config(config: Dict[str, Any]) -> None:
    """
    Perform basic validation of configuration.
    
    Args:
        config: Configuration dictionary to validate
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Check required sections
    required_sections = ["paths", "dataset", "models", "evaluation", "system", "temperature", "mode"]
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required config section: {section}")
            
    # Validate dataset section
    if "target" not in config["dataset"]:
        raise ValueError("Missing required dataset.target configuration")
    
    # Validate temperature section
    current_temp = config["temperature"]["current"]
    available_temps = config["temperature"]["available"]
    
    if str(current_temp) not in [str(t) for t in available_temps]:
        raise ValueError(f"Current temperature {current_temp} is not in the list of available temperatures")
    
    # Check that at least one model is enabled
    any_model_enabled = False
    for model_name, model_config in config.get("models", {}).items():
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            any_model_enabled = True
            break
            
    if not any_model_enabled:
        logger.warning("No models are enabled in configuration")
        
    # Additional validation could be added as needed
            
def get_enabled_models(config: Dict[str, Any]) -> List[str]:
    """
    Get list of enabled model names from config.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of enabled model names
    """
    enabled_models = []
    
    for model_name, model_config in config.get("models", {}).items():
        if model_name != "common" and isinstance(model_config, dict) and model_config.get("enabled", False):
            enabled_models.append(model_name)
            
    return enabled_models

def get_model_config(config: Dict[str, Any], model_name: str) -> Dict[str, Any]:
    """
    Get configuration for a specific model, with common settings applied.
    
    Args:
        config: Full configuration dictionary
        model_name: Name of the model
        
    Returns:
        Model-specific configuration with common settings merged in
        
    Raises:
        ValueError: If model_name is not found in configuration
    """
    models_config = config.get("models", {})
    
    if model_name not in models_config:
        raise ValueError(f"Model '{model_name}' not found in configuration")
        
    model_config = models_config[model_name]
    common_config = models_config.get("common", {})
    
    # Merge common config with model-specific config
    merged_config = deep_merge(common_config, model_config)
    
    return merged_config

def get_available_temperatures(config: Dict[str, Any]) -> List[Union[int, str]]:
    """
    Get list of available temperatures from config.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of available temperatures
    """
    return config["temperature"]["available"]

def get_output_dir_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> str:
    """
    Get output directory path for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value
        
    Returns:
        Path to output directory for the specified temperature
    """
    base_output_dir = config["paths"]["output_dir"]
    return os.path.join(base_output_dir, f"outputs_{temperature}")

def get_models_dir_for_temperature(config: Dict[str, Any], temperature: Union[int, str]) -> str:
    """
    Get models directory path for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Temperature value
        
    Returns:
        Path to models directory for the specified temperature
    """
    base_models_dir = config["paths"]["models_dir"]
    return os.path.join(base_models_dir, f"models_{temperature}")

def get_comparison_output_dir(config: Dict[str, Any]) -> str:
    """
    Get output directory path for temperature comparisons.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Path to output directory for temperature comparisons
    """
    base_output_dir = config["paths"]["output_dir"]
    return os.path.join(base_output_dir, "outputs_comparison")
===== FILE: flexseq/pipeline.py =====
"""
Main pipeline orchestration for the FlexSeq ML workflow.

This module provides the Pipeline class that handles the entire ML workflow
from data loading to evaluation, with temperature-specific functionality.
"""

import os
import logging
import time
from typing import Dict, List, Tuple, Optional, Any, Union

import pandas as pd
import numpy as np
import joblib

from flexseq.config import (
    load_config, 
    get_enabled_models, 
    get_model_config,
    get_output_dir_for_temperature
)
from flexseq.utils.helpers import progress_bar, ProgressCallback
from flexseq.models import get_model_class
from flexseq.data.processor import (
    load_and_process_data, 
    split_data, 
    prepare_data_for_model,
    process_features
)
from flexseq.utils.metrics import evaluate_predictions


logger = logging.getLogger(__name__)

class Pipeline:
    """
    Main pipeline orchestration for FlexSeq.
    Handles the full ML workflow from data loading to evaluation.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the pipeline with configuration.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.models = {}
        
        # Create output directories if they don't exist
        self.prepare_directories()
        
        # Log mode information
        mode = config["mode"]["active"]
        logger.info(f"Pipeline initialized in {mode.upper()} mode")
        
        # Log temperature information
        temperature = config["temperature"]["current"]
        logger.info(f"Using temperature: {temperature}")
        
    def prepare_directories(self) -> None:
        """
        Create necessary output directories.
        """
        paths = self.config["paths"]
        
        # Ensure data directory exists
        data_dir = paths.get("data_dir", "./data")
        os.makedirs(data_dir, exist_ok=True)
        
        # Ensure output directory exists
        output_dir = paths.get("output_dir", "./output")
        os.makedirs(output_dir, exist_ok=True)
        
        # Ensure models directory exists
        models_dir = paths.get("models_dir", "./models")
        os.makedirs(models_dir, exist_ok=True)
        
        # Create subdirectories for different types of output
        os.makedirs(os.path.join(output_dir, "comparisons"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "feature_importance"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "residue_analysis"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "domain_analysis"), exist_ok=True)
        
    def load_data(self, data_path: Optional[str] = None) -> pd.DataFrame:
        """
        Load and process input data.
        
        Args:
            data_path: Optional explicit path to data file
            
        Returns:
            Processed DataFrame
        """
        # Use explicit file path or temperature-specific data
        return load_and_process_data(data_path, self.config)
    
    def train(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Train specified models on the data.
        
        Args:
            model_names: Optional list of model names to train (if None, use all enabled models)
            data_path: Optional explicit path to data file
            
        Returns:
            Dictionary of trained models
        """
        from flexseq.utils.helpers import progress_bar
        
        # Determine which models to train
        if model_names is None:
            model_names = get_enabled_models(self.config)
            
        if not model_names:
            logger.warning("No models specified for training")
            return {}
            
        # Load and preprocess data
        logger.info("Loading and processing data")
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        logger.info("Splitting data into train/validation/test sets")
        with ProgressCallback(total=1, desc="Splitting data") as pbar:
            train_df, val_df, test_df = split_data(df, self.config)
            pbar.update()
        
        # Prepare training data
        with ProgressCallback(total=1, desc="Preparing features") as pbar:
            X_train, y_train, feature_names = prepare_data_for_model(train_df, self.config)
            pbar.update()
        
        # Train each model
        trained_models = {}
        
        for model_name in progress_bar(model_names, desc="Training models"):
            logger.info(f"Training model: {model_name}")
            
            try:
                # Get model class and config
                model_class = get_model_class(model_name)
                model_config = get_model_config(self.config, model_name)
                
                if not model_config.get("enabled", False):
                    logger.warning(f"Model {model_name} is disabled in config. Skipping.")
                    continue
                
                # Get hyperparameter optimization config
                optimize_hyperparams = False
                if model_name == "neural_network":
                    optimize_hyperparams = model_config.get("hyperparameter_optimization", {}).get("enabled", False)
                elif model_name == "random_forest":
                    optimize_hyperparams = model_config.get("randomized_search", {}).get("enabled", False)
                
                # Remove non-init params from config
                if model_name == "neural_network":
                    init_params = {
                        "architecture": model_config.get("architecture", {}),
                        "training": model_config.get("training", {}),
                        "random_state": self.config["system"].get("random_state", 42)
                    }
                else:
                    init_params = {k: v for k, v in model_config.items() 
                                if k not in ['enabled', 'cross_validation', 'save_best', 'randomized_search', 'hyperparameter_optimization']}
                
                # Create model instance
                model = model_class(**init_params)
                
                # Perform hyperparameter optimization if enabled
                if optimize_hyperparams:
                    with ProgressCallback(total=1, desc=f"Optimizing hyperparameters for {model_name}") as pbar:
                        logger.info(f"Performing hyperparameter optimization for {model_name}")
                        
                        if model_name == "neural_network":
                            opt_config = model_config["hyperparameter_optimization"]
                            method = opt_config.get("method", "bayesian")
                            trials = opt_config.get("trials", 20)
                            param_grid = opt_config.get("parameters", {})
                            
                            # Prepare validation data for hyperparameter tuning
                            X_val, y_val, _ = prepare_data_for_model(val_df, self.config)
                            
                            # Combine train and validation for cross-validation
                            X_combined = np.vstack([X_train, X_val])
                            y_combined = np.concatenate([y_train, y_val])
                            
                            # Optimize hyperparameters
                            best_params = model.hyperparameter_optimize(
                                X_combined, y_combined, param_grid, method, trials, cv=3
                            )
                            
                            logger.info(f"Best hyperparameters for {model_name}: {best_params}")
                            
                        elif model_name == "random_forest":
                            # Random forest uses its internal RandomizedSearchCV
                            # Just log that optimization is enabled
                            logger.info("RandomizedSearchCV will be used for Random Forest training")
                        
                        pbar.update()
                
                # Train the model
                start_time = time.time()
                
                if model_name == "neural_network":
                    # Neural network training shows progress
                    model.fit(X_train, y_train, feature_names)
                else:
                    # Other models use simple progress indicator
                    with ProgressCallback(total=1, desc=f"Training {model_name}") as pbar:
                        model.fit(X_train, y_train, feature_names)
                        pbar.update()
                
                # Store trained model
                trained_models[model_name] = model
                
                # Log training time
                train_time = time.time() - start_time
                logger.info(f"Trained {model_name} in {train_time:.2f} seconds")
                
                # Save training history if available
                if hasattr(model, 'get_training_history') and model.get_training_history():
                    history = model.get_training_history()
                    history_df = pd.DataFrame(history)
                    history_path = os.path.join(self.config["paths"]["output_dir"], f"{model_name}_training_history.csv")
                    history_df.to_csv(history_path)
                    logger.info(f"Saved training history to {history_path}")
                
                # Save model if configured
                if model_config.get("save_best", True):
                    with ProgressCallback(total=1, desc=f"Saving {model_name}") as pbar:
                        self.save_model(model, model_name)
                        pbar.update()
                
                # Evaluate on validation set
                with ProgressCallback(total=1, desc=f"Validating {model_name}") as pbar:
                    X_val, y_val, _ = prepare_data_for_model(val_df, self.config)
                    val_predictions = model.predict(X_val)
                    val_metrics = evaluate_predictions(y_val, val_predictions, self.config)
                    logger.info(f"Validation metrics for {model_name}: {val_metrics}")
                    pbar.update()
                
            except Exception as e:
                logger.error(f"Error training model {model_name}: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        self.models = trained_models
        return trained_models

    
    def save_model(self, model: Any, model_name: str) -> None:
        """
        Save a trained model to disk.
        
        Args:
            model: Trained model instance
            model_name: Name of the model
        """
        models_dir = self.config["paths"]["models_dir"]
        model_path = os.path.join(models_dir, f"{model_name}.pkl")
        
        try:
            model.save(model_path)
            logger.info(f"Saved model {model_name} to {model_path}")
        except Exception as e:
            logger.error(f"Error saving model {model_name}: {e}")
    
    def load_model(self, model_name: str) -> Any:
        """
        Load a trained model from disk.
        
        Args:
            model_name: Name of the model to load
            
        Returns:
            Loaded model instance
        """
        models_dir = self.config["paths"]["models_dir"]
        model_path = os.path.join(models_dir, f"{model_name}.pkl")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        try:
            model_class = get_model_class(model_name)
            model = model_class.load(model_path)
            return model
        except Exception as e:
            logger.error(f"Error loading model {model_name}: {e}")
            raise
        
    
    def evaluate(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        Evaluate models on test data.
        
        Args:
            model_names: Optional list of model names to evaluate
            data_path: Optional explicit path to data file
            
        Returns:
            Dictionary of evaluation metrics for each model
        """
        from flexseq.utils.helpers import progress_bar, ProgressCallback
        
        # Determine which models to evaluate
        if model_names is None:
            model_names = list(self.models.keys())
            
            # If no models in memory, use enabled models from config
            if not model_names:
                model_names = get_enabled_models(self.config)
                
        if not model_names:
            logger.warning("No models specified for evaluation")
            return {}
        
        # Load data if needed
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        with ProgressCallback(total=1, desc="Splitting data") as pbar:
            train_df, val_df, test_df = split_data(df, self.config)
            pbar.update()
        
        # Use test or validation set based on config
        comparison_set = self.config["evaluation"]["comparison_set"]
        
        if comparison_set == "test":
            eval_df = test_df
            logger.info("Using test set for evaluation")
        elif comparison_set == "validation":
            eval_df = val_df
            logger.info("Using validation set for evaluation")
        else:
            logger.warning(f"Unknown comparison_set '{comparison_set}', using test set")
            eval_df = test_df
        
        # Prepare evaluation data
        with ProgressCallback(total=1, desc="Preparing features") as pbar:
            X_eval, y_eval, feature_names = prepare_data_for_model(eval_df, self.config)
            pbar.update()
        
        # Evaluate each model
        results = {}
        predictions = {}
        uncertainties = {}
        
        for model_name in progress_bar(model_names, desc="Evaluating models"):
            logger.info(f"Evaluating model: {model_name}")
            
            try:
                # Load model if not in memory
                with ProgressCallback(total=1, desc=f"Loading {model_name}", leave=False) as pbar:
                    if model_name in self.models:
                        model = self.models[model_name]
                    else:
                        model = self.load_model(model_name)
                    pbar.update()
                
                # Generate predictions
                with ProgressCallback(total=1, desc=f"Predicting with {model_name}", leave=False) as pbar:
                    # Try to get uncertainty estimates if available
                    if hasattr(model, 'predict_with_std'):
                        preds, stds = model.predict_with_std(X_eval)
                        predictions[model_name] = preds
                        uncertainties[model_name] = stds
                    else:
                        preds = model.predict(X_eval)
                        predictions[model_name] = preds
                    pbar.update()
                
                # Calculate metrics
                with ProgressCallback(total=1, desc=f"Computing metrics", leave=False) as pbar:
                    metrics = evaluate_predictions(y_eval, preds, self.config, X_eval, X_eval.shape[1])
                    pbar.update()
                
                # Store results
                results[model_name] = metrics
                
                # Log results
                logger.info(f"Evaluation metrics for {model_name}: {metrics}")
                
            except Exception as e:
                logger.error(f"Error evaluating model {model_name}: {e}")
        
        # Save evaluation results
        with ProgressCallback(total=1, desc="Saving evaluation results") as pbar:
            self.save_evaluation_results(results, eval_df, predictions, uncertainties)
            pbar.update()
            
        
        
        return results
    
    def save_evaluation_results(
        self, 
        results: Dict[str, Dict[str, float]],
        eval_df: pd.DataFrame,
        predictions: Dict[str, np.ndarray] = None,
        uncertainties: Dict[str, np.ndarray] = None
    ) -> None:
        """
        Save evaluation results to disk.
        
        Args:
            results: Dictionary of evaluation metrics for each model
            eval_df: DataFrame with evaluation data
            predictions: Dictionary of predictions by model
            uncertainties: Dictionary of prediction uncertainties by model
        """
        output_dir = self.config["paths"]["output_dir"]
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Save metrics to CSV
        results_path = os.path.join(output_dir, "evaluation_results.csv")
        results_df = pd.DataFrame(results).T
        results_df.index.name = "model"
        results_df.to_csv(results_path)
        logger.info(f"Saved evaluation metrics to {results_path}")
        
        # Save all results (predictions and optionally uncertainties)
        if predictions:
            all_results_df = eval_df.copy()
            target_col = self.config["dataset"]["target"]
            
            # Add predictions for each model
            for model_name, preds in predictions.items():
                all_results_df[f"{model_name}_predicted"] = preds
                
                # Add errors
                all_results_df[f"{model_name}_error"] = preds - all_results_df[target_col]
                all_results_df[f"{model_name}_abs_error"] = np.abs(all_results_df[f"{model_name}_error"])
                
                # Add uncertainties if available
                if uncertainties and model_name in uncertainties:
                    all_results_df[f"{model_name}_uncertainty"] = uncertainties[model_name]
            
            # Save to CSV
            all_results_path = os.path.join(output_dir, "all_results.csv")
            all_results_df.to_csv(all_results_path, index=False)
            logger.info(f"Saved detailed results to {all_results_path}")
            
            # Save domain-level metrics
            self.save_domain_metrics(all_results_df, target_col, predictions.keys())
    
    def save_domain_metrics(
        self,
        results_df: pd.DataFrame,
        target_col: str,
        model_names: List[str]
    ) -> None:
        """
        Calculate and save domain-level metrics.
        
        Args:
            results_df: DataFrame with all results
            target_col: Target column name
            model_names: List of model names
        """
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        output_dir = self.config["paths"]["output_dir"]
        domain_metrics = []
        
        # Calculate metrics per domain
        for domain_id, domain_df in results_df.groupby("domain_id"):
            domain_result = {"domain_id": domain_id}
            
            # Calculate metrics for each model
            for model_name in model_names:
                pred_col = f"{model_name}_predicted"
                if pred_col not in domain_df.columns:
                    continue
                
                actual = domain_df[target_col].values
                predicted = domain_df[pred_col].values
                
                # Calculate metrics
                rmse = np.sqrt(mean_squared_error(actual, predicted))
                mae = mean_absolute_error(actual, predicted)
                r2 = r2_score(actual, predicted)
                
                # Store metrics
                domain_result[f"{model_name}_rmse"] = rmse
                domain_result[f"{model_name}_mae"] = mae
                domain_result[f"{model_name}_r2"] = r2
                
                # Basic statistics
                domain_result[f"{model_name}_mean_error"] = np.mean(domain_df[f"{model_name}_error"])
                domain_result[f"{model_name}_std_error"] = np.std(domain_df[f"{model_name}_error"])
                
                # Calculate temperature-dependent metrics if in OmniFlex mode
                if self.config["mode"]["active"] == "omniflex":
                    # Add correlation with ESM and voxel predictions if available
                    for pred_type in ["esm_rmsf", "voxel_rmsf"]:
                        if pred_type in domain_df.columns:
                            corr = np.corrcoef(predicted, domain_df[pred_type])[0, 1]
                            domain_result[f"{model_name}_corr_with_{pred_type}"] = corr
            
            # Add domain properties
            domain_result["num_residues"] = len(domain_df)
            
            # Calculate protein properties if available
            if "core_exterior_encoded" in domain_df.columns:
                domain_result["percent_surface"] = domain_df["core_exterior_encoded"].mean() * 100
                
            if "secondary_structure_encoded" in domain_df.columns:
                # Count residues by structure type
                ss_counts = domain_df["secondary_structure_encoded"].value_counts(normalize=True) * 100
                for ss_type, value in zip([0, 1, 2], ["helix", "sheet", "loop"]):
                    if ss_type in ss_counts:
                        domain_result[f"percent_{value}"] = ss_counts[ss_type]
                    else:
                        domain_result[f"percent_{value}"] = 0.0
                    
            domain_metrics.append(domain_result)
        
        # Save domain metrics to CSV
        if domain_metrics:
            domain_metrics_df = pd.DataFrame(domain_metrics)
            domain_metrics_path = os.path.join(output_dir, "domain_metrics.csv")
            domain_metrics_df.to_csv(domain_metrics_path, index=False)
            logger.info(f"Saved domain-level metrics to {domain_metrics_path}")

    def predict(
        self, 
        data: Union[str, pd.DataFrame],
        model_name: Optional[str] = None,
        with_uncertainty: bool = False
    ) -> pd.DataFrame:
        """
        Generate predictions for new data.
        
        Args:
            data: DataFrame or path to CSV file with protein data
            model_name: Model to use for prediction (if None, use best model)
            with_uncertainty: Whether to include uncertainty estimates
            
        Returns:
            DataFrame with original data and predictions
        """
        # Load data if string path provided
        if isinstance(data, str):
            df = load_and_process_data(data, self.config)
        else:
            df = data.copy()
            df = process_features(df, self.config)
        
        # Determine which model to use
        if model_name is None:
            # Find best model based on previous evaluation
            try:
                output_dir = self.config["paths"]["output_dir"]
                results_path = os.path.join(output_dir, "evaluation_results.csv")
                
                if os.path.exists(results_path):
                    results_df = pd.read_csv(results_path, index_col="model")
                    
                    # Use R^2 or RMSE to determine best model
                    if "r2" in results_df.columns:
                        best_model = results_df["r2"].idxmax()
                    elif "rmse" in results_df.columns:
                        best_model = results_df["rmse"].idxmin()
                    else:
                        best_model = results_df.index[0]
                        
                    model_name = best_model
                    logger.info(f"Using best model based on evaluation: {model_name}")
                else:
                    # No evaluation results, use first enabled model
                    model_name = get_enabled_models(self.config)[0]
                    logger.info(f"No evaluation results found, using enabled model: {model_name}")
                    
            except Exception as e:
                logger.error(f"Error finding best model: {e}")
                # Fall back to first enabled model
                model_name = get_enabled_models(self.config)[0]
        
        # Load model if not in memory
        if model_name in self.models:
            model = self.models[model_name]
        else:
            model = self.load_model(model_name)
        
        # Prepare data for prediction
        X, _, feature_names = prepare_data_for_model(
            df, self.config, include_target=False
        )
        
        # Generate predictions, possibly with uncertainty
        target_col = self.config["dataset"]["target"]
        result_df = df.copy()
        
        if with_uncertainty and hasattr(model, 'predict_with_std'):
            try:
                predictions, uncertainties = model.predict_with_std(X)
                
                # Add predictions and uncertainties to result
                result_df[f"{target_col}_predicted"] = predictions
                result_df[f"{target_col}_uncertainty"] = uncertainties
                
                # If in OmniFlex mode, add prediction quality indicators
                if self.config["mode"]["active"] == "omniflex":
                    # Calculate z-scores (deviation / uncertainty)
                    if target_col in result_df.columns:
                        z_scores = np.abs(predictions - result_df[target_col]) / uncertainties
                        result_df[f"{target_col}_z_score"] = z_scores
                
            except Exception as e:
                logger.error(f"Error generating predictions with uncertainty: {e}")
                # Fall back to standard prediction
                predictions = model.predict(X)
                result_df[f"{target_col}_predicted"] = predictions
        else:
            # Standard prediction without uncertainty
            predictions = model.predict(X)
            result_df[f"{target_col}_predicted"] = predictions
            
            # If target is available, calculate error
            if target_col in result_df.columns:
                result_df[f"{target_col}_error"] = predictions - result_df[target_col]
                result_df[f"{target_col}_abs_error"] = np.abs(result_df[f"{target_col}_error"])
        
        return result_df
    
    def analyze(
        self,
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None
    ) -> None:
        """
        Perform analysis of model results.
        
        Args:
            model_names: Optional list of model names to analyze
            data_path: Optional explicit path to data file
        """
        from flexseq.utils.helpers import progress_bar, ProgressCallback
        
        # Determine which models to analyze
        if model_names is None:
            model_names = list(self.models.keys())
            
            # If no models in memory, use enabled models from config
            if not model_names:
                model_names = get_enabled_models(self.config)
                
        if not model_names:
            logger.warning("No models specified for analysis")
            return
        
        # Load data if needed
        with ProgressCallback(total=1, desc="Loading data") as pbar:
            df = self.load_data(data_path)
            pbar.update()
        
        # Split data
        with ProgressCallback(total=1, desc="Preparing test data") as pbar:
            _, _, test_df = split_data(df, self.config)
            pbar.update()
        
        # Generate predictions for each model
        predictions = {}
        feature_importances = {}
        
        for model_name in progress_bar(model_names, desc="Analyzing models"):
            try:
                # Load model if not in memory
                if model_name in self.models:
                    model = self.models[model_name]
                else:
                    with ProgressCallback(total=1, desc=f"Loading {model_name}", leave=False) as pbar:
                        model = self.load_model(model_name)
                        pbar.update()
                
                # Prepare data
                with ProgressCallback(total=1, desc="Preparing features", leave=False) as pbar:
                    X_test, y_test, feature_names = prepare_data_for_model(test_df, self.config)
                    pbar.update()
                
                # Generate predictions
                with ProgressCallback(total=1, desc=f"Predicting with {model_name}", leave=False) as pbar:
                    predictions[model_name] = model.predict(X_test)
                    pbar.update()
                
                
                # Get feature importances if available
                importance = model.get_feature_importance()
                if importance:
                    feature_importances[model_name] = importance
                    
                    # Save feature importance to CSV
                    importance_df = pd.DataFrame({
                        'feature': list(importance.keys()),
                        'importance': list(importance.values())
                    })
                    importance_df = importance_df.sort_values('importance', ascending=False)
                    
                    output_dir = self.config["paths"]["output_dir"]
                    importance_path = os.path.join(
                        output_dir, 
                        "feature_importance", 
                        f"{model_name}_feature_importance.csv"
                    )
                    os.makedirs(os.path.dirname(importance_path), exist_ok=True)
                    importance_df.to_csv(importance_path, index=False)
                    logger.info(f"Saved feature importance to {importance_path}")
                    
            except Exception as e:
                logger.error(f"Error analyzing model {model_name}: {e}")
        
        # Generate combined results
        target_col = self.config["dataset"]["target"]
        target_values = test_df[target_col].values
        
        # Generate a combined results dataframe
        combined_df = test_df.copy()
        
        for model_name, preds in predictions.items():
            combined_df[f"{model_name}_predicted"] = preds
            combined_df[f"{model_name}_error"] = preds - combined_df[target_col]
            combined_df[f"{model_name}_abs_error"] = np.abs(combined_df[f"{model_name}_error"])
        
        # Save combined results
        output_dir = self.config["paths"]["output_dir"]
        combined_path = os.path.join(output_dir, "combined_analysis_results.csv")
        combined_df.to_csv(combined_path, index=False)
        logger.info(f"Saved combined analysis results to {combined_path}")
        
            # Import visualization functions
        from flexseq.utils.visualization import (
            plot_r2_comparison,
            plot_residue_level_rmsf,
            plot_amino_acid_error_analysis,
            plot_amino_acid_error_boxplot,
            plot_amino_acid_scatter_plot,
            plot_error_analysis_by_property,
            plot_r2_comparison_scatter,
            plot_scatter_with_density_contours,
            plot_flexibility_vs_dihedral_angles,
            plot_flexibility_sequence_neighborhood,
            plot_error_response_surface,
            plot_secondary_structure_error_correlation
        )

        # Prepare data for visualization
        predictions = {}
        for model_name in model_names:
            pred_col = f"{model_name}_predicted"
            if pred_col in combined_df.columns:
                predictions[model_name] = combined_df[pred_col].values

        # Generate various visualizations
        plot_r2_comparison(predictions, combined_df[target_col].values, model_names, self.config)
        plot_residue_level_rmsf(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_error_analysis(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_error_boxplot(combined_df, predictions, target_col, model_names, self.config)
        plot_amino_acid_scatter_plot(combined_df, predictions, target_col, model_names, self.config)
        plot_error_analysis_by_property(combined_df, predictions, target_col, model_names, self.config)
        plot_r2_comparison_scatter(predictions, combined_df[target_col].values, model_names, self.config)
        plot_scatter_with_density_contours(combined_df, predictions, target_col, model_names, self.config)
        plot_flexibility_vs_dihedral_angles(combined_df, predictions, target_col, model_names, self.config)
        plot_flexibility_sequence_neighborhood(combined_df, predictions, target_col, model_names, self.config)
        plot_error_response_surface(combined_df, predictions, target_col, model_names, self.config)
        plot_secondary_structure_error_correlation(combined_df, predictions, target_col, model_names, self.config)

        
        # Generate residue-level analysis
        self.residue_level_analysis(combined_df, model_names)
        
        # Generate secondary structure analysis
        self.secondary_structure_analysis(combined_df, model_names)
        
        # Generate amino acid type analysis
        self.amino_acid_analysis(combined_df, model_names)
        
        # Process visualization data (CSV files for later visualization)
        self.generate_visualization_data(combined_df, model_names)
    
    def residue_level_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform residue-level analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        target_col = self.config["dataset"]["target"]
        output_dir = self.config["paths"]["output_dir"]
        residue_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(residue_dir, exist_ok=True)
        
        # Calculate error statistics per residue position
        residue_stats = []
        
        # Group by normalized residue position (if available) or by residue ID
        groupby_col = "normalized_resid" if "normalized_resid" in df.columns else "resid"
        
        # Bin values if using normalized_resid
        if groupby_col == "normalized_resid":
            df["resid_bin"] = pd.cut(df[groupby_col], bins=20, labels=False)
            groupby_col = "resid_bin"
        
        for pos, group in df.groupby(groupby_col):
            row = {groupby_col: pos, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            residue_stats.append(row)
        
        if residue_stats:
            residue_df = pd.DataFrame(residue_stats)
            residue_path = os.path.join(residue_dir, "residue_position_errors.csv")
            residue_df.to_csv(residue_path, index=False)
            logger.info(f"Saved residue position analysis to {residue_path}")
    
    def secondary_structure_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform secondary structure analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        if "secondary_structure_encoded" not in df.columns:
            logger.warning("Secondary structure information not available for analysis")
            return
        
        output_dir = self.config["paths"]["output_dir"]
        ss_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(ss_dir, exist_ok=True)
        
        # Map encoded values to types
        ss_mapping = {0: "helix", 1: "sheet", 2: "loop"}
        
        # Calculate error statistics per secondary structure type
        ss_stats = []
        
        for ss_code, group in df.groupby("secondary_structure_encoded"):
            ss_type = ss_mapping.get(ss_code, f"unknown_{ss_code}")
            row = {"secondary_structure": ss_type, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            ss_stats.append(row)
        
        if ss_stats:
            ss_df = pd.DataFrame(ss_stats)
            ss_path = os.path.join(ss_dir, "secondary_structure_errors.csv")
            ss_df.to_csv(ss_path, index=False)
            logger.info(f"Saved secondary structure analysis to {ss_path}")
    
    def amino_acid_analysis(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Perform amino acid-specific analysis of prediction errors.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        output_dir = self.config["paths"]["output_dir"]
        aa_dir = os.path.join(output_dir, "residue_analysis")
        os.makedirs(aa_dir, exist_ok=True)
        
        # Calculate error statistics per amino acid type
        aa_stats = []
        
        for aa, group in df.groupby("resname"):
            row = {"resname": aa, "count": len(group)}
            
            for model_name in model_names:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
            
            aa_stats.append(row)
        
        if aa_stats:
            aa_df = pd.DataFrame(aa_stats)
            aa_path = os.path.join(aa_dir, "amino_acid_errors.csv")
            aa_df.to_csv(aa_path, index=False)
            logger.info(f"Saved amino acid analysis to {aa_path}")
    
    def generate_visualization_data(self, df: pd.DataFrame, model_names: List[str]) -> None:
        """
        Generate data files for visualizations.
        
        Args:
            df: DataFrame with predictions and actual values
            model_names: List of model names that have been analyzed
        """
        target_col = self.config["dataset"]["target"]
        output_dir = self.config["paths"]["output_dir"]
        vis_dir = os.path.join(output_dir, "visualization_data")
        os.makedirs(vis_dir, exist_ok=True)
        
        # Generate histogram data for RMSF distributions
        try:
            histogram_data = []
            
            # Get actual values
            actual_values = df[target_col].dropna()
            
            actual_hist, actual_bins = np.histogram(actual_values, bins=20)
            
            for i in range(len(actual_hist)):
                histogram_data.append({
                    'source': 'actual',
                    'bin_start': actual_bins[i],
                    'bin_end': actual_bins[i+1],
                    'count': actual_hist[i]
                })
            
            # Get predicted values for each model
            for model_name in model_names:
                pred_col = f"{model_name}_predicted"
                if pred_col in df.columns:
                    pred_values = df[pred_col].dropna()
                    pred_hist, pred_bins = np.histogram(pred_values, bins=actual_bins)
                    
                    for i in range(len(pred_hist)):
                        histogram_data.append({
                            'source': model_name,
                            'bin_start': pred_bins[i],
                            'bin_end': pred_bins[i+1],
                            'count': pred_hist[i]
                        })
            
            # Save histogram data
            histogram_df = pd.DataFrame(histogram_data)
            histogram_path = os.path.join(vis_dir, "rmsf_distribution.csv")
            histogram_df.to_csv(histogram_path, index=False)
            logger.info(f"Saved RMSF distribution data to {histogram_path}")
            
        except Exception as e:
            logger.error(f"Error generating histogram data: {e}")
        
        # Generate scatter plot data
        try:
            scatter_data = []
            
            # Sample rows to avoid too large data files
            sample_size = min(5000, len(df))
            sampled_df = df.sample(sample_size, random_state=self.config["system"]["random_state"])
            
            for _, row in sampled_df.iterrows():
                data_point = {
                    'domain_id': row['domain_id'],
                    'resid': row['resid'],
                    'resname': row['resname'],
                    'actual': row[target_col]
                }
                
                # Add predicted values
                for model_name in model_names:
                    pred_col = f"{model_name}_predicted"
                    if pred_col in row:
                        data_point[model_name] = row[pred_col]
                
                # Add structural features if available
                for feature in ['secondary_structure_encoded', 'core_exterior_encoded', 'normalized_resid']:
                    if feature in row:
                        data_point[feature] = row[feature]
                
                scatter_data.append(data_point)
            
            # Save scatter data
            scatter_df = pd.DataFrame(scatter_data)
            scatter_path = os.path.join(vis_dir, "rmsf_scatter_data.csv")
            scatter_df.to_csv(scatter_path, index=False)
            logger.info(f"Saved scatter plot data to {scatter_path}")
            
        except Exception as e:
            logger.error(f"Error generating scatter plot data: {e}")
    
    def run_pipeline(
        self, 
        model_names: Optional[List[str]] = None,
        data_path: Optional[str] = None,
        skip_visualization: bool = False
    ) -> Dict[str, Dict[str, float]]:
        """
        Run the complete pipeline: train, evaluate, and analyze.
        
        Args:
            model_names: Optional list of model names to use
            data_path: Optional explicit path to data file
            skip_visualization: Whether to skip visualization data generation
            
        Returns:
            Dictionary of evaluation metrics for each model
        """
        # Train models
        self.train(model_names, data_path)
        
        # Evaluate models
        results = self.evaluate(model_names, data_path)
        
        # Analyze (optional)
        if not skip_visualization:
            self.analyze(model_names, data_path)
        
        return results
===== FILE: flexseq/cli.py =====
"""
Command-line interface for the FlexSeq ML pipeline.

This module provides the CLI commands for training, evaluating, and
analyzing protein flexibility predictions across multiple temperatures.
"""

import os
import sys
import logging
from typing import List, Optional, Tuple, Dict, Any, Union

import click

from flexseq.config import (
    load_config, 
    get_enabled_models, 
    get_model_config,
    get_available_temperatures,
    get_output_dir_for_temperature,
    get_models_dir_for_temperature,
    get_comparison_output_dir
)
from flexseq.pipeline import Pipeline
from flexseq.models import get_available_models

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def parse_model_list(model_arg: Optional[str]) -> List[str]:
    """
    Parse comma-separated list of models.
    
    Args:
        model_arg: Comma-separated model names or None
        
    Returns:
        List of model names
    """
    if not model_arg:
        return []
        
    return [m.strip() for m in model_arg.split(",")]

@click.group()
@click.version_option(version="0.1.0")
def cli():
    """
    FlexSeq: ML pipeline for protein flexibility prediction.
    
    This tool provides a complete pipeline for predicting protein flexibility
    (RMSF values) from sequence and structural features using machine learning
    across multiple temperatures.
    """
    pass

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter (e.g. models.random_forest.n_estimators=200)")
@click.option("--domains", 
              help="Specific domains to include (comma-separated)")
@click.option("--exclude-domains", 
              help="Domains to exclude (comma-separated)")
@click.option("--disable-feature", 
              help="Features to disable (comma-separated)")
@click.option("--window-size", 
              type=int, 
              help="Window size for feature engineering")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def train(
    model, config, param, domains, exclude_domains, 
    disable_feature, window_size, input, temperature, mode
):
    """
    Train flexibility prediction models.
    
    Examples:
        flexseq train
        flexseq train --model random_forest
        flexseq train --temperature 320
        flexseq train --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Apply CLI-specific overrides
    if domains:
        domain_list = [d.strip() for d in domains.split(",")]
        cfg["dataset"]["domains"]["include"] = domain_list
        
    if exclude_domains:
        exclude_list = [d.strip() for d in exclude_domains.split(",")]
        cfg["dataset"]["domains"]["exclude"] = exclude_list
        
    if disable_feature:
        features = [f.strip() for f in disable_feature.split(",")]
        for feature in features:
            if feature in cfg["dataset"]["features"]["use_features"]:
                cfg["dataset"]["features"]["use_features"][feature] = False
                
    if window_size is not None:
        cfg["dataset"]["features"]["window"]["size"] = window_size
    
    # Determine which models to train
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Create temperature-specific output directory
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # Create pipeline and train models
    pipeline = Pipeline(cfg)
    
    try:
        trained_models = pipeline.train(model_list, input)
        click.echo(f"Successfully trained {len(trained_models)} models for temperature {current_temp}")
    except Exception as e:
        click.echo(f"Error during training: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to evaluate (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def evaluate(model, config, param, input, temperature, mode):
    """
    Evaluate trained models.
    
    Examples:
        flexseq evaluate
        flexseq evaluate --model random_forest
        flexseq evaluate --temperature 320
        flexseq evaluate --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Determine which models to evaluate
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create pipeline and evaluate models
    pipeline = Pipeline(cfg)
    
    try:
        results = pipeline.evaluate(model_list, input)
        
        # Display results
        click.echo("\nEvaluation Results:")
        for model_name, metrics in results.items():
            click.echo(f"\n{model_name}:")
            for metric, value in metrics.items():
                click.echo(f"  {metric}: {value:.4f}")
        
    except Exception as e:
        click.echo(f"Error during evaluation: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to use (defaults to best model)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              required=True,
              help="Input data file (CSV)")
@click.option("--output", 
              type=click.Path(), 
              help="Output file path (defaults to input_predictions.csv)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
@click.option("--uncertainty",
              is_flag=True,
              help="Include uncertainty estimates in predictions")
def predict(model, config, param, input, output, temperature, mode, uncertainty):
    """
    Generate predictions for new data.
    
    Examples:
        flexseq predict --input new_proteins.csv
        flexseq predict --model random_forest --input new_proteins.csv
        flexseq predict --temperature 320 --input new_proteins.csv
        flexseq predict --mode omniflex --input new_proteins.csv
        flexseq predict --uncertainty --input new_proteins.csv
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create pipeline
    pipeline = Pipeline(cfg)
    
    try:
        # Generate predictions
        predictions_df = pipeline.predict(input, model, with_uncertainty=uncertainty)
        
        # Determine output path
        if not output:
            base = os.path.splitext(input)[0]
            output = f"{base}_predictions_{current_temp}.csv"
            
        # Save predictions
        os.makedirs(os.path.dirname(os.path.abspath(output)), exist_ok=True)
        predictions_df.to_csv(output, index=False)
        click.echo(f"Saved predictions to {output}")
        
    except Exception as e:
        click.echo(f"Error generating predictions: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def train_all_temps(model, config, param, mode):
    """
    Train models on all available temperatures.
    
    Examples:
        flexseq train-all-temps
        flexseq train-all-temps --model random_forest
        flexseq train-all-temps --mode omniflex
    """
    # Load configuration without temperature override
    cfg = load_config(config, param)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get all available temperatures
    temperatures = get_available_temperatures(cfg)
    
    # Determine which models to train
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Train for each temperature
    for temp in temperatures:
        click.echo(f"\nTraining for temperature: {temp}")
        
        # Create temperature-specific config
        temp_cfg = load_config(config, param, temperature=temp)
        
        if mode:
            temp_cfg["mode"]["active"] = mode
        
        # Get temperature-specific directories
        output_dir = get_output_dir_for_temperature(temp_cfg, temp)
        models_dir = get_models_dir_for_temperature(temp_cfg, temp)
        
        # Update config with temperature-specific directories
        temp_cfg["paths"]["output_dir"] = output_dir
        temp_cfg["paths"]["models_dir"] = models_dir
        
        # Create directories
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(models_dir, exist_ok=True)
        
        # Create pipeline and train models
        pipeline = Pipeline(temp_cfg)
        
        try:
            trained_models = pipeline.train(model_list)
            click.echo(f"Successfully trained {len(trained_models)} models for temperature {temp}")
            
            # Evaluate models
            results = pipeline.evaluate(model_list)
            
            # Display results
            click.echo(f"Evaluation Results for temperature {temp}:")
            for model_name, metrics in results.items():
                click.echo(f"{model_name}:")
                for metric, value in metrics.items():
                    click.echo(f"  {metric}: {value:.4f}")
                    
        except Exception as e:
            click.echo(f"Error during training for temperature {temp}: {e}")
            # Continue with next temperature

@cli.command()
@click.option("--model", 
              help="Model to compare (defaults to random_forest)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def compare_temperatures(model, config, param, mode):
    """
    Compare results across temperatures.
    
    Examples:
        flexseq compare-temperatures
        flexseq compare-temperatures --model neural_network
    """
    from flexseq.temperature.comparison import prepare_temperature_comparison_data
    
    # Load configuration
    cfg = load_config(config, param)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Get model to use
    if not model:
        model = "random_forest"  # Default to random forest
        click.echo(f"No model specified, using {model}")
    
    # Get all available temperatures
    temperatures = get_available_temperatures(cfg)
    
    # Check if temperature comparison is enabled
    if not cfg["temperature"]["comparison"]["enabled"]:
        click.echo("Temperature comparison is disabled in config")
        return
    
    # Get comparison output directory
    comparison_dir = get_comparison_output_dir(cfg)
    os.makedirs(comparison_dir, exist_ok=True)
    
    # Generate comparison data
    try:
        comparison_data = prepare_temperature_comparison_data(cfg, model, comparison_dir)
        
        click.echo(f"\nTemperature comparison data saved to {comparison_dir}")
        
        # Display available files
        click.echo("Generated files:")
        for filename in os.listdir(comparison_dir):
            file_path = os.path.join(comparison_dir, filename)
            if os.path.isfile(file_path):
                file_size = os.path.getsize(file_path)
                click.echo(f"  {filename} ({file_size} bytes)")
        
    except Exception as e:
        click.echo(f"Error during temperature comparison: {e}")
        sys.exit(1)

@cli.command()
@click.option("--input", 
              type=click.Path(exists=True), 
              required=True,
              help="Input data file (CSV)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--output", 
              type=click.Path(), 
              help="Output file path (defaults to input_processed.csv)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
def preprocess(input, config, param, output, temperature, mode):
    """
    Preprocess data only without training or prediction.
    
    Examples:
        flexseq preprocess --input raw_data.csv
        flexseq preprocess --temperature 320 --input raw_data.csv
        flexseq preprocess --mode omniflex --input raw_data.csv
    """
    from flexseq.data.processor import load_and_process_data
    
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    try:
        # Process data
        processed_df = load_and_process_data(input, cfg)
        
        # Determine output path
        if not output:
            base = os.path.splitext(input)[0]
            current_temp = cfg["temperature"]["current"]
            output = f"{base}_processed_{current_temp}.csv"
            
        # Save processed data
        os.makedirs(os.path.dirname(os.path.abspath(output)), exist_ok=True)
        processed_df.to_csv(output, index=False)
        click.echo(f"Saved processed data to {output}")
        
    except Exception as e:
        click.echo(f"Error preprocessing data: {e}")
        sys.exit(1)

@cli.command()
@click.option("--model", 
              help="Model to train (comma-separated for multiple)")
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
@click.option("--param", 
              multiple=True, 
              help="Override config parameter")
@click.option("--input", 
              type=click.Path(exists=True), 
              help="Input data file (CSV)")
@click.option("--temperature", 
              type=str,
              help="Temperature to use (e.g., 320, 348, average)")
@click.option("--mode",
              type=click.Choice(["flexseq", "omniflex"]),
              help="Operation mode")
@click.option("--skip-visualization", 
              is_flag=True,
              help="Skip visualization steps")
def run(model, config, param, input, temperature, mode, skip_visualization):
    """
    Run the complete pipeline (train, evaluate, analyze).
    
    Examples:
        flexseq run
        flexseq run --model random_forest
        flexseq run --temperature 320
        flexseq run --mode omniflex
    """
    # Load configuration
    cfg = load_config(config, param, temperature=temperature)
    
    # Set mode if specified
    if mode:
        cfg["mode"]["active"] = mode
    
    # Determine which models to use
    model_list = parse_model_list(model)
    if not model_list:
        model_list = get_enabled_models(cfg)
        
    if not model_list:
        click.echo("No models specified or enabled in config")
        return
    
    # Get temperature-specific directories
    current_temp = cfg["temperature"]["current"]
    output_dir = get_output_dir_for_temperature(cfg, current_temp)
    models_dir = get_models_dir_for_temperature(cfg, current_temp)
    
    # Update config with temperature-specific directories
    cfg["paths"]["output_dir"] = output_dir
    cfg["paths"]["models_dir"] = models_dir
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # Create pipeline and run
    pipeline = Pipeline(cfg)
    
    try:
        results = pipeline.run_pipeline(
            model_list, input, skip_visualization
        )
        
        click.echo("\nPipeline completed successfully!")
        click.echo(f"Results saved to {output_dir}")
        
    except Exception as e:
        click.echo(f"Error running pipeline: {e}")
        sys.exit(1)

@cli.command()
def list_models():
    """
    List available models in the registry.
    
    Examples:
        flexseq list-models
    """
    from flexseq.models import get_available_models
    
    models = get_available_models()
    
    click.echo("Available models:")
    for model in models:
        click.echo(f"  - {model}")

@cli.command()
@click.option("--config", 
              type=click.Path(exists=True), 
              help="Path to config file")
def list_temperatures(config):
    """
    List available temperatures in the configuration.
    
    Examples:
        flexseq list-temperatures
    """
    # Load configuration
    cfg = load_config(config)
    
    temperatures = get_available_temperatures(cfg)
    
    click.echo("Available temperatures:")
    for temp in temperatures:
        click.echo(f"  - {temp}")

if __name__ == "__main__":
    cli()
### Model Files ###
---------------------------------------------------------
===== FILE: flexseq/models/__init__.py =====
"""
Model registry for the FlexSeq ML pipeline.

This module provides a registry for model classes and utility functions
for model discovery and management.
"""

from importlib import import_module
from pathlib import Path
from typing import Dict, Type, List, Optional

from .base import BaseModel

# Global model registry
MODEL_REGISTRY: Dict[str, Type[BaseModel]] = {}

def register_model(name: str):
    """
    Decorator to register a model class in the global registry.
    
    Args:
        name: Name to register the model under
        
    Returns:
        Decorator function
    """
    def decorator(cls):
        MODEL_REGISTRY[name] = cls
        return cls
    return decorator

def get_model_class(model_name: str) -> Type[BaseModel]:
    """
    Get a model class by name.
    
    Args:
        model_name: Name of the model to get
        
    Returns:
        Model class
        
    Raises:
        ValueError: If model_name is not found in registry
    """
    if model_name not in MODEL_REGISTRY:
        raise ValueError(f"Model '{model_name}' not found in registry. " 
                         f"Available models: {', '.join(MODEL_REGISTRY.keys())}")
    
    return MODEL_REGISTRY[model_name]

def get_available_models() -> List[str]:
    """
    Get list of available model names.
    
    Returns:
        List of registered model names
    """
    return list(MODEL_REGISTRY.keys())

# Auto-discover models using importlib
models_dir = Path(__file__).parent
for model_file in models_dir.glob('*.py'):
    if model_file.stem not in ('__init__', 'base'):
        try:
            import_module(f'flexseq.models.{model_file.stem}')
        except ImportError as e:
            import logging
            logging.getLogger(__name__).warning(f"Error importing model module {model_file.stem}: {e}")
===== FILE: flexseq/models/base.py =====
"""
Base model class for FlexSeq ML pipeline.

This module defines the BaseModel abstract class that all
protein flexibility prediction models must implement.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple

import numpy as np
import pandas as pd

class BaseModel(ABC):
    """
    Base class for all FlexSeq ML models.
    All models must implement these methods.
    """
    
    @abstractmethod
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]) -> 'BaseModel':
        """
        Train the model on input data.
        
        Args:
            X: Feature matrix
            y: Target values
            
        Returns:
            Self, for method chaining
        """
        pass
    
    @abstractmethod
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate predictions for input data.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predictions
        """
        pass
    
    @abstractmethod
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        pass
    
    @classmethod
    @abstractmethod
    def load(cls, path: str) -> 'BaseModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded model instance
        """
        pass
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        # Default implementation returns predictions with zeros for std dev
        # Models that support uncertainty should override this
        predictions = self.predict(X)
        std_deviation = np.zeros_like(predictions)
        return predictions, std_deviation
    
    def get_feature_importance(self) -> Optional[Dict[str, float]]:
        """
        Get feature importance values if available.
        
        Returns:
            Dictionary mapping feature names to importance values,
            or None if feature importance is not available
        """
        return None
    
    def get_params(self) -> Dict[str, Any]:
        """
        Get model parameters.
        
        Returns:
            Dictionary of model parameters
        """
        return {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('_') and key != 'model'
        }
    
    def get_model_name(self) -> str:
        """
        Get model name.
        
        Returns:
            String representing model name
        """
        return self.__class__.__name__
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
            
        Raises:
            NotImplementedError: If the model doesn't support hyperparameter optimization
        """
        raise NotImplementedError("This model doesn't support hyperparameter optimization")
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return None
===== FILE: flexseq/models/random_forest.py =====
"""
Random Forest model implementation for the FlexSeq ML pipeline.

This module provides a RandomForestModel for protein flexibility prediction
with support for uncertainty estimation and hyperparameter optimization.
"""

import os
import logging
from typing import Dict, Any, Optional, Union, List, Tuple

import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

from flexseq.models import register_model
from flexseq.models.base import BaseModel
from flexseq.utils.helpers import ProgressCallback

logger = logging.getLogger(__name__)

@register_model("random_forest")
class RandomForestModel(BaseModel):
    """
    Random Forest model for protein flexibility prediction.
    
    This model uses an ensemble of decision trees to capture non-linear
    relationships between protein features and flexibility.
    """
    
    def __init__(
        self, 
        n_estimators: int = 100, 
        max_depth: Optional[int] = None,
        min_samples_split: int = 2,
        min_samples_leaf: int = 1,
        max_features: Union[str, float, int] = 0.7,
        bootstrap: bool = True,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Random Forest model.
        
        Args:
            n_estimators: Number of trees in the forest
            max_depth: Maximum depth of each tree (None = unlimited)
            min_samples_split: Minimum samples required to split an internal node
            min_samples_leaf: Minimum samples required to be at a leaf node
            max_features: Number of features to consider for best split
            bootstrap: Whether to use bootstrap samples
            random_state: Random seed for reproducibility
            **kwargs: Additional parameters passed to RandomForestRegressor
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.random_state = random_state
        self.model_params = kwargs
        self.model = None
        self.feature_names_ = None
        self.best_params_ = None
        
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'RandomForestModel':
        """
        Train the Random Forest model.
        
        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names
            
        Returns:
            Self, for method chaining
        """
        # Store feature names if available
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        elif feature_names is not None:
            self.feature_names_ = feature_names
        
        # Check if randomized search is enabled in the model parameters
        use_randomized_search = self.model_params.pop('use_randomized_search', False)
        
        if use_randomized_search:
            # Get hyperparameter search config
            n_iter = self.model_params.pop('n_iter', 20)
            cv = self.model_params.pop('cv', 3)
            param_distributions = self.model_params.pop('param_distributions', None)
            
            # Use default param distributions if not provided
            if param_distributions is None:
                param_distributions = {
                    'n_estimators': [50, 100, 200, 300],
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['auto', 'sqrt', 'log2', 0.7],
                    'bootstrap': [True, False]
                }
            
            # Base estimator with fixed random_state
            base_rf = RandomForestRegressor(random_state=self.random_state, **self.model_params)
            
            with ProgressCallback(total=1, desc="Setting up RandomizedSearchCV") as pbar:
                logger.info(f"Setting up RandomizedSearchCV with {n_iter} iterations and {cv} folds")
                
                # Create the randomized search
                search = RandomizedSearchCV(
                    base_rf,
                    param_distributions=param_distributions,
                    n_iter=n_iter,
                    cv=cv,
                    scoring='neg_mean_squared_error',
                    n_jobs=-1,
                    random_state=self.random_state,
                    verbose=0,
                    return_train_score=True
                )
                pbar.update()
            
            # Fit the randomized search
            with ProgressCallback(total=1, desc="Training with RandomizedSearchCV") as pbar:
                search.fit(X, y)
                self.model = search.best_estimator_
                self.best_params_ = search.best_params_
                pbar.update()
                
            logger.info(f"Best hyperparameters: {self.best_params_}")
        else:
            # Create and train a standard RandomForestRegressor
            with ProgressCallback(total=1, desc="Training Random Forest") as pbar:
                self.model = RandomForestRegressor(
                    n_estimators=self.n_estimators,
                    max_depth=self.max_depth,
                    min_samples_split=self.min_samples_split,
                    min_samples_leaf=self.min_samples_leaf,
                    max_features=self.max_features,
                    bootstrap=self.bootstrap,
                    random_state=self.random_state,
                    n_jobs=-1,
                    **self.model_params
                )
                
                self.model.fit(X, y)
                pbar.update()
        
        return self
        
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate RMSF predictions.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predicted RMSF values
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        return self.model.predict(X)
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate RMSF predictions with standard deviation (uncertainty).
        
        Uses the variance of predictions across the ensemble of trees
        as a measure of prediction uncertainty.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_dev) arrays
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Make predictions from all trees
        predictions = np.array([tree.predict(X) for tree in self.model.estimators_])
        
        # Calculate mean and standard deviation
        mean_prediction = np.mean(predictions, axis=0)
        std_prediction = np.std(predictions, axis=0)
        
        return mean_prediction, std_prediction
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Random Forest ignores the method and n_trials parameters, using RandomizedSearchCV instead
        if method != "random":
            logger.warning(f"RandomForest only supports 'random' method for optimization, ignoring '{method}'")
            
        with ProgressCallback(total=1, desc="Hyperparameter optimization") as pbar:
            search = RandomizedSearchCV(
                RandomForestRegressor(random_state=self.random_state),
                param_distributions=param_grid,
                n_iter=n_trials,
                cv=cv,
                scoring='neg_mean_squared_error',
                n_jobs=-1,
                random_state=self.random_state,
                verbose=0,
                return_train_score=True
            )
            
            search.fit(X, y)
            pbar.update()
            
        # Update model with the best estimator
        self.model = search.best_estimator_
        self.best_params_ = search.best_params_
        
        logger.info(f"Best hyperparameters: {self.best_params_}")
        
        return self.best_params_
        
    def save(self, path: str) -> None:
        """
        Save model to disk using joblib.
        
        Args:
            path: Path to save location
        """
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
            
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save model state
        state = {
            'model': self.model,
            'feature_names': self.feature_names_,
            'best_params': self.best_params_,
            'params': {
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth,
                'min_samples_split': self.min_samples_split,
                'min_samples_leaf': self.min_samples_leaf,
                'max_features': self.max_features,
                'bootstrap': self.bootstrap,
                'random_state': self.random_state,
                'model_params': self.model_params
            }
        }
        
        joblib.dump(state, path)
        logger.info(f"Model saved to {path}")
        
    @classmethod
    def load(cls, path: str) -> 'RandomForestModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded RandomForestModel instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")
        
        try:
            state = joblib.load(path)
            
            # Create new instance with saved parameters
            params = state['params']
            instance = cls(
                n_estimators=params.get('n_estimators', 100),
                max_depth=params.get('max_depth', None),
                min_samples_split=params.get('min_samples_split', 2),
                min_samples_leaf=params.get('min_samples_leaf', 1),
                max_features=params.get('max_features', 0.7),
                bootstrap=params.get('bootstrap', True),
                random_state=params.get('random_state', 42),
                **params.get('model_params', {})
            )
            
            # Restore model and feature names
            instance.model = state['model']
            instance.feature_names_ = state.get('feature_names', None)
            instance.best_params_ = state.get('best_params', None)
            
            return instance
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
        
    def get_feature_importance(self, X_val=None, y_val=None) -> Dict[str, float]:
        """
        Get feature importance values using permutation importance.
        
        Args:
            X_val: Optional validation features for permutation importance
            y_val: Optional validation targets for permutation importance
            
        Returns:
            Dictionary mapping feature names to importance values
        """
        if self.model is None:
            return {}
        
        # If validation data is provided, use permutation importance
        if X_val is not None and y_val is not None and len(X_val) > 0:
            try:
                from sklearn.inspection import permutation_importance
                
                # Calculate permutation importance
                r = permutation_importance(
                    self.model, X_val, y_val, 
                    n_repeats=10, 
                    random_state=self.random_state
                )
                
                # Use mean importance as the feature importance
                importance_values = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                    return dict(zip(self.feature_names_, importance_values))
                else:
                    return {f"feature_{i}": imp for i, imp in enumerate(importance_values)}
                    
            except Exception as e:
                logger.warning(f"Could not compute permutation importance: {e}")
                # Fall back to built-in feature importance
        
        # Use built-in feature importance as fallback
        if hasattr(self.model, 'feature_importances_'):
            importance_values = self.model.feature_importances_
            
            if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                return dict(zip(self.feature_names_, importance_values))
            else:
                return {f"feature_{i}": importance for i, importance in enumerate(importance_values)}
        
        return {}
===== FILE: flexseq/models/neural_network.py =====
"""
Neural Network model implementation for the FlexSeq ML pipeline.

This module provides a PyTorch-based neural network model
for protein flexibility prediction, with support for
hyperparameter optimization.
"""

import os
import logging
import json
from typing import Dict, Any, Optional, Union, List, Tuple

import numpy as np
import pandas as pd
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import KFold

from flexseq.models import register_model
from flexseq.models.base import BaseModel
from flexseq.utils.helpers import ProgressCallback, progress_bar

logger = logging.getLogger(__name__)

class FlexibilityNN(nn.Module):
    """
    Neural network architecture for protein flexibility prediction.
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_layers: List[int] = [64, 32],
        activation: str = "relu",
        dropout: float = 0.2
    ):
        """
        Initialize the neural network architecture.
        
        Args:
            input_dim: Number of input features
            hidden_layers: List of hidden layer sizes
            activation: Activation function to use
            dropout: Dropout rate
        """
        super(FlexibilityNN, self).__init__()
        
        # Define activation function
        if activation.lower() == "relu":
            act_fn = nn.ReLU()
        elif activation.lower() == "leaky_relu":
            act_fn = nn.LeakyReLU()
        elif activation.lower() == "tanh":
            act_fn = nn.Tanh()
        elif activation.lower() == "sigmoid":
            act_fn = nn.Sigmoid()
        else:
            act_fn = nn.ReLU()
            logger.warning(f"Unknown activation function '{activation}', using ReLU")
        
        # Create layers
        layers = []
        prev_dim = input_dim
        
        # Hidden layers
        for i, dim in enumerate(hidden_layers):
            layers.append(nn.Linear(prev_dim, dim))
            layers.append(act_fn)
            layers.append(nn.Dropout(dropout))
            prev_dim = dim
        
        # Output layer (single value for RMSF prediction)
        layers.append(nn.Linear(prev_dim, 1))
        
        # Create sequential model
        self.model = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the network.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor
        """
        return self.model(x).squeeze()

@register_model("neural_network")
class NeuralNetworkModel(BaseModel):
    """
    Neural Network model for protein flexibility prediction.
    
    This model uses a feed-forward neural network to learn complex
    relationships between protein features and flexibility.
    """
    
    def __init__(
        self,
        architecture: Dict[str, Any] = None,
        training: Dict[str, Any] = None,
        random_state: int = 42,
        **kwargs
    ):
        """
        Initialize the Neural Network model.
        
        Args:
            architecture: Dictionary of architecture parameters
            training: Dictionary of training parameters
            random_state: Random seed for reproducibility
            **kwargs: Additional parameters
        """
        # Set default architecture if not provided
        if architecture is None:
            architecture = {
                "hidden_layers": [64, 32],
                "activation": "relu",
                "dropout": 0.2
            }
        
        # Set default training parameters if not provided
        if training is None:
            training = {
                "optimizer": "adam",
                "learning_rate": 0.001,
                "batch_size": 32,
                "epochs": 100,
                "early_stopping": True,
                "patience": 10
            }
        
        self.architecture = architecture
        self.training = training
        self.random_state = random_state
        self.model = None
        self.feature_names_ = None
        self.scaler = None
        self.history = None
        
        # Set random seeds for reproducibility
        torch.manual_seed(random_state)
        np.random.seed(random_state)
        
        # Device configuration
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
    
    def _create_model(self, input_dim: int) -> FlexibilityNN:
        """
        Create the neural network model.
        
        Args:
            input_dim: Number of input features
            
        Returns:
            Initialized FlexibilityNN model
        """
        model = FlexibilityNN(
            input_dim=input_dim,
            hidden_layers=self.architecture.get("hidden_layers", [64, 32]),
            activation=self.architecture.get("activation", "relu"),
            dropout=self.architecture.get("dropout", 0.2)
        )
        return model.to(self.device)
    
    def _get_optimizer(self, model: FlexibilityNN) -> optim.Optimizer:
        """
        Get the appropriate optimizer.
        
        Args:
            model: The neural network model
            
        Returns:
            Configured optimizer
        """
        optimizer_name = self.training.get("optimizer", "adam").lower()
        lr = self.training.get("learning_rate", 0.001)
        
        if optimizer_name == "adam":
            return optim.Adam(model.parameters(), lr=lr)
        elif optimizer_name == "sgd":
            return optim.SGD(model.parameters(), lr=lr)
        elif optimizer_name == "rmsprop":
            return optim.RMSprop(model.parameters(), lr=lr)
        else:
            logger.warning(f"Unknown optimizer '{optimizer_name}', using Adam")
            return optim.Adam(model.parameters(), lr=lr)
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], feature_names: Optional[List[str]] = None) -> 'NeuralNetworkModel':
        """
        Train the Neural Network model.
        
        Args:
            X: Feature matrix
            y: Target RMSF values
            feature_names: Optional list of feature names
            
        Returns:
            Self, for method chaining
        """
        # Store feature names if available
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        elif feature_names is not None:
            self.feature_names_ = feature_names
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Convert target to numpy array if needed
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Scale features using StandardScaler
        from sklearn.preprocessing import StandardScaler
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_array)
        
        # Create PyTorch tensors
        X_tensor = torch.FloatTensor(X_scaled)
        y_tensor = torch.FloatTensor(y_array)
        
        # Create validation split (20% of training data)
        from sklearn.model_selection import train_test_split
        train_indices, val_indices = train_test_split(
            np.arange(len(X_scaled)), 
            test_size=0.2, 
            random_state=self.random_state
        )
        
        # Create dataset and dataloader for training
        train_dataset = TensorDataset(X_tensor[train_indices], y_tensor[train_indices])
        batch_size = self.training.get("batch_size", 32)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # Initialize model
        input_dim = X_array.shape[1]
        self.model = self._create_model(input_dim)
        
        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = self._get_optimizer(self.model)
        
        # Training parameters
        epochs = self.training.get("epochs", 100)
        early_stopping = self.training.get("early_stopping", True)
        patience = self.training.get("patience", 10)
        
        # Initialize training history
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_r2': [],
            'val_r2': [],
            'learning_rate': []
        }
        
        # Training loop
        best_loss = float('inf')
        patience_counter = 0
        
        # Use progress bar for epochs
        epoch_pbar = progress_bar(range(epochs), desc="Training NN")
        
        for epoch in epoch_pbar:
            # Training phase
            self.model.train()
            train_running_loss = 0.0
            train_preds = []
            train_targets = []
            
            # Track batch progress
            batch_pbar = progress_bar(train_dataloader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)
            
            for batch_X, batch_y in batch_pbar:
                # Move tensors to the configured device
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                # Forward pass
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                
                # Backward pass and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                train_running_loss += loss.item() * batch_X.size(0)
                
                # Collect predictions and targets for R² calculation
                train_preds.append(outputs.detach().cpu().numpy())
                train_targets.append(batch_y.detach().cpu().numpy())
                
                # Update batch progress bar
                batch_pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            # Compute average epoch loss for training
            train_epoch_loss = train_running_loss / len(train_dataset)
            
            # Concatenate predictions and targets for full training set R²
            train_preds = np.concatenate(train_preds)
            train_targets = np.concatenate(train_targets)
            
            # Calculate R² for training set
            from sklearn.metrics import r2_score
            train_r2 = r2_score(train_targets, train_preds)
            
            # Validation phase
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                val_X = X_tensor[val_indices].to(self.device)
                val_y = y_tensor[val_indices].to(self.device)
                val_outputs = self.model(val_X)
                val_batch_loss = criterion(val_outputs, val_y)
                val_loss = val_batch_loss.item()
                
                # Calculate R² for validation set
                val_preds = val_outputs.cpu().numpy()
                val_targets = val_y.cpu().numpy()
                val_r2 = r2_score(val_targets, val_preds)
            
            # Store metrics in history
            self.history['train_loss'].append(train_epoch_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_r2'].append(train_r2)
            self.history['val_r2'].append(val_r2)
            self.history['learning_rate'].append(self.training.get("learning_rate", 0.001))
            
            # Update epoch progress bar
            epoch_pbar.set_postfix(
                train_loss=f"{train_epoch_loss:.4f}",
                val_loss=f"{val_loss:.4f}",
                val_r2=f"{val_r2:.4f}"
            )
            
            # Early stopping check
            if early_stopping:
                if val_loss < best_loss:
                    best_loss = val_loss
                    patience_counter = 0
                    # Save best model state
                    best_state = {
                        'model_state': self.model.state_dict(),
                        'input_dim': input_dim
                    }
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logger.info(f"Early stopping at epoch {epoch+1}")
                        # Restore best model state
                        self.model.load_state_dict(best_state['model_state'])
                        break
        
        return self
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Generate RMSF predictions.
        
        Args:
            X: Feature matrix
            
        Returns:
            Array of predicted RMSF values
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to evaluation mode
        self.model.eval()
        
        # Make predictions
        with torch.no_grad():
            predictions = self.model(X_tensor).cpu().numpy()
        
        return predictions
    
    def predict_with_std(self, X: Union[np.ndarray, pd.DataFrame]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate predictions with uncertainty estimates using MC Dropout.
        
        Args:
            X: Feature matrix
            
        Returns:
            Tuple of (predictions, std_deviation)
        """
        if self.model is None:
            raise RuntimeError("Model must be trained before prediction")
        
        # Convert to numpy array if DataFrame
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        
        # Scale features
        X_scaled = self.scaler.transform(X_array)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        # Set model to training mode to enable dropout for MC Dropout
        self.model.train()
        
        # Perform multiple forward passes with dropout enabled
        n_samples = 30  # Number of MC samples
        samples = []
        
        with torch.no_grad():  # No gradients needed
            for _ in range(n_samples):
                predictions = self.model(X_tensor).cpu().numpy()
                samples.append(predictions)
        
        # Calculate mean and standard deviation across samples
        samples = np.stack(samples, axis=0)
        mean_prediction = np.mean(samples, axis=0)
        std_prediction = np.std(samples, axis=0)
        
        return mean_prediction, std_prediction
    
    def hyperparameter_optimize(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        method: str = "bayesian",
        n_trials: int = 20,
        cv: int = 3
    ) -> Dict[str, Any]:
        """
        Perform hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid or distributions
            method: Optimization method ("grid", "random", or "bayesian")
            n_trials: Number of trials for random or bayesian methods
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        if method.lower() == "bayesian":
            # Try to use optuna for Bayesian optimization
            try:
                import optuna
                logger.info("Using Optuna for Bayesian hyperparameter optimization")
                return self._bayesian_optimization(X, y, param_grid, n_trials, cv)
            except ImportError:
                logger.warning("Optuna not available, falling back to random search")
                method = "random"
        
        if method.lower() == "random":
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        elif method.lower() == "grid":
            return self._grid_optimization(X, y, param_grid, cv)
        
        else:
            logger.warning(f"Unknown optimization method '{method}', using random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _random_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform random search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Create parameter combinations
        from itertools import product
        import random
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        # Limit to n_trials
        if len(param_combinations) > n_trials:
            random.seed(self.random_state)
            param_combinations = random.sample(param_combinations, n_trials)
            
        logger.info(f"Performing random search with {len(param_combinations)} parameter combinations")
        
        # Train and evaluate each combination
        results = []
        
        for i, params in enumerate(progress_bar(param_combinations, desc="Parameter combinations")):
            # Extract architecture and training params
            arch_params = {
                'hidden_layers': params.get('hidden_layers', self.architecture.get('hidden_layers', [64, 32])),
                'activation': params.get('activation', self.architecture.get('activation', 'relu')),
                'dropout': params.get('dropout', self.architecture.get('dropout', 0.2))
            }
            
            train_params = {
                'optimizer': params.get('optimizer', self.training.get('optimizer', 'adam')),
                'learning_rate': params.get('learning_rate', self.training.get('learning_rate', 0.001)),
                'batch_size': params.get('batch_size', self.training.get('batch_size', 32)),
                'epochs': params.get('epochs', self.training.get('epochs', 100)),
                'early_stopping': params.get('early_stopping', self.training.get('early_stopping', True)),
                'patience': params.get('patience', self.training.get('patience', 10))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=arch_params,
                    training=train_params,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            # Store results
            mean_score = np.mean(cv_scores)
            results.append((mean_score, arch_params, train_params))
            logger.debug(f"Parameters: {params}, Score: {mean_score:.4f}")
        
        # Find best parameters
        results.sort(reverse=True)  # Higher score is better
        best_score, best_arch, best_train = results[0]
        
        logger.info(f"Best score: {best_score:.4f}")
        logger.info(f"Best architecture parameters: {best_arch}")
        logger.info(f"Best training parameters: {best_train}")
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def _grid_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform grid search hyperparameter optimization.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        # Grid search is same as random search but with all combinations
        from itertools import product
        
        # Get all possible combinations
        param_combinations = []
        
        # If hidden_layers is in the grid
        if 'hidden_layers' in param_grid:
            hidden_layer_options = param_grid['hidden_layers']
            rest_params = {k: v for k, v in param_grid.items() if k != 'hidden_layers'}
        else:
            hidden_layer_options = [self.architecture.get('hidden_layers', [64, 32])]
            rest_params = param_grid
            
        # Get combinations of the rest
        keys = list(rest_params.keys())
        values = list(rest_params.values())
        
        for hidden_layer in hidden_layer_options:
            for combination in product(*values):
                param_combo = dict(zip(keys, combination))
                param_combo['hidden_layers'] = hidden_layer
                param_combinations.append(param_combo)
                
        logger.info(f"Performing grid search with {len(param_combinations)} parameter combinations")
        
        # Use random optimization with all combinations
        n_trials = len(param_combinations)
        return self._random_optimization(X, y, param_grid, n_trials, cv)
    
    def _bayesian_optimization(
        self, 
        X: Union[np.ndarray, pd.DataFrame], 
        y: Union[np.ndarray, pd.Series],
        param_grid: Dict[str, Any],
        n_trials: int,
        cv: int
    ) -> Dict[str, Any]:
        """
        Perform Bayesian hyperparameter optimization using Optuna.
        
        Args:
            X: Feature matrix
            y: Target values
            param_grid: Parameter grid
            n_trials: Number of trials
            cv: Number of cross-validation folds
            
        Returns:
            Dictionary with best parameters
        """
        try:
            import optuna
        except ImportError:
            logger.warning("Optuna not available, falling back to random search")
            return self._random_optimization(X, y, param_grid, n_trials, cv)
        
        # Convert to numpy arrays
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
            
        if isinstance(y, pd.Series):
            y_array = y.values
        else:
            y_array = y
        
        # Create KFold cross-validator
        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # Define objective function for Optuna
        def objective(trial):
            # Suggest hyperparameters
            hidden_layers_options = param_grid.get('hidden_layers', [[64, 32], [128, 64], [32, 16]])
            architecture = {
                'hidden_layers': trial.suggest_categorical('hidden_layers', hidden_layers_options),
                'activation': trial.suggest_categorical('activation', param_grid.get('activation', ['relu', 'leaky_relu'])),
                'dropout': trial.suggest_float('dropout', 
                                            low=min(param_grid.get('dropout', [0.1, 0.5])),
                                            high=max(param_grid.get('dropout', [0.1, 0.5])))
            }
            
            learning_rates = param_grid.get('learning_rate', [0.0001, 0.01])
            batch_sizes = param_grid.get('batch_size', [16, 32, 64])
            patience_range = param_grid.get('patience', [5, 15])
            epochs_range = param_grid.get('epochs', [50, 100])
            optimizer_options = param_grid.get('optimizer', ['adam', 'rmsprop'])
            
            training = {
                'optimizer': trial.suggest_categorical('optimizer', optimizer_options),
                'learning_rate': trial.suggest_float('learning_rate', 
                                                low=min(learning_rates),
                                                high=max(learning_rates),
                                                log=True),
                'batch_size': trial.suggest_categorical('batch_size', batch_sizes),
                'early_stopping': True,
                'patience': trial.suggest_int('patience', 
                                            low=min(patience_range),
                                            high=max(patience_range)),
                'epochs': trial.suggest_int('epochs', 
                                        low=min(epochs_range),
                                        high=max(epochs_range))
            }
            
            # Perform cross-validation
            cv_scores = []
            
            for train_idx, val_idx in kf.split(X_array):
                X_train, X_val = X_array[train_idx], X_array[val_idx]
                y_train, y_val = y_array[train_idx], y_array[val_idx]
                
                # Create and train model
                model = NeuralNetworkModel(
                    architecture=architecture,
                    training=training,
                    random_state=self.random_state
                )
                
                # Limit epochs for CV
                model.training['epochs'] = min(model.training['epochs'], 30)
                
                # Train model
                model.fit(X_train, y_train)
                
                # Evaluate on validation set
                preds = model.predict(X_val)
                from sklearn.metrics import mean_squared_error
                score = -mean_squared_error(y_val, preds)  # Negative MSE (higher is better)
                cv_scores.append(score)
            
            return np.mean(cv_scores)
        
        # Create and run Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        # Get best parameters
        best_params = study.best_params
        logger.info(f"Best score: {study.best_value:.4f}")
        logger.info(f"Best parameters: {best_params}")
        
        # Construct best parameters dictionary
        best_hidden_layers = param_grid.get('hidden_layers', [[64, 32], [128, 64], [32, 16]])
        hidden_layers_idx = 0
        if 'hidden_layers' in best_params:
            for i, layers in enumerate(param_grid.get('hidden_layers', [])):
                if str(layers) == str(best_params['hidden_layers']):
                    hidden_layers_idx = i
                    break
        
        best_arch = {
            'hidden_layers': param_grid.get('hidden_layers', [[64, 32]])[hidden_layers_idx] if param_grid.get('hidden_layers') else [64, 32],
            'activation': best_params.get('activation', self.architecture.get('activation', 'relu')),
            'dropout': best_params.get('dropout', self.architecture.get('dropout', 0.2))
        }
        
        best_train = {
            'optimizer': best_params.get('optimizer', self.training.get('optimizer', 'adam')),
            'learning_rate': best_params.get('learning_rate', self.training.get('learning_rate', 0.001)),
            'batch_size': best_params.get('batch_size', self.training.get('batch_size', 32)),
            'early_stopping': True,
            'patience': best_params.get('patience', self.training.get('patience', 10)),
            'epochs': best_params.get('epochs', self.training.get('epochs', 100))
        }
        
        # Update model parameters
        self.architecture = best_arch
        self.training = best_train
        
        # Train final model on all data
        self.fit(X, y)
        
        # Return combined parameters
        return {**best_arch, **best_train}
    
    def save(self, path: str) -> None:
        """
        Save model to disk.
        
        Args:
            path: Path to save location
        """
        if self.model is None:
            raise RuntimeError("Cannot save untrained model")
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save model state
        state = {
            'model_state': self.model.state_dict(),
            'architecture': self.architecture,
            'training': self.training,
            'random_state': self.random_state,
            'feature_names': self.feature_names_,
            'scaler': self.scaler,
            'history': self.history,
            'input_dim': self.model.model[0].in_features  # Get input dimension from first layer
        }
        
        # Save state
        torch.save(state, path)
        logger.info(f"Model saved to {path}")
        
        # Save training history separately as CSV
        if self.history:
            history_path = os.path.splitext(path)[0] + "_history.csv"
            history_df = pd.DataFrame(self.history)
            history_df.to_csv(history_path, index=False)
            logger.info(f"Training history saved to {history_path}")
    
    @classmethod
    def load(cls, path: str) -> 'NeuralNetworkModel':
        """
        Load model from disk.
        
        Args:
            path: Path to saved model
            
        Returns:
            Loaded NeuralNetworkModel instance
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")
        
        try:
            # Load state dictionary
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            state = torch.load(path, map_location=device)
            
            # Create new instance with saved parameters
            instance = cls(
                architecture=state['architecture'],
                training=state['training'],
                random_state=state['random_state']
            )
            
            # Restore feature names, scaler, and history
            instance.feature_names_ = state['feature_names']
            instance.scaler = state['scaler']
            instance.history = state.get('history')
            
            # Create and restore model
            input_dim = state['input_dim']
            instance.model = instance._create_model(input_dim)
            instance.model.load_state_dict(state['model_state'])
            instance.model.eval()
            
            # Load training history if available
            history_path = os.path.splitext(path)[0] + "_history.csv"
            if os.path.exists(history_path):
                try:
                    history_df = pd.read_csv(history_path)
                    instance.history = {col: history_df[col].tolist() for col in history_df.columns}
                except Exception as e:
                    logger.warning(f"Could not load training history: {e}")
            
            return instance
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def get_feature_importance(self, X_val=None, y_val=None) -> Optional[Dict[str, float]]:
        """
        Get feature importance values using permutation importance.
        
        Args:
            X_val: Optional validation features for permutation importance
            y_val: Optional validation targets for permutation importance
            
        Returns:
            Dictionary mapping feature names to importance values or None
        """
        if self.model is None:
            return None
        
        # If validation data is provided, use permutation importance
        if X_val is not None and y_val is not None and len(X_val) > 0:
            try:
                from sklearn.inspection import permutation_importance
                
                # Set model to evaluation mode
                self.model.eval()
                
                # Define a prediction function for permutation importance
                def predict_fn(X_test):
                    X_tensor = torch.FloatTensor(self.scaler.transform(X_test)).to(self.device)
                    with torch.no_grad():
                        return self.model(X_tensor).cpu().numpy()
                
                # Calculate permutation importance
                r = permutation_importance(
                    predict_fn, X_val, y_val, 
                    n_repeats=10, 
                    random_state=self.random_state
                )
                
                # Use mean importance as the feature importance
                importance_values = r.importances_mean
                
                # Map to feature names if available
                if self.feature_names_ is not None and len(self.feature_names_) == len(importance_values):
                    return dict(zip(self.feature_names_, importance_values))
                else:
                    return {f"feature_{i}": imp for i, imp in enumerate(importance_values)}
                    
            except Exception as e:
                logger.warning(f"Could not compute permutation importance: {e}")
                # Fall back to weight-based importance
        
        # Fall back to weight-based importance
        try:
            # Get weights from the first layer
            first_layer = self.model.model[0]
            weights = first_layer.weight.data.cpu().numpy()
            
            # Use absolute values of weights as importance
            importance = np.mean(np.abs(weights), axis=0)
            
            # Map to feature names if available
            if self.feature_names_ is not None and len(self.feature_names_) == len(importance):
                return dict(zip(self.feature_names_, importance))
            else:
                return {f"feature_{i}": imp for i, imp in enumerate(importance)}
                
        except Exception as e:
            logger.warning(f"Could not compute feature importance: {e}")
            return None
    
    def get_training_history(self) -> Optional[Dict[str, List[float]]]:
        """
        Get training history if available.
        
        Returns:
            Dictionary with training metrics by epoch, or None if not available
        """
        return self.history
### Data Handling Files ###
---------------------------------------------------------
===== FILE: flexseq/data/__init__.py =====
"""
Data handling modules for the FlexSeq ML pipeline.

This package contains functions for loading, processing, and
manipulating protein data across multiple temperatures.
"""

# Import key functions for easier access
from flexseq.data.loader import load_file, list_data_files, get_temperature_files
from flexseq.data.processor import (
    load_and_process_data, 
    clean_data, 
    prepare_data_for_model
)
===== FILE: flexseq/data/loader.py =====
"""
Data loading utilities for the FlexSeq ML pipeline.

This module provides functions for loading protein data from various formats,
with special support for temperature-specific files.
"""

import os
import logging
import re
from typing import List, Dict, Any, Optional, Union, Tuple
from functools import lru_cache

import pandas as pd
import numpy as np
import glob


logger = logging.getLogger(__name__)

def list_data_files(data_dir: str, file_pattern: str) -> List[str]:
    """
    List data files matching a pattern in a directory.
    
    Args:
        data_dir: Directory to search
        file_pattern: File pattern to match
        
    Returns:
        List of file paths
    """
    
    # Get absolute path
    data_dir = os.path.abspath(data_dir)
    
    # Find matching files
    pattern = os.path.join(data_dir, file_pattern)
    matching_files = glob.glob(pattern)
    
    if not matching_files:
        logger.warning(f"No files found matching pattern {pattern}")
    
    return matching_files

def get_temperature_files(data_dir: str, file_pattern: str = "temperature_*.csv") -> Dict[Union[int, str], str]:
    """
    Get a dictionary mapping temperature values to file paths.
    
    Args:
        data_dir: Directory to search
        file_pattern: File pattern to match
        
    Returns:
        Dictionary mapping temperature values to file paths
    """
    # Get all matching files
    files = list_data_files(data_dir, file_pattern)
    
    # Extract temperatures from filenames
    temperature_files = {}
    
    for file_path in files:
        filename = os.path.basename(file_path)
        
        # Try to extract temperature from filename
        # Pattern: temperature_{temp}_train.csv
        match = re.match(r"temperature_(\d+|average)_.*\.csv", filename)
        
        if match:
            temp_str = match.group(1)
            
            # Convert to int if numeric, keep as string for "average"
            temperature = int(temp_str) if temp_str.isdigit() else temp_str
            
            temperature_files[temperature] = file_path
    
    if not temperature_files:
        logger.warning(f"No temperature files found in {data_dir}")
    
    return temperature_files

def detect_file_format(file_path: str) -> str:
    """
    Detect file format based on extension and content.
    
    Args:
        file_path: Path to data file
        
    Returns:
        Format string ('csv', 'tsv', 'pickle', etc.)
    """
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    
    if ext == '.csv':
        return 'csv'
    elif ext == '.tsv':
        return 'tsv'
    elif ext in ['.pkl', '.pickle']:
        return 'pickle'
    elif ext == '.json':
        return 'json'
    elif ext == '.parquet':
        return 'parquet'
    elif ext == '.h5':
        return 'hdf5'
    else:
        # Try to detect CSV/TSV by reading first line
        try:
            with open(file_path, 'r') as f:
                first_line = f.readline()
                if '\t' in first_line:
                    return 'tsv'
                elif ',' in first_line:
                    return 'csv'
        except:
            pass
        
        # Default to CSV if can't determine
        logger.warning(f"Could not determine format for {file_path}, defaulting to CSV")
        return 'csv'

@lru_cache(maxsize=16)
def load_file(file_path: str, **kwargs) -> pd.DataFrame:
    """
    Load data from a file with format auto-detection.
    
    Args:
        file_path: Path to data file
        **kwargs: Additional arguments to pass to pandas
        
    Returns:
        Loaded DataFrame
        
    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file format is not supported or loading fails
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found: {file_path}")
    
    # Detect format
    file_format = detect_file_format(file_path)
    
    try:
        # Load based on format
        if file_format == 'csv':
            return pd.read_csv(file_path, **kwargs)
        elif file_format == 'tsv':
            return pd.read_csv(file_path, sep='\t', **kwargs)
        elif file_format == 'pickle':
            return pd.read_pickle(file_path, **kwargs)
        elif file_format == 'json':
            return pd.read_json(file_path, **kwargs)
        elif file_format == 'parquet':
            return pd.read_parquet(file_path, **kwargs)
        elif file_format == 'hdf5':
            return pd.read_hdf(file_path, **kwargs)
        else:
            raise ValueError(f"Unsupported file format: {file_format}")
    except Exception as e:
        logger.error(f"Error loading file {file_path}: {e}")
        raise ValueError(f"Failed to load file {file_path}: {e}")

def merge_data_files(file_paths: List[str], **kwargs) -> pd.DataFrame:
    """
    Merge multiple data files into a single DataFrame.
    
    Args:
        file_paths: List of paths to data files
        **kwargs: Additional arguments to pass to pandas
        
    Returns:
        Merged DataFrame
    """
    if not file_paths:
        raise ValueError("No files provided for merging")
    
    # Load and concatenate files
    dfs = []
    
    for file_path in file_paths:
        try:
            df = load_file(file_path, **kwargs)
            dfs.append(df)
        except Exception as e:
            logger.warning(f"Skipping file {file_path} due to error: {e}")
    
    if not dfs:
        raise ValueError("No data files could be loaded")
    
    return pd.concat(dfs, ignore_index=True)

def validate_data_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
    """
    Validate that DataFrame contains all required columns.
    
    Args:
        df: DataFrame to validate
        required_columns: List of required column names
        
    Returns:
        True if all required columns are present, False otherwise
    """
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.warning(f"Missing required columns: {missing_columns}")
        return False
    
    return True

def load_temperature_data(
    config: Dict[str, Any],
    temperature: Optional[Union[int, str]] = None
) -> pd.DataFrame:
    """
    Load data for a specific temperature.
    
    Args:
        config: Configuration dictionary
        temperature: Optional temperature to load (if None, use config["temperature"]["current"])
        
    Returns:
        DataFrame for the specified temperature
        
    Raises:
        FileNotFoundError: If data file doesn't exist
        ValueError: If temperature is not in available temperatures
    """
    # Get temperature from config if not provided
    if temperature is None:
        temperature = config["temperature"]["current"]
    
    # Validate temperature
    available_temps = config["temperature"]["available"]
    if str(temperature) not in [str(t) for t in available_temps]:
        raise ValueError(f"Temperature {temperature} is not in the list of available temperatures: {available_temps}")
    
    # Get data directory and file pattern
    data_dir = config["paths"]["data_dir"]
    file_pattern = config["dataset"]["file_pattern"]
    
    # Replace {temperature} placeholder in file pattern
    file_pattern = file_pattern.replace("{temperature}", str(temperature))
    
    # Find matching file
    matching_files = list_data_files(data_dir, file_pattern)
    
    if not matching_files:
        raise FileNotFoundError(f"No data file found for temperature {temperature} with pattern {file_pattern}")
    
    # Use the first matching file
    file_path = matching_files[0]
    
    # Load data
    df = load_file(file_path)
    
    return df

def load_all_temperature_data(config: Dict[str, Any]) -> Dict[Union[int, str], pd.DataFrame]:
    """
    Load data for all available temperatures.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Dictionary mapping temperature values to DataFrames
    """
    # Get available temperatures
    available_temps = config["temperature"]["available"]
    
    # Load data for each temperature
    temperature_data = {}
    
    for temp in available_temps:
        try:
            df = load_temperature_data(config, temp)
            temperature_data[temp] = df
        except Exception as e:
            logger.warning(f"Error loading data for temperature {temp}: {e}")
    
    if not temperature_data:
        raise ValueError("No temperature data could be loaded")
    
    return temperature_data

def summarize_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Generate summary statistics for a dataset.
    
    Args:
        df: Input DataFrame
        
    Returns:
        Dictionary of summary statistics
    """
    summary = {
        "num_rows": len(df),
        "num_columns": len(df.columns),
        "columns": list(df.columns),
        "memory_usage": None,
        "domains": None,
        "residues_per_domain": None,
        "column_types": {},
        "missing_values": {},
    }
    
    # Memory usage
    try:
        memory_bytes = df.memory_usage(deep=True).sum()
        
        if memory_bytes < 1024:
            summary["memory_usage"] = f"{memory_bytes} bytes"
        elif memory_bytes < 1024**2:
            summary["memory_usage"] = f"{memory_bytes / 1024:.2f} KB"
        elif memory_bytes < 1024**3:
            summary["memory_usage"] = f"{memory_bytes / (1024**2):.2f} MB"
        else:
            summary["memory_usage"] = f"{memory_bytes / (1024**3):.2f} GB"
    except:
        pass
    
    # Domain statistics if domain_id is present
    if "domain_id" in df.columns:
        domains = df["domain_id"].unique()
        summary["domains"] = {
            "count": len(domains),
            "examples": list(domains[:5])
        }
        
        # Residues per domain
        residue_counts = df.groupby("domain_id").size()
        summary["residues_per_domain"] = {
            "min": residue_counts.min(),
            "max": residue_counts.max(),
            "mean": residue_counts.mean(),
            "median": residue_counts.median()
        }
    
    # Column types and missing values
    for col in df.columns:
        summary["column_types"][col] = str(df[col].dtype)
        missing = df[col].isna().sum()
        if missing > 0:
            summary["missing_values"][col] = {
                "count": missing,
                "percentage": (missing / len(df)) * 100
            }
    
    # Check for temperature-specific RMSF columns
    rmsf_columns = [col for col in df.columns if col.startswith("rmsf_")]
    if rmsf_columns:
        summary["rmsf_columns"] = rmsf_columns
    
    # Check for OmniFlex specific columns
    omniflex_columns = ["esm_rmsf", "voxel_rmsf"]
    found_omniflex_columns = [col for col in omniflex_columns if col in df.columns]
    if found_omniflex_columns:
        summary["omniflex_columns"] = found_omniflex_columns
    
    return summary

def log_data_summary(summary: Dict[str, Any]) -> None:
    """
    Log a summary of dataset statistics.
    
    Args:
        summary: Dictionary of summary statistics
    """
    logger.info("=== Dataset Summary ===")
    logger.info(f"Rows: {summary['num_rows']}, Columns: {summary['num_columns']}")
    
    if "memory_usage" in summary and summary["memory_usage"]:
        logger.info(f"Memory usage: {summary['memory_usage']}")
    
    if "domains" in summary and summary["domains"]:
        logger.info(f"Domains: {summary['domains']['count']} unique domains")
        logger.info(f"Examples: {', '.join(summary['domains']['examples'])}")
    
    if "residues_per_domain" in summary and summary["residues_per_domain"]:
        stats = summary["residues_per_domain"]
        logger.info(f"Residues per domain: min={stats['min']}, max={stats['max']}, mean={stats['mean']:.1f}")
    
    if "missing_values" in summary and summary["missing_values"]:
        missing = summary["missing_values"]
        if missing:
            logger.info("Columns with missing values:")
            for col, stats in missing.items():
                logger.info(f"  {col}: {stats['count']} missing ({stats['percentage']:.1f}%)")
        else:
            logger.info("No missing values detected")
    
    if "rmsf_columns" in summary:
        logger.info(f"RMSF columns: {', '.join(summary['rmsf_columns'])}")
    
    if "omniflex_columns" in summary:
        logger.info(f"OmniFlex columns: {', '.join(summary['omniflex_columns'])}")
    
    logger.info("========================")
===== FILE: flexseq/data/processor.py =====
# flexseq/data/processor.py
"""
Data processing for the FlexSeq ML pipeline.

This module provides functions for preprocessing protein data,
including cleaning, feature engineering, window-based feature generation,
data splitting, and preparation for machine learning models.
"""

import os
import logging
from functools import lru_cache
from typing import Dict, List, Tuple, Optional, Any, Union

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split # For split_data

# Assuming flexseq.data.loader provides these:
from flexseq.data.loader import load_file, load_temperature_data, lru_cache, validate_data_columns
from flexseq.utils.helpers import progress_bar # Assuming progress_bar helper exists

logger = logging.getLogger(__name__)

# --- Amino Acid and Secondary Structure Mappings ---
AA_MAP = {
    'ALA': 1, 'ARG': 2, 'ASN': 3, 'ASP': 4, 'CYS': 5, 'GLN': 6,
    'GLU': 7, 'GLY': 8, 'HIS': 12, 'HSD': 9, 'HSE': 10, 'HSP': 11, # Group Histidines
    'ILE': 13, 'LEU': 14, 'LYS': 15, 'MET': 16, 'PHE': 17, 'PRO': 23, 
    'SER': 18, 'THR': 19, 'TRP': 20, 'TYR': 21, 'VAL': 22,
    'UNK': 0, None: 0 # Handle unknown and potential None values
}

SS_MAP = {
    'H': 0, 'G': 0, 'I': 0, # Helices
    'E': 1, 'B': 1,         # Strands
    'T': 2, 'S': 2, 'C': 2, # Coil/Turn/Bend
    '-': 2, None: 2         # Unknown/None default to Coil
}

# --- Data Cleaning and Basic Feature Engineering ---

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans input DataFrame by handling missing values and ensuring types.

    Args:
        df: Input DataFrame with raw protein data.

    Returns:
        Cleaned DataFrame.
    """
    logger.debug(f"Starting data cleaning on DataFrame with shape {df.shape}.")
    cleaned_df = df.copy()

    # Define fill values for different column types/meanings
    fill_values = {
        'dssp': 'C',                         # Default SS to Coil
        'relative_accessibility': 0.5,       # Moderate accessibility
        'core_exterior': 'core',             # Assume core if unknown
        'phi': 0.0,                          # Neutral angle
        'psi': 0.0,                          # Neutral angle
        'esm_rmsf': lambda d: d.mean(),      # Fill with mean of existing values
        'voxel_rmsf': lambda d: d.mean()     # Fill with mean of existing values
    }

    for col, fill_val in fill_values.items():
        if col in cleaned_df.columns:
            if callable(fill_val): # Handle functions like mean()
                mean_val = fill_val(cleaned_df[col].dropna()) if not cleaned_df[col].dropna().empty else 0.0
                if col in ['esm_rmsf', 'voxel_rmsf']: # Ensure numeric type before filling
                     cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')
                cleaned_df[col].fillna(mean_val, inplace=True)
                logger.debug(f"Filled NaNs in '{col}' with calculated value: {mean_val:.4f}")
            else:
                 if col in ['esm_rmsf', 'voxel_rmsf', 'relative_accessibility', 'phi', 'psi']: # Ensure numeric type
                     cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')
                 cleaned_df[col].fillna(fill_val, inplace=True)
                 logger.debug(f"Filled NaNs in '{col}' with default value: {fill_val}")

    logger.debug(f"Data cleaning finished. DataFrame shape: {cleaned_df.shape}")
    return cleaned_df

def _add_derived_features(df: pd.DataFrame) -> pd.DataFrame:
    """Adds derived features like normalized position and encoded categories."""
    logger.debug("Adding derived features...")
    derived_df = df.copy()

    # 1. Normalized Residue Position
    if 'normalized_resid' not in derived_df.columns and 'resid' in derived_df.columns and 'domain_id' in derived_df.columns:
        # Calculate max - min per group, handle single-residue domains (max-min=0)
        grp = derived_df.groupby('domain_id')['resid']
        min_res = grp.transform('min')
        max_minus_min = (grp.transform('max') - min_res).replace(0, 1) # Avoid division by zero
        derived_df['normalized_resid'] = (derived_df['resid'] - min_res) / max_minus_min
        logger.debug("Added 'normalized_resid'.")

    # 2. Encoded Residue Name
    if 'resname' in derived_df.columns and 'resname_encoded' not in derived_df.columns:
        derived_df['resname_encoded'] = derived_df['resname'].map(AA_MAP).fillna(0).astype(int)
        logger.debug("Added 'resname_encoded'.")

    # 3. Encoded Core/Exterior
    if 'core_exterior' in derived_df.columns and 'core_exterior_encoded' not in derived_df.columns:
        # Ensure 'core' maps to 0, others (e.g., 'surface', 'exterior') to 1
        derived_df['core_exterior_encoded'] = derived_df['core_exterior'].apply(
            lambda x: 0 if isinstance(x, str) and x.lower() == 'core' else 1
        ).astype(int)
        logger.debug("Added 'core_exterior_encoded'.")

    # 4. Encoded Secondary Structure
    if 'dssp' in derived_df.columns and 'secondary_structure_encoded' not in derived_df.columns:
        # Apply mapping, fill potential NaNs introduced by unknown DSSP codes
        derived_df['secondary_structure_encoded'] = derived_df['dssp'].map(SS_MAP).fillna(2).astype(int)
        logger.debug("Added 'secondary_structure_encoded'.")

    # 5. Normalized Phi/Psi Angles
    if 'phi' in derived_df.columns and 'phi_norm' not in derived_df.columns:
        # Normalize angle to [-1, 1] using sine for cyclical nature
        derived_df['phi_norm'] = np.sin(np.radians(derived_df['phi'].fillna(0)))
        logger.debug("Added 'phi_norm'.")
    if 'psi' in derived_df.columns and 'psi_norm' not in derived_df.columns:
        derived_df['psi_norm'] = np.sin(np.radians(derived_df['psi'].fillna(0)))
        logger.debug("Added 'psi_norm'.")

    return derived_df


# --- Domain Filtering ---

def filter_domains(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Filters DataFrame based on domain inclusion/exclusion and size rules.

    Args:
        df: Input DataFrame with protein data.
        config: Configuration dictionary with domain filtering rules.

    Returns:
        Filtered DataFrame.
    """
    domain_config = config.get("dataset", {}).get("domains", {})
    if not domain_config: # No filtering rules specified
        return df.copy()

    filtered_df = df.copy()
    initial_rows = len(filtered_df)
    logger.debug(f"Starting domain filtering from {initial_rows} rows.")

    # Apply inclusion/exclusion lists
    include_domains = set(domain_config.get("include", []))
    exclude_domains = set(domain_config.get("exclude", []))

    if include_domains:
        filtered_df = filtered_df[filtered_df['domain_id'].isin(include_domains)]
        logger.debug(f"Applied inclusion filter: {len(filtered_df)} rows remain.")
    if exclude_domains:
        filtered_df = filtered_df[~filtered_df['domain_id'].isin(exclude_domains)]
        logger.debug(f"Applied exclusion filter: {len(filtered_df)} rows remain.")

    # Apply size filters (calculate sizes only if needed)
    min_size = domain_config.get("min_protein_size", 0)
    max_size = domain_config.get("max_protein_size") # None means no upper limit

    if min_size > 0 or max_size is not None:
        if filtered_df.empty:
             logger.warning("DataFrame is empty before size filtering.")
        else:
            domain_sizes = filtered_df.groupby('domain_id')['resid'].transform('size') # Efficient size calculation
            if min_size > 0:
                filtered_df = filtered_df[domain_sizes >= min_size]
                logger.debug(f"Applied min size filter ({min_size}): {len(filtered_df)} rows remain.")
            if max_size is not None:
                filtered_df = filtered_df[domain_sizes <= max_size]
                logger.debug(f"Applied max size filter ({max_size}): {len(filtered_df)} rows remain.")

    final_rows = len(filtered_df)
    if final_rows < initial_rows:
         logger.info(f"Domain filtering removed {initial_rows - final_rows} rows. {final_rows} rows remaining.")
    else:
         logger.debug("No rows removed by domain filtering.")

    return filtered_df

# --- Window Feature Generation (Optimized) ---

def create_window_features_optimized(
    df: pd.DataFrame,
    window_size: int,
    feature_cols: List[str]
) -> pd.DataFrame:
    """
    Creates window-based features using vectorized operations and pd.concat.

    This version is optimized to reduce DataFrame fragmentation warnings and
    improve performance compared to iterative column addition.

    Args:
        df: Input DataFrame with protein data, indexed uniquely.
        window_size: Number of residues on each side (k). Window is size 2k+1.
        feature_cols: List of *existing* feature column names in df to use.

    Returns:
        DataFrame with added window features.
    """
    if 'domain_id' not in df.columns or 'resid' not in df.columns:
        raise ValueError("DataFrame must contain 'domain_id' and 'resid' columns for windowing.")
    if window_size <= 0:
        logger.warning("Window size must be positive. Skipping window feature creation.")
        return df.copy()

    original_cols = set(df.columns)
    # Only use features that actually exist in the input df
    valid_feature_cols = [col for col in feature_cols if col in original_cols]

    if not valid_feature_cols:
        logger.warning("No valid feature columns found for windowing.")
        return df.copy()

    all_domain_windows_dfs = [] # Collect DataFrames of window features per domain
    logger.info(f"Creating window features (k={window_size}) for {len(valid_feature_cols)} features using optimized method...")

    # Ensure the DataFrame index is unique if it's not already the default RangeIndex
    if not isinstance(df.index, pd.RangeIndex) or not df.index.is_unique:
        logger.debug("Resetting index for reliable window feature alignment.")
        df_indexed = df.reset_index(drop=True) # Work with a re-indexed copy
    else:
        df_indexed = df # Use original if index is okay

    # Process each domain
    grouped_domains = df_indexed.groupby('domain_id', observed=False, sort=False) # sort=False might be faster
    for domain_id, domain_indices in progress_bar(grouped_domains.groups.items(), desc="Processing Domains"):
        domain_df = df_indexed.loc[domain_indices].sort_values('resid') # Essential to sort by position
        domain_window_series_dict = {} # Collect {col_name: pd.Series} for this domain

        for feature in valid_feature_cols:
            feature_values = domain_df[feature].values # Efficient NumPy array access
            dtype = feature_values.dtype
            # Use a sensible default fill value based on dtype (0 for int, NaN for float)
            default_fill = 0 if np.issubdtype(dtype, np.integer) else np.nan

            for offset in range(-window_size, window_size + 1):
                if offset == 0: continue # Skip self

                col_name = f"{feature}_offset_{offset}"
                # Pre-allocate array with default fill value
                offset_values = np.full_like(feature_values, fill_value=default_fill, dtype=dtype)

                # Apply shifts using slicing (vectorized)
                if offset < 0: # Look backwards (use values from start)
                    offset_values[-offset:] = feature_values[:offset]
                else: # Look forwards (use values towards end)
                    offset_values[:-offset] = feature_values[offset:]

                # Store as Series with the correct index from domain_df
                domain_window_series_dict[col_name] = pd.Series(offset_values, index=domain_df.index, name=col_name)

        # Combine all window series for this domain into one DataFrame
        if domain_window_series_dict:
            all_domain_windows_dfs.append(pd.concat(domain_window_series_dict.values(), axis=1))

    # Concatenate window features from all domains (aligns by original index)
    if not all_domain_windows_dfs:
         logger.warning("No window features were generated.")
         return df.copy() # Return a copy of the original df

    logger.info("Concatenating window features for all domains...")
    window_features_combined = pd.concat(all_domain_windows_dfs)

    # Merge window features back to the *original* df using the index
    result_df = pd.concat([df, window_features_combined], axis=1)

    # --- NaN Filling (After all columns are combined) ---
    all_new_col_names = list(window_features_combined.columns)
    logger.info(f"Added {len(all_new_col_names)} window feature columns. Filling NaNs...")

    # Pre-calculate fill values for base features to avoid repeated computation
    base_feature_fill_values = {}
    for col in all_new_col_names:
        base_feature = col.split('_offset_')[0]
        if base_feature not in base_feature_fill_values and base_feature in result_df.columns:
            if np.issubdtype(result_df[base_feature].dtype, np.integer):
                try: fill_val = result_df[base_feature].mode()[0]; logger.debug(f"Using mode ({fill_val}) for {base_feature}")
                except IndexError: fill_val = 0; logger.debug(f"Mode failed, using 0 for {base_feature}")
            elif np.issubdtype(result_df[base_feature].dtype, np.floating):
                 fill_val = result_df[base_feature].median(); logger.debug(f"Using median ({fill_val}) for {base_feature}")
                 if pd.isna(fill_val): fill_val = 0.0; logger.debug(f"Median failed, using 0.0 for {base_feature}")
            else: # Object or other types
                 try: fill_val = result_df[base_feature].mode()[0]; logger.debug(f"Using mode ({fill_val}) for {base_feature}")
                 except IndexError: fill_val = ''; logger.debug(f"Mode failed, using empty string for {base_feature}")
            base_feature_fill_values[base_feature] = fill_val

    # Fill NaNs using pre-calculated values
    for col in all_new_col_names:
        if col not in result_df.columns: continue # Should not happen

        base_feature = col.split('_offset_')[0]
        fill_val = base_feature_fill_values.get(base_feature, 0) # Default to 0 if base feature somehow missed

        nan_count = result_df[col].isna().sum()
        if nan_count > 0:
             result_df[col].fillna(fill_val, inplace=True)
             # logger.debug(f"Filled {nan_count} NaNs in '{col}' with {fill_val}.")


    logger.info("Window feature creation and NaN filling complete (Optimized).")
    # Concat generally produces a non-fragmented frame, but a copy ensures it
    # return result_df.copy() # Optionally add copy here for absolute certainty
    return result_df

# --- Main Processing Function ---

def process_features(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Processes features: cleaning, deriving features, adding window features.

    Args:
        df: Input DataFrame with raw protein data.
        config: Configuration dictionary.

    Returns:
        DataFrame with processed features ready for filtering/splitting.
    """
    try:
        logger.info(f"Starting feature processing for DataFrame with shape {df.shape}.")
        # 1. Basic Cleaning
        cleaned_df = clean_data(df)

        # 2. Add Derived Features (Encodings, Normalizations)
        derived_df = _add_derived_features(cleaned_df)

        # 3. Add Protein Size (if necessary and possible)
        if "protein_size" not in derived_df.columns and 'domain_id' in derived_df.columns and 'resid' in derived_df.columns:
            logger.debug("Calculating 'protein_size'...")
            derived_df["protein_size"] = derived_df.groupby("domain_id")["resid"].transform("count")
        elif "protein_size" not in derived_df.columns:
             logger.warning("Cannot calculate 'protein_size': missing 'domain_id' or 'resid'.")

        # 4. Ensure Target Column is Numeric and filled
        target_col = config["dataset"]["target"] # Already templated by load_config
        if target_col in derived_df.columns:
            initial_nan_count = derived_df[target_col].isna().sum()
            derived_df[target_col] = pd.to_numeric(derived_df[target_col], errors='coerce')
            # Fill NaNs in target with 0.0 - Check if this is appropriate for the task!
            # Alternatively, drop rows with NaN target: derived_df.dropna(subset=[target_col], inplace=True)
            fill_target_val = 0.0
            final_nan_count = derived_df[target_col].isna().sum()
            if final_nan_count > 0:
                derived_df[target_col].fillna(fill_target_val, inplace=True)
                logger.warning(f"Filled {final_nan_count} NaN values in target column '{target_col}' with {fill_target_val}.")
            elif initial_nan_count > 0 and final_nan_count == 0: # Only log if NaNs were actually present and filled
                 logger.debug(f"NaNs in target column '{target_col}' handled (filled/coerced).")

        else:
            logger.error(f"Target column '{target_col}' not found in data after cleaning/deriving features!")
            # Depending on requirements, either raise an error or return the df as is
            raise ValueError(f"Target column '{target_col}' missing.")

        # 5. Identify Active Features for Windowing (based on config *after* derivation)
        feature_config = config["dataset"]["features"]
        use_features_config = feature_config.get("use_features", {})
        active_features_for_windowing = []
        for feature, enabled in use_features_config.items():
            # Check if feature exists *now* (could be derived) and is enabled
            if enabled and feature in derived_df.columns:
                active_features_for_windowing.append(feature)
            elif enabled and feature not in derived_df.columns:
                logger.warning(f"Feature '{feature}' is enabled but not found after deriving features.")

        # 6. Add Window Features (using optimized method)
        window_config = feature_config.get("window", {})
        processed_df = derived_df # Start with the derived features df
        if window_config.get("enabled", False):
            window_size = window_config.get("size", 3)
            if window_size > 0 and active_features_for_windowing:
                 processed_df = create_window_features_optimized(
                     processed_df, window_size, active_features_for_windowing
                 )
            else:
                 logger.warning("Windowing enabled but size <= 0 or no active features found. Skipping.")
                 processed_df = processed_df.copy() # Ensure consistency by returning a copy
        else:
             processed_df = processed_df.copy() # Return a copy even if windowing disabled

        logger.info(f"Feature processing complete. Final shape: {processed_df.shape}")
        return processed_df

    except Exception as e:
        logger.exception(f"Feature processing failed catastrophically: {e}")
        # Return a copy of the original DataFrame on error to prevent side effects
        return df.copy()


# --- Data Preparation for Models ---

def prepare_data_for_model(
    df: pd.DataFrame,
    config: Dict[str, Any],
    include_target: bool = True
) -> Tuple[np.ndarray, Optional[np.ndarray], List[str]]:
    """
    Prepares final data matrices (X, y) and feature names for model input.

    Selects features based on config, including generated window features.

    Args:
        df: Processed DataFrame containing all potential features.
        config: Configuration dictionary.
        include_target: Whether to include the target variable (y) in the output.

    Returns:
        Tuple containing:
        - X (np.ndarray): Feature matrix.
        - y (Optional[np.ndarray]): Target vector (or None if include_target=False).
        - feature_names (List[str]): List of names for columns in X.

    Raises:
        ValueError: If the target column is missing when include_target is True,
                    or if no valid features are selected.
    """
    logger.debug("Preparing data for model...")
    feature_config = config["dataset"]["features"]
    use_features_config = feature_config.get("use_features", {})
    window_config = feature_config.get("window", {})
    window_enabled = window_config.get("enabled", False)
    window_size = window_config.get("size", 0)

    # 1. Determine base features enabled in config
    base_features_enabled = {
        feature for feature, enabled in use_features_config.items()
        if enabled and feature in df.columns
    }
    logger.debug(f"Base features enabled in config and present in data: {sorted(list(base_features_enabled))}")

    # 2. Determine window features to include (if enabled)
    window_feature_names = []
    if window_enabled and window_size > 0:
        for base_feature in base_features_enabled:
            for offset in range(-window_size, window_size + 1):
                if offset == 0: continue
                col_name = f"{base_feature}_offset_{offset}"
                if col_name in df.columns:
                    window_feature_names.append(col_name)
        logger.debug(f"Including {len(window_feature_names)} window features.")

    # 3. Combine base and window features
    final_feature_names = sorted(list(base_features_enabled)) + sorted(window_feature_names)

    # 4. Sanity check and prepare X matrix
    missing_final_features = [f for f in final_feature_names if f not in df.columns]
    if missing_final_features:
        logger.warning(f"Some selected features are missing from the final DataFrame: {missing_final_features}. They will be excluded.")
        final_feature_names = [f for f in final_feature_names if f in df.columns]

    if not final_feature_names:
        raise ValueError("No valid features selected or available for the model.")

    logger.info(f"Preparing feature matrix X with {len(final_feature_names)} columns.")
    # Select columns and convert to NumPy, ensuring numeric types where possible
    X_df = df[final_feature_names]
    # Attempt conversion, coercing errors - check dtypes afterwards
    for col in X_df.columns:
        if X_df[col].dtype == 'object':
             try: X_df[col] = pd.to_numeric(X_df[col], errors='coerce').fillna(0) # Fill coerced NaNs with 0
             except ValueError: logger.warning(f"Could not convert object column '{col}' to numeric. It might cause issues.")
        elif pd.api.types.is_bool_dtype(X_df[col].dtype):
             X_df[col] = X_df[col].astype(int) # Convert bools to int

    # Check final dtypes
    non_numeric_cols = X_df.select_dtypes(exclude=np.number).columns
    if len(non_numeric_cols) > 0:
        logger.warning(f"Non-numeric columns remain in feature matrix: {list(non_numeric_cols)}. Models might fail.")

    X = X_df.values

    # 5. Prepare target vector y (if requested)
    y = None
    if include_target:
        target_col = config["dataset"]["target"] # Assumes target name is correct
        if target_col in df.columns:
            # Ensure target is numeric and handle potential NaNs (e.g., fill with 0 or mean)
            y_series = pd.to_numeric(df[target_col], errors='coerce')
            nan_count = y_series.isna().sum()
            if nan_count > 0:
                fill_val = 0.0 # Or calculate mean/median: y_series.mean()
                y_series.fillna(fill_val, inplace=True)
                logger.warning(f"Filled {nan_count} NaNs in target column '{target_col}' with {fill_val} before creating y vector.")
            y = y_series.values
        else:
            raise ValueError(f"Target column '{target_col}' not found in DataFrame for y preparation.")

    logger.debug(f"Data preparation complete. X shape: {X.shape}, y shape: {y.shape if y is not None else 'None'}")
    return X, y, final_feature_names


# --- Main Loading and Processing Function ---

def load_and_process_data(
    data_path: Optional[str] = None,
    config: Dict[str, Any] = None,
    temperature: Optional[Union[int, str]] = None
) -> pd.DataFrame:
    """
    Loads data, validates required columns, processes features, and filters domains.

    This is the main entry point for getting processed data ready for splitting.

    Args:
        data_path: Path to data file (takes precedence if provided).
        config: Configuration dictionary (required if data_path is None).
        temperature: Optional specific temperature to load (overrides config).

    Returns:
        Processed and filtered DataFrame.

    Raises:
        FileNotFoundError: If the specified data file does not exist.
        ValueError: If config is missing when needed, or required columns are absent.
    """
    if not config:
         # Allow loading without config for basic preprocessing, but log heavily
         if data_path and os.path.exists(data_path):
              logger.warning("Configuration not provided to load_and_process_data. Performing basic loading and cleaning only.")
              try: df = _cached_load_data(data_path); return clean_data(df)
              except Exception as e: raise ValueError(f"Error loading data from {data_path}: {e}")
         else:
              raise ValueError("Config is required unless a valid data_path is provided.")

    # Determine the temperature to use
    current_temp = temperature if temperature is not None else config["temperature"]["current"]
    current_temp_str = str(current_temp)
    logger.info(f"Loading and processing data for temperature: {current_temp_str}")

    # Load initial data
    if data_path:
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Data file not found: {data_path}")
        logger.info(f"Loading data from specified path: {data_path}")
        try:
            df = _cached_load_data(data_path)
        except Exception as e:
            raise ValueError(f"Error loading data from {data_path}: {e}")
    else:
        logger.info(f"Loading data based on temperature config (T={current_temp_str}).")
        df = load_temperature_data(config, current_temp) # Handles file finding

    # --- Validation of Required Columns (after loading, before processing) ---
    required_cols_templates = config["dataset"]["features"].get("required", [])
    if not required_cols_templates:
         logger.warning("No required columns specified in config.")
    else:
        missing_cols = []
        for col_template in required_cols_templates:
            # Substitute temperature placeholder
            col_actual = col_template.replace("{temperature}", current_temp_str)
            if col_actual not in df.columns:
                missing_cols.append(col_actual)
        if missing_cols:
            raise ValueError(f"Loaded data is missing required columns for T={current_temp_str}: {missing_cols}")
        logger.debug("All required columns found in loaded data.")


    # --- Process Features (Cleaning, Deriving, Windowing) ---
    # This function now handles cleaning, derived features, and windowing internally
    processed_df = process_features(df, config) # Pass the config which contains the correct target name


    # --- Filter Domains ---
    filtered_df = filter_domains(processed_df, config)

    logger.info(f"Data loading and processing complete for T={current_temp_str}. Final shape: {filtered_df.shape}")
    return filtered_df

# --- Data Splitting ---

def split_data(
    df: pd.DataFrame,
    config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Splits data into train, validation, and test sets.

    Supports stratified splitting by domain ID to prevent data leakage.

    Args:
        df: Processed DataFrame with features and target.
        config: Configuration dictionary containing split parameters.

    Returns:
        Tuple of (train_df, val_df, test_df).
    """
    split_config = config["dataset"]["split"]
    test_size = split_config.get("test_size", 0.2)
    val_size = split_config.get("validation_size", 0.15) # Proportion of *original* data for validation
    random_state = config["system"].get("random_state", 42) # Use global random state
    stratify_by_domain = split_config.get("stratify_by_domain", True)

    logger.info(f"Splitting data: Test={test_size*100:.1f}%, Val={val_size*100:.1f}%, Stratify by domain={stratify_by_domain}")

    if stratify_by_domain:
        if 'domain_id' not in df.columns:
            raise ValueError("Cannot stratify by domain: 'domain_id' column missing.")

        unique_domains = df['domain_id'].unique()
        n_domains = len(unique_domains)
        logger.debug(f"Found {n_domains} unique domains for stratified splitting.")

        if n_domains < 3: # Need at least one domain in each split
             logger.warning("Too few domains for reliable stratified splitting. Falling back to random split.")
             stratify_by_domain = False # Override stratification

    if stratify_by_domain:
        # Calculate actual validation proportion relative to the training set size
        if (1 - test_size) <= 0: raise ValueError("test_size must be less than 1")
        val_ratio_of_train = val_size / (1 - test_size)
        if not (0 < val_ratio_of_train < 1):
             raise ValueError(f"Invalid split sizes: val_size ({val_size}) must be smaller than remaining train size ({1-test_size})")

        # Split domains into train+val / test
        domains_train_val, domains_test = train_test_split(
            unique_domains,
            test_size=test_size,
            random_state=random_state
        )
        # Split train+val domains into train / val
        domains_train, domains_val = train_test_split(
            domains_train_val,
            test_size=val_ratio_of_train,
            random_state=random_state # Use same seed for reproducibility at this stage too
        )

        logger.debug(f"Domain split: Train={len(domains_train)}, Val={len(domains_val)}, Test={len(domains_test)}")

        # Create DataFrames based on domain splits
        train_df = df[df['domain_id'].isin(domains_train)].copy()
        val_df = df[df['domain_id'].isin(domains_val)].copy()
        test_df = df[df['domain_id'].isin(domains_test)].copy()

    else: # Regular sample-based splitting (if stratification disabled or not possible)
        logger.info("Performing random sample split (not stratified by domain).")
         # Calculate actual validation proportion relative to the training set size
        if (1 - test_size) <= 0: raise ValueError("test_size must be less than 1")
        val_ratio_of_train = val_size / (1 - test_size)
        if not (0 < val_ratio_of_train < 1):
             raise ValueError(f"Invalid split sizes: val_size ({val_size}) must be smaller than remaining train size ({1-test_size})")

        # First split into (train + val) and test
        train_val_df, test_df = train_test_split(
            df,
            test_size=test_size,
            random_state=random_state
        )
        # Then split (train + val) into train and val
        train_df, val_df = train_test_split(
            train_val_df,
            test_size=val_ratio_of_train,
            random_state=random_state # Use same seed
        )

    logger.info(f"Split complete: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)} rows.")

    # Final check for empty splits
    if train_df.empty or val_df.empty or test_df.empty:
        logger.error("One or more data splits are empty! Check split sizes and data.")
        # Depending on desired behavior, could raise error or return empty DFs
        # raise ValueError("Empty data split generated.")

    return train_df, val_df, test_df
### Temperature Handling Files ###
---------------------------------------------------------
===== FILE: flexseq/temperature/__init__.py =====
"""
Temperature handling modules for the FlexSeq ML pipeline.

This package contains functions for managing and comparing protein
flexibility predictions across multiple temperatures.
"""

# from flexseq.temperature.comparison import (
#     compare_temperature_predictions,
#     calculate_temperature_correlations,
#     generate_temperature_metrics
# )
===== FILE: flexseq/temperature/comparison.py =====
# flexseq/temperature/comparison.py

import os
import logging
from typing import Dict, List, Tuple, Any, Optional, Union, Sequence

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import mean_squared_error, r2_score

from flexseq.data.loader import load_temperature_data
from flexseq.utils.helpers import progress_bar, ProgressCallback
import time

logger = logging.getLogger(__name__)

# --- Constants for Model Comparison ---
# Define the models we want to directly compare.
# Assumes columns like 'neural_network_predicted', 'random_forest_predicted' exist
# in the source all_results.csv files.
MODELS_TO_COMPARE = ['neural_network', 'random_forest']

# ============================================================================
# FUNCTION 1: compare_temperature_predictions (MODIFIED)
# ============================================================================
def compare_temperature_predictions(
    predictions: Dict[Union[int, str], pd.DataFrame],
    config: Dict[str, Any],
    model_names: List[str] # Now expects a list of models
) -> pd.DataFrame:
    """
    Compare predictions across temperatures for multiple models and merge.

    Args:
        predictions: Dictionary mapping temperatures to prediction DataFrames.
        config: Configuration dictionary.
        model_names: List of model names to include predictions for.

    Returns:
        DataFrame with actual values and predictions/errors/uncertainty
        for all specified models across all temperatures.
    """
    if not predictions:
        raise ValueError("No predictions provided for comparison")

    merged_dfs = []
    temperatures = list(predictions.keys())
    logger.info(f"Preparing data for models {model_names} across {len(temperatures)} temps...")

    # Base features common across all residues (take from first available temp df)
    base_feature_cols = ['domain_id', 'resid', 'resname']
    potential_common_features = ['secondary_structure_encoded', 'core_exterior_encoded', 'relative_accessibility', 'normalized_resid']
    first_valid_df = next((df for df in predictions.values() if df is not None), None)
    if first_valid_df is not None:
         for feat in potential_common_features:
              if feat in first_valid_df.columns and feat not in base_feature_cols:
                   base_feature_cols.append(feat)

    for temp in progress_bar(temperatures, desc="Preparing temperature subsets"):
        df = predictions.get(temp) # Use .get for safety
        if df is None or df.empty:
            logger.warning(f"No data found for temperature {temp}. Skipping.")
            continue
        if not all(col in df.columns for col in ['domain_id', 'resid', 'resname']):
             logger.warning(f"Skipping temperature {temp}: Missing essential ID columns.")
             continue

        # Start subset with base features + IDs
        subset_cols = [col for col in base_feature_cols if col in df.columns]
        subset = df[subset_cols].copy()

        # Add actual RMSF for this temp
        target_col = f"rmsf_{temp}" if temp != "average" else "rmsf_average"
        merged_actual_col = f"actual_{temp}"
        if target_col in df.columns:
            subset[merged_actual_col] = df[target_col]
        else:
            logger.warning(f"Actual RMSF column '{target_col}' not found for temperature {temp}")
            subset[merged_actual_col] = np.nan # Add NaN column to ensure consistency

        # Add predictions, errors, uncertainty for EACH requested model
        for model_name in model_names:
            source_pred_col = f"{model_name}_predicted"
            source_error_col = f"{model_name}_error"
            source_abs_error_col = f"{model_name}_abs_error"
            source_uncertainty_col = f"{model_name}_uncertainty"

            # Define merged column names
            merged_pred_col = f"{model_name}_pred_{temp}"
            merged_error_col = f"{model_name}_error_{temp}"
            merged_abs_error_col = f"{model_name}_abs_error_{temp}"
            merged_uncertainty_col = f"{model_name}_unc_{temp}"

            # Add prediction
            if source_pred_col in df.columns:
                subset[merged_pred_col] = df[source_pred_col]
            else:
                # logger.warning(f"'{source_pred_col}' not found for temp {temp}")
                subset[merged_pred_col] = np.nan

            # Add pre-calculated errors if available, otherwise calculate
            if source_abs_error_col in df.columns:
                subset[merged_abs_error_col] = df[source_abs_error_col]
            elif source_pred_col in df.columns and target_col in df.columns:
                 subset[merged_abs_error_col] = (df[source_pred_col] - df[target_col]).abs()
            else: subset[merged_abs_error_col] = np.nan

            if source_error_col in df.columns:
                subset[merged_error_col] = df[source_error_col]
            elif source_pred_col in df.columns and target_col in df.columns:
                 subset[merged_error_col] = df[source_pred_col] - df[target_col]
            else: subset[merged_error_col] = np.nan

            # Add uncertainty
            if source_uncertainty_col in df.columns:
                subset[merged_uncertainty_col] = df[source_uncertainty_col]
            # else: logger.debug(f"'{source_uncertainty_col}' not found for temp {temp}")

        merged_dfs.append(subset)

    if not merged_dfs:
        raise ValueError("No valid dataframes available after preparation for merging.")

    logger.info(f"Merging dataframes for {len(merged_dfs)} temperatures...")
    start_merge_time = time.time()

    # Initialize result with the first DataFrame
    result = merged_dfs[0]
    # Get base feature cols again from the first df actually added
    base_feature_cols = [col for col in base_feature_cols if col in result.columns]

    # Iteratively merge the remaining DataFrames
    for df_to_merge in progress_bar(merged_dfs[1:], desc="Merging temperature data"):
        # Identify columns to keep from the right dataframe (only the temp-specific ones)
        temp_specific_cols = [col for col in df_to_merge.columns if col not in base_feature_cols]
        cols_to_merge = ['domain_id', 'resid', 'resname'] + temp_specific_cols

        try:
            # Merge only the keys and temperature-specific data
            result = pd.merge(
                result, df_to_merge[cols_to_merge],
                on=['domain_id', 'resid', 'resname'],
                how='outer',
                suffixes=('', '_drop_right')
            )
            drop_cols = [col for col in result.columns if col.endswith('_drop_right')]
            if drop_cols: result = result.drop(columns=drop_cols)
        except Exception as e: logger.error(f"Merge step failed: {e}"); raise

    merge_time = time.time() - start_merge_time
    logger.info(f"Finished merging data in {merge_time:.2f}s. Final shape: {result.shape}")

    # --- Add Model Difference Columns ---
    logger.info("Calculating model prediction/error differences...")
    if all(model in model_names for model in MODELS_TO_COMPARE): # Check if both models are present
        m1, m2 = MODELS_TO_COMPARE[0], MODELS_TO_COMPARE[1]
        for temp in temperatures:
            pred1_col = f"{m1}_pred_{temp}"
            pred2_col = f"{m2}_pred_{temp}"
            abs_err1_col = f"{m1}_abs_error_{temp}"
            abs_err2_col = f"{m2}_abs_error_{temp}"

            # Calculate prediction difference if both columns exist
            if pred1_col in result.columns and pred2_col in result.columns:
                result[f"pred_diff_{temp}"] = result[pred1_col] - result[pred2_col]

            # Calculate absolute error difference if both columns exist
            if abs_err1_col in result.columns and abs_err2_col in result.columns:
                result[f"abs_error_diff_{temp}"] = result[abs_err1_col] - result[abs_err2_col]
    else:
        logger.warning(f"Cannot calculate model differences. Required models {MODELS_TO_COMPARE} not found in input model list: {model_names}")


    return result

# ============================================================================
# FUNCTION 2: calculate_temperature_correlations (Unchanged from last version)
# ============================================================================
def calculate_temperature_correlations(
    combined_df: pd.DataFrame,
    temperatures: List[Union[int, str]],
    use_actual: bool = True
) -> pd.DataFrame:
    """
    Calculate correlations between RMSF values at different temperatures.
    (Function content remains the same as the last provided correct version)
    """
    prefix = "actual_" if use_actual else "predicted_"
    valid_temps = [t for t in temperatures if f"{prefix}{t}" in combined_df.columns]
    n_temps = len(valid_temps)
    if n_temps < 2: logger.warning(f"Cannot calculate {prefix} correlations, < 2 valid temp columns."); return pd.DataFrame(index=[str(t) for t in temperatures], columns=[str(t) for t in temperatures])
    corr_matrix = np.full((n_temps, n_temps), np.nan); temp_map = {t: i for i, t in enumerate(valid_temps)}
    for i, temp1 in enumerate(valid_temps):
        col1 = f"{prefix}{temp1}"
        for j, temp2 in enumerate(valid_temps):
            col2 = f"{prefix}{temp2}";
            if i > j: continue
            valid_mask = combined_df[col1].notna() & combined_df[col2].notna()
            if valid_mask.sum() > 1:
                try:
                    x_vals, y_vals = combined_df.loc[valid_mask, col1], combined_df.loc[valid_mask, col2]
                    if x_vals.nunique() > 1 and y_vals.nunique() > 1:
                         pearson_r, _ = stats.pearsonr(x_vals, y_vals); corr_matrix[i, j] = pearson_r;
                         if i != j: corr_matrix[j, i] = pearson_r
                    elif i == j: corr_matrix[i, j] = 1.0
                    else: corr_matrix[i, j] = np.nan; corr_matrix[j, i] = np.nan
                except ValueError: corr_matrix[i, j] = np.nan; corr_matrix[j, i] = np.nan
            elif i == j: corr_matrix[i, j] = 1.0
        corr_matrix[i, i] = 1.0
    corr_df = pd.DataFrame(corr_matrix, index=[str(t) for t in valid_temps], columns=[str(t) for t in valid_temps])
    corr_df = corr_df.reindex(index=[str(t) for t in temperatures], columns=[str(t) for t in temperatures])
    return corr_df

# ============================================================================
# FUNCTION 3: generate_temperature_metrics (MODIFIED for multiple models)
# ============================================================================
def generate_temperature_metrics(
    metrics_input: Dict[Union[int, str], Dict[str, Dict[str, float]]],
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Generate metrics comparing model performance across temperatures.

    Args:
        metrics_input: Nested dict mapping temperature -> model_name -> metrics.
        config: Configuration dictionary

    Returns:
        DataFrame with metrics for each model and temperature
    """
    metrics_list = config.get("temperature", {}).get("comparison", {}).get("metrics", [])
    if not metrics_list:
         logger.warning("No comparison metrics specified in config, using defaults.")
         metrics_list = ['rmse', 'r2', 'pearson_correlation'] # Default
    results = []
    all_models = set() # Keep track of all models found
    for temp, models_metrics in metrics_input.items():
        for model_name, metrics in models_metrics.items():
            all_models.add(model_name)
            row = {'temperature': temp, 'model': model_name}
            for metric_key in metrics_list:
                row[metric_key] = metrics.get(metric_key, np.nan)
            results.append(row)
    if not results:
        logger.warning("No metrics data generated for temperature comparison.")
        return pd.DataFrame(columns=['temperature', 'model'] + metrics_list)
    result_df = pd.DataFrame(results)
    # Pivot for easier comparison (optional, depends on desired format)
    try:
         # Use pivot_table to handle potential missing model/temp combinations gracefully
         pivot_df = pd.pivot_table(result_df, index='temperature', columns='model', values=metrics_list)
         # Flatten MultiIndex columns if desired: pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]
         # Return the pivoted table or the original long format
         # For CSV saving, long format might be more standard. Keep original for now.
         # return pivot_df
         return result_df
    except Exception as e:
         logger.warning(f"Could not pivot metrics table: {e}. Returning long format.")
         return result_df


# ============================================================================
# FUNCTION 4: analyze_temperature_effects (Unchanged from last version)
# ============================================================================
def analyze_temperature_effects(
    combined_df: pd.DataFrame,
    temperatures: List[Union[int, str]],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Analyze how protein flexibility changes with temperature.
    (Function content remains the same as the last provided correct version)
    """
    numeric_temps = [t for t in temperatures if isinstance(t, (int, float)) or (isinstance(t, str) and t.isdigit())]
    if len(numeric_temps) < 2: logger.warning("Not enough numeric temps for trend analysis"); return {}
    numeric_temps = sorted([int(t) if isinstance(t, str) else t for t in numeric_temps]); logger.info(f"Analyzing trends across temps: {numeric_temps}")
    results = {'domain_trends': [], 'residue_outliers': [], 'domain_stats': [], 'aa_responses': [] }; logger.info("Analyzing per-residue temperature trends...")
    start_analysis_time = time.time(); total_residue_groups = len(combined_df[['domain_id', 'resid']].drop_duplicates())
    grouped_residues = progress_bar(combined_df.groupby(['domain_id', 'resid']), desc="Analyzing residue trends", total=total_residue_groups)
    all_slopes_for_stats = []
    for (_, residue_group) in grouped_residues:
        if residue_group.empty: continue
        try: domain_id, resid, resname = residue_group[['domain_id', 'resid', 'resname']].iloc[0]
        except (IndexError, KeyError): logger.warning(f"Could not extract identifiers for a residue group."); continue
        flex_values, temps_used = [], [];
        for temp in numeric_temps:
            actual_col, pred_col = f"actual_{temp}", f"predicted_{temp}" # Note: This uses the *first* model's predicted if actual is missing
            val_to_use = np.nan
            if actual_col in residue_group.columns and pd.notna(residue_group[actual_col].iloc[0]): val_to_use = residue_group[actual_col].iloc[0]
            # elif pred_col in residue_group.columns and pd.notna(residue_group[pred_col].iloc[0]): val_to_use = residue_group[pred_col].iloc[0] # Decide if using predicted for trends is ok
            if pd.notna(val_to_use): flex_values.append(val_to_use); temps_used.append(temp)
        if len(temps_used) < 2: continue
        x, y = np.array(temps_used), np.array(flex_values)
        if np.all(y == y[0]) or np.all(x == x[0]): continue
        try:
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
            if pd.isna(slope) or pd.isna(r_value): continue
            r_squared = r_value**2
        except ValueError: continue
        results['domain_trends'].append({'domain_id': domain_id, 'resid': resid, 'resname': resname, 'slope': slope, 'intercept': intercept, 'r_squared': r_squared, 'p_value': p_value})
        all_slopes_for_stats.append(slope)
    analysis_time = time.time() - start_analysis_time; logger.info(f"Finished residue trend analysis in {analysis_time:.2f} seconds.")
    if all_slopes_for_stats:
         mean_slope, std_slope = np.nanmean(all_slopes_for_stats), np.nanstd(all_slopes_for_stats); upper_slope_threshold, lower_slope_threshold = mean_slope + 2 * std_slope, 0
         for trend in results['domain_trends']:
              slope = trend['slope']
              if slope < lower_slope_threshold or slope > upper_slope_threshold: results['residue_outliers'].append({'domain_id': trend['domain_id'], 'resid': trend['resid'], 'resname': trend['resname'], 'slope': slope, 'r_squared': trend['r_squared'], 'behavior': 'negative_trend' if slope < 0 else 'high_increase'})
    logger.info("Calculating domain-level trend statistics..."); unique_domains_in_trends = list(set(trend['domain_id'] for trend in results['domain_trends']))
    for domain in progress_bar(unique_domains_in_trends, desc="Analyzing domain trends"):
        domain_trends = [r for r in results['domain_trends'] if r['domain_id'] == domain]; slopes = [r['slope'] for r in domain_trends if pd.notna(r['slope'])]; r_squared = [r['r_squared'] for r in domain_trends if pd.notna(r['r_squared'])]
        if not slopes: continue
        results['domain_stats'].append({'domain_id': domain, 'avg_slope': np.nanmean(slopes), 'std_slope': np.nanstd(slopes), 'avg_r_squared': np.nanmean(r_squared), 'num_residues_analyzed': len(domain_trends), 'outliers_count': len([r for r in results['residue_outliers'] if r['domain_id'] == domain])})
    logger.info("Calculating amino acid-specific trend statistics..."); unique_resnames_in_trends = list(set(trend['resname'] for trend in results['domain_trends']))
    for resname in progress_bar(unique_resnames_in_trends, desc="Analyzing AA trends"):
        resname_trends = [r for r in results['domain_trends'] if r['resname'] == resname]; slopes = [r['slope'] for r in resname_trends if pd.notna(r['slope'])]; r_squared = [r['r_squared'] for r in resname_trends if pd.notna(r['r_squared'])]
        if not slopes: continue
        results['aa_responses'].append({'resname': resname, 'avg_slope': np.nanmean(slopes), 'std_slope': np.nanstd(slopes), 'avg_r_squared': np.nanmean(r_squared), 'num_residues_analyzed': len(resname_trends)})
    logger.info("Finished analyzing temperature effects."); return results

# ============================================================================
# FUNCTION 5: calculate_grouped_error_summary_by_temp (MODIFIED for multiple models)
# ============================================================================
def calculate_grouped_error_summary_by_temp(
    combined_df: pd.DataFrame,
    grouping_col: str,
    temperatures: List[Union[int, str]],
    model_names: List[str], # Now takes list of models
    group_labels: Optional[Dict[Any, str]] = None
) -> pd.DataFrame:
    """
    Calculates error summaries grouped by a specific column, for each temperature, FOR MULTIPLE MODELS.

    Args:
        combined_df: Merged DataFrame with actual and predicted values for multiple models.
        grouping_col: Column name to group by.
        temperatures: List of relevant temperatures.
        model_names: List of model names to include summaries for.
        group_labels: Optional mapping for group labels.

    Returns:
        DataFrame with grouped error summaries per temperature per model.
    """
    if grouping_col not in combined_df.columns:
        logger.warning(f"Grouping column '{grouping_col}' not found. Cannot calculate grouped errors.")
        return pd.DataFrame()

    results = []
    logger.info(f"Calculating error summary grouped by '{grouping_col}' per temperature for models: {model_names}...")

    # Get valid temps common to all requested models (or at least one)
    valid_temps_per_model = {model: [] for model in model_names}
    all_valid_temps = set()
    for temp in temperatures:
        actual_col = f"actual_{temp}"
        if actual_col not in combined_df.columns: continue
        for model in model_names:
            pred_col = f"{model}_pred_{temp}"
            abs_error_col = f"{model}_abs_error_{temp}"
            if pred_col in combined_df.columns and abs_error_col in combined_df.columns:
                 if combined_df[abs_error_col].notna().any():
                      valid_temps_per_model[model].append(temp)
                      all_valid_temps.add(temp)

    if not all_valid_temps:
        logger.warning(f"No valid temperatures with error data found for any model. Skipping grouped summary by {grouping_col}.")
        return pd.DataFrame()

    grouped_data = progress_bar(
        combined_df.groupby(grouping_col, observed=False, dropna=False),
        desc=f"Processing {grouping_col} groups"
    )

    for group_val, group_df in grouped_data:
        group_label = group_labels.get(group_val, str(group_val)) if group_labels else str(group_val)
        count = len(group_df)

        base_info = { 'group_value': group_label, 'count': count }

        # Calculate metrics for each model for each valid temp
        for model in model_names:
            for temp in valid_temps_per_model.get(model, []): # Only loop through valid temps for this model
                abs_error_col = f"{model}_abs_error_{temp}"
                valid_errors = group_df[abs_error_col].dropna()
                if not valid_errors.empty:
                    base_info[f"{model}_mean_abs_error_{temp}"] = valid_errors.mean()
                    base_info[f"{model}_median_abs_error_{temp}"] = valid_errors.median()
                    base_info[f"{model}_std_abs_error_{temp}"] = valid_errors.std()
                else:
                    base_info[f"{model}_mean_abs_error_{temp}"] = np.nan
                    base_info[f"{model}_median_abs_error_{temp}"] = np.nan
                    base_info[f"{model}_std_abs_error_{temp}"] = np.nan

        results.append(base_info)

    if not results: logger.warning(f"No results for grouped error summary by '{grouping_col}'."); return pd.DataFrame()
    summary_df = pd.DataFrame(results).rename(columns={'group_value': grouping_col})
    return summary_df

# ============================================================================
# FUNCTION 6: calculate_feature_binned_errors (MODIFIED for multiple models)
# ============================================================================
def calculate_feature_binned_errors(
    combined_df: pd.DataFrame,
    feature_col: str,
    temperatures: List[Union[int, str]],
    model_names: List[str], # Takes list
    n_bins: int = 10
) -> pd.DataFrame:
    """
    Calculates error summaries binned by a continuous feature, for each temperature, FOR MULTIPLE MODELS.

    Args:
        combined_df: Merged DataFrame.
        feature_col: Continuous feature column name to bin.
        temperatures: List of relevant temperatures.
        model_names: List of model names to include summaries for.
        n_bins: Number of bins.

    Returns:
        DataFrame with binned error summaries per temperature per model.
    """
    if feature_col not in combined_df.columns: logger.warning(f"Feature '{feature_col}' not found."); return pd.DataFrame()
    results = []
    logger.info(f"Calculating error summary binned by '{feature_col}' for models: {model_names}...")

    # Binning
    feature_values = combined_df[feature_col].dropna()
    if feature_values.empty or feature_values.nunique() <= 1: logger.warning(f"Not enough unique values in '{feature_col}' for binning."); return pd.DataFrame()
    bin_col_name = f"{feature_col}_bin_temp"; df_copy = combined_df.copy() # Work on copy for bins
    try:
        bins, bin_edges = pd.cut(feature_values, bins=n_bins, labels=False, retbins=True, include_lowest=True, duplicates='drop')
        df_copy[bin_col_name] = bins.reindex(df_copy.index)
    except Exception as e: logger.error(f"Failed to create bins for '{feature_col}': {e}"); return pd.DataFrame()

    # Valid temps common to all models
    valid_temps_per_model = {model: [] for model in model_names}; all_valid_temps = set()
    for temp in temperatures:
        actual_col = f"actual_{temp}"
        if actual_col not in df_copy.columns: continue
        for model in model_names:
             pred_col = f"{model}_pred_{temp}"
             abs_error_col = f"{model}_abs_error_{temp}" # Need abs error column
             if abs_error_col not in df_copy.columns: # Calculate if missing
                  if pred_col in df_copy.columns: df_copy[abs_error_col] = (df_copy[pred_col] - df_copy[actual_col]).abs()
                  else: df_copy[abs_error_col] = np.nan # Cannot calculate
             if abs_error_col in df_copy.columns and df_copy[abs_error_col].notna().any():
                  valid_temps_per_model[model].append(temp); all_valid_temps.add(temp)
    if not all_valid_temps: logger.warning(f"No valid temps for '{feature_col}' bin summary."); return pd.DataFrame()

    grouped_data = progress_bar(df_copy.groupby(bin_col_name, observed=False, dropna=False), desc=f"Processing {feature_col} bins")
    for bin_idx, group_df in grouped_data:
         if pd.isna(bin_idx): continue
         bin_idx_int = int(bin_idx);
         if bin_idx_int < 0 or bin_idx_int >= len(bin_edges) - 1: continue
         bin_start, bin_end = bin_edges[bin_idx_int], bin_edges[bin_idx_int + 1]
         base_info = {'feature': feature_col, 'bin_start': bin_start, 'bin_end': bin_end, 'bin_center': (bin_start + bin_end) / 2, 'count': len(group_df)}
         for model in model_names:
             for temp in valid_temps_per_model.get(model, []):
                 abs_error_col = f"{model}_abs_error_{temp}"
                 valid_errors = group_df[abs_error_col].dropna()
                 base_info[f"{model}_mean_abs_error_{temp}"] = valid_errors.mean() if not valid_errors.empty else np.nan
         results.append(base_info)

    if not results: logger.warning(f"No results for binned error summary by '{feature_col}'."); return pd.DataFrame()
    summary_df = pd.DataFrame(results)
    return summary_df

# ============================================================================
# FUNCTION 7: calculate_domain_performance_by_temp (MODIFIED for multiple models)
# ============================================================================
def calculate_domain_performance_by_temp(
    combined_df: pd.DataFrame,
    temperatures: List[Union[int, str]],
    model_names: List[str] # Takes list
) -> pd.DataFrame:
    """
    Calculates performance metrics (RMSE, R2, Pearson) per domain, per temperature, FOR MULTIPLE MODELS.

    Args:
        combined_df: Merged DataFrame.
        temperatures: List of relevant temperatures.
        model_names: List of model names to include performance for.

    Returns:
        DataFrame with domain performance metrics per temperature per model.
    """
    results = []
    logger.info(f"Calculating domain performance per temperature for models: {model_names}...")

    # Get valid temps common to all models
    valid_temps_per_model = {model: [] for model in model_names}; all_valid_temps = set()
    for temp in temperatures:
        actual_col = f"actual_{temp}"
        if actual_col not in combined_df.columns: continue
        for model in model_names:
             pred_col = f"{model}_pred_{temp}"
             if pred_col in combined_df.columns and combined_df[pred_col].notna().any():
                  valid_temps_per_model[model].append(temp); all_valid_temps.add(temp)
    if not all_valid_temps: logger.warning("No valid temps for domain performance."); return pd.DataFrame()

    grouped_data = progress_bar(combined_df.groupby('domain_id', observed=False), desc="Processing domains")
    for domain_id, domain_df in grouped_data:
        base_info = {'domain_id': domain_id}
        for temp in sorted(list(all_valid_temps)): # Iterate through all temps that had data for at least one model
            actual_col = f"actual_{temp}"
            # Calculate base metrics for this temp/domain if actual exists
            actual_vals_temp_domain = domain_df[actual_col].dropna()
            base_info[f'num_residues_{temp}'] = len(actual_vals_temp_domain)
            base_info[f'mean_actual_{temp}'] = actual_vals_temp_domain.mean() if not actual_vals_temp_domain.empty else np.nan
            base_info[f'std_actual_{temp}'] = actual_vals_temp_domain.std() if len(actual_vals_temp_domain) > 1 else 0.0

            # Calculate metrics for each model *if* it has valid predictions for this temp
            for model in model_names:
                 if temp in valid_temps_per_model.get(model,[]):
                      pred_col = f"{model}_pred_{temp}"
                      temp_model_df = domain_df[[actual_col, pred_col]].dropna()
                      y_true = temp_model_df[actual_col].values
                      y_pred = temp_model_df[pred_col].values
                      num_eval = len(y_true)

                      rmse, r2, pearson_r = np.nan, np.nan, np.nan
                      if num_eval > 1:
                           try:
                               rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                               if np.var(y_true) > 1e-9 and np.var(y_pred) > 1e-9:
                                    r2 = r2_score(y_true, y_pred)
                                    pearson_r, _ = stats.pearsonr(y_true, y_pred)
                                    if pd.isna(pearson_r): pearson_r = 0.0
                               else:
                                    r2 = 0.0 if np.allclose(y_true, y_pred) else np.nan
                                    pearson_r = 1.0 if np.allclose(y_true, y_pred) else np.nan
                           except ValueError: pass
                      elif num_eval == 1: rmse = np.abs(y_true[0] - y_pred[0])

                      base_info[f"{model}_rmse_{temp}"] = rmse
                      base_info[f"{model}_r2_{temp}"] = r2
                      base_info[f"{model}_pearson_{temp}"] = pearson_r
                 else: # Fill with NaN if model didn't have valid data for this temp
                      base_info[f"{model}_rmse_{temp}"] = np.nan
                      base_info[f"{model}_r2_{temp}"] = np.nan
                      base_info[f"{model}_pearson_{temp}"] = np.nan

        results.append(base_info)

    if not results: logger.warning("No domain performance results generated."); return pd.DataFrame()
    summary_df = pd.DataFrame(results)
    return summary_df


# ============================================================================
# MAIN FUNCTION: prepare_temperature_comparison_data (ENHANCED for new CSVs)
# ============================================================================
def prepare_temperature_comparison_data(
    config: Dict[str, Any],
    model_name: str, # NOTE: This argument is now less relevant, comparison uses MODELS_TO_COMPARE
    output_dir: str
) -> Dict[str, Optional[pd.DataFrame]]:
    """
    Prepare data for temperature comparison visualization and analysis.
    Generates multiple CSV files containing processed data suitable for figures.

    Args:
        config: Configuration dictionary.
        model_name: Primary model name (used for fallback/default naming if needed).
        output_dir: Output directory to save comparison data.

    Returns:
        Dictionary mapping data_name -> DataFrame or None if generation failed.
    """
    # Use the hardcoded list of models to compare for this enhanced version
    models_to_analyze = MODELS_TO_COMPARE
    primary_model = model_name if model_name in models_to_analyze else models_to_analyze[0]
    logger.info(f"Starting temperature comparison for models: {models_to_analyze} (Primary: {primary_model})")

    start_prep_time = time.time()
    os.makedirs(output_dir, exist_ok=True)

    # --- Configurable Parameters ---
    analysis_config = config.get("analysis", {}).get("temperature_comparison", {})
    # Define features for automatic binning (can be customized in config)
    default_features_to_bin = ['relative_accessibility', 'normalized_resid']
    features_to_bin = analysis_config.get("binned_features", default_features_to_bin)
    n_bins_for_features = analysis_config.get("n_bins", 10)

    # --- Load results for each temperature ---
    temperatures = config.get("temperature", {}).get("available", [])
    if not temperatures: logger.error("No available temperatures defined in config."); return {}
    predictions = {}; model_metrics_by_temp_model = {}
    logger.info("Loading results from individual temperature runs...")
    base_output_dir_from_config = config.get("paths", {}).get("output_dir", "./output")

    # Load data and metrics for ALL specified models
    for temp in progress_bar(temperatures, desc="Loading temperature results"):
        temp_output_dir = os.path.join(base_output_dir_from_config, f"outputs_{temp}")
        results_path = os.path.join(temp_output_dir, "all_results.csv")
        if not os.path.exists(results_path): logger.warning(f"Results file not found: {results_path}. Skipping."); continue
        try:
            df_temp = pd.read_csv(results_path)
            # Basic check if essential columns are there
            if not all(c in df_temp for c in ['domain_id', 'resid', 'resname']): continue
            predictions[temp] = df_temp
        except Exception as e: logger.error(f"Failed to load {results_path}: {e}"); continue

        metrics_path = os.path.join(temp_output_dir, "evaluation_results.csv")
        if os.path.exists(metrics_path):
             try:
                metrics_df = pd.read_csv(metrics_path, index_col=0)
                temp_metrics = {}
                for model in models_to_analyze: # Load metrics for all relevant models
                    if model in metrics_df.index:
                        temp_metrics[model] = metrics_df.loc[model].to_dict()
                    else: logger.warning(f"Model '{model}' not found in metrics file: {metrics_path}")
                if temp_metrics: model_metrics_by_temp_model[temp] = temp_metrics # Store {temp: {model: {metrics}}}
             except Exception as e: logger.error(f"Failed to load/parse {metrics_path}: {e}")
        else: logger.warning(f"Metrics file not found: {metrics_path}")

    if not predictions: logger.error("No prediction data loaded. Aborting."); return {}
    logger.info(f"Loaded prediction data for {len(predictions)} temperatures.")

    # --- Create combined predictions dataframe (includes all models) ---
    logger.info("Creating combined predictions dataframe...")
    # Pass the list of models we want to include
    combined_preds = compare_temperature_predictions(predictions, config, models_to_analyze)

    # --- Add explicit error columns if not already present (redundancy check) ---
    valid_temps_present = []
    for temp in temperatures:
        actual_col = f"actual_{temp}"
        if actual_col not in combined_preds.columns: continue # Skip if actual is missing
        temp_had_valid_model_data = False
        for model in models_to_analyze:
            pred_col = f"{model}_pred_{temp}"
            error_col = f"{model}_error_{temp}"
            abs_error_col = f"{model}_abs_error_{temp}"
            if pred_col in combined_preds.columns and combined_preds[pred_col].notna().any():
                 temp_had_valid_model_data = True
                 if error_col not in combined_preds.columns: combined_preds[error_col] = combined_preds[pred_col] - combined_preds[actual_col]
                 if abs_error_col not in combined_preds.columns: combined_preds[abs_error_col] = combined_preds[error_col].abs()
        if temp_had_valid_model_data: valid_temps_present.append(temp)
    logger.info(f"Verified/calculated error columns for temperatures: {valid_temps_present}")

    # --- Save enhanced combined predictions (now includes multiple models and differences) ---
    combined_path = os.path.join(output_dir, "combined_predictions_models_errors_diffs.csv") # More descriptive name
    logger.info(f"Saving combined predictions with multi-model errors/diffs to {combined_path}")
    combined_preds.to_csv(combined_path, index=False)
    generated_data = {'combined_multi_model_predictions': combined_preds} # Store in results

    # --- Calculate and save correlations (Actual vs Actual, Pred vs Pred for primary model) ---
    logger.info("Calculating temperature correlations...")
    actual_corr = calculate_temperature_correlations(combined_preds, valid_temps_present, use_actual=True)
    generated_data['actual_correlations'] = actual_corr # Store DataFrame or None
    if actual_corr is not None and not actual_corr.empty:
        actual_corr_path = os.path.join(output_dir, "actual_correlations.csv"); logger.info(f"Saving actual correlations to {actual_corr_path}"); actual_corr.to_csv(actual_corr_path)

    # Calculate predicted correlations based on the *primary* model for consistency
    predicted_corr = calculate_temperature_correlations(combined_preds, valid_temps_present, use_actual=False) # This now uses predicted_<temp> which corresponds to primary model
    generated_data['predicted_correlations'] = predicted_corr
    if predicted_corr is not None and not predicted_corr.empty:
        predicted_corr_path = os.path.join(output_dir, f"predicted_{primary_model}_correlations.csv"); logger.info(f"Saving {primary_model} predicted correlations to {predicted_corr_path}"); predicted_corr.to_csv(predicted_corr_path)

    # --- Generate and save metrics comparison table (includes multiple models) ---
    logger.info("Generating cross-temperature metrics table...")
    metrics_comparison_df = generate_temperature_metrics(model_metrics_by_temp_model, config)
    generated_data['temperature_metrics_multi_model'] = metrics_comparison_df
    if not metrics_comparison_df.empty:
        metrics_path = os.path.join(output_dir, "temperature_metrics_multi_model.csv"); logger.info(f"Saving multi-model temperature metrics table to {metrics_path}"); metrics_comparison_df.to_csv(metrics_path, index=False)

    # --- Analyze and save temperature effects (based on actual/primary model predicted) ---
    logger.info("Analyzing temperature effects...")
    effects = analyze_temperature_effects(combined_preds, valid_temps_present, config)
    logger.info("Saving temperature effects analysis...")
    for key, data in effects.items():
        if isinstance(data, list) and data:
            df_effects = pd.DataFrame(data); df_path = os.path.join(output_dir, f"{key}.csv"); logger.info(f"Saving {key} to {df_path}"); df_effects.to_csv(df_path, index=False)
            generated_data[key] = df_effects
        # else: logger.warning(f"Skipping saving for effect '{key}'.") # Don't warn for empty lists

    # --- Generate and save grouped/binned error summaries (FOR EACH MODEL) ---
    # 1. By Amino Acid
    aa_error_summary = calculate_grouped_error_summary_by_temp(combined_preds, 'resname', valid_temps_present, models_to_analyze)
    generated_data['aa_error_summary_multi_model'] = aa_error_summary
    if aa_error_summary is not None and not aa_error_summary.empty:
        aa_path = os.path.join(output_dir, f"aa_error_summary_multi_model.csv"); logger.info(f"Saving multi-model amino acid error summary to {aa_path}"); aa_error_summary.to_csv(aa_path, index=False)

    # 2. By Secondary Structure
    if 'secondary_structure_encoded' in combined_preds.columns:
        ss_labels = {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other', -1.0: 'Unknown', np.nan: 'Unknown'} # Handle NaN explicitly
        combined_preds['secondary_structure_encoded'].fillna(-1.0, inplace=True) # Fill NaN before grouping
        ss_error_summary = calculate_grouped_error_summary_by_temp(combined_preds, 'secondary_structure_encoded', valid_temps_present, models_to_analyze, group_labels=ss_labels)
        generated_data['ss_error_summary_multi_model'] = ss_error_summary
        if ss_error_summary is not None and not ss_error_summary.empty:
            ss_path = os.path.join(output_dir, f"ss_error_summary_multi_model.csv"); logger.info(f"Saving multi-model secondary structure error summary to {ss_path}"); ss_error_summary.to_csv(ss_path, index=False)
    else: logger.warning("SS column not found, skipping SS error summary."); generated_data['ss_error_summary_multi_model'] = None

    # 3. By Binned Features (Automatic Detection & Per Model)
    generated_data['feature_binned_errors_multi_model'] = {}
    potential_features = [col for col in combined_preds.columns if combined_preds[col].dtype in [np.float64, np.int64] and not any(str(t) in col for t in temperatures) and col not in ['resid', 'secondary_structure_encoded', 'core_exterior_encoded', 'resname_encoded']] # Basic heuristic
    logger.info(f"Attempting automatic feature binning for: {potential_features}")
    actual_features_to_bin = [f for f in features_to_bin if f in potential_features] # Use configured list if valid
    if not actual_features_to_bin: actual_features_to_bin = [f for f in default_features_to_bin if f in potential_features] # Fallback to default if config list invalid
    logger.info(f"Final features selected for binning: {actual_features_to_bin}")

    for feature in actual_features_to_bin:
         binned_errors = calculate_feature_binned_errors(combined_preds.copy(), feature, valid_temps_present, models_to_analyze, n_bins=n_bins_for_features)
         generated_data['feature_binned_errors_multi_model'][feature] = binned_errors
         if binned_errors is not None and not binned_errors.empty:
             bin_path = os.path.join(output_dir, f"error_by_{feature}_bins_multi_model.csv"); logger.info(f"Saving binned error summary for {feature} to {bin_path}"); binned_errors.to_csv(bin_path, index=False)

    # --- Generate and save domain performance by temp (FOR EACH MODEL) ---
    domain_perf = calculate_domain_performance_by_temp(combined_preds, valid_temps_present, models_to_analyze)
    generated_data['domain_performance_multi_model'] = domain_perf
    if domain_perf is not None and not domain_perf.empty:
        domain_perf_path = os.path.join(output_dir, f"domain_performance_multi_model.csv"); logger.info(f"Saving multi-model domain performance summary to {domain_perf_path}"); domain_perf.to_csv(domain_perf_path, index=False)

    # --- Prepare and save histogram data (Still based on primary model for clarity) ---
    histogram_data = []
    logger.info(f"Generating histogram data (using primary model: {primary_model})...")
    all_actual_rmsf, all_predicted_rmsf = [], []
    for temp in valid_temps_present:
        all_actual_rmsf.extend(combined_preds[f"actual_{temp}"].dropna().tolist())
        all_predicted_rmsf.extend(combined_preds[f"{primary_model}_pred_{temp}"].dropna().tolist())
    if all_actual_rmsf or all_predicted_rmsf:
        q_low = np.percentile(all_actual_rmsf + all_predicted_rmsf, 1) if (all_actual_rmsf + all_predicted_rmsf) else 0
        q_high = np.percentile(all_actual_rmsf + all_predicted_rmsf, 99) if (all_actual_rmsf + all_predicted_rmsf) else 1
        min_val, max_val = max(0, q_low), q_high
        if max_val <= min_val: max_val = min_val + 1e-6
        common_bins = np.linspace(min_val, max_val, 21)
    else: common_bins = np.linspace(0, 1, 21)
    for temp in progress_bar(valid_temps_present, desc="Generating histogram data"):
        actual_col, pred_col = f"actual_{temp}", f"{primary_model}_pred_{temp}"
        actual_values, predicted_values = combined_preds[actual_col].dropna(), combined_preds[pred_col].dropna()
        if not actual_values.empty:
            actual_hist, actual_bin_edges = np.histogram(actual_values, bins=common_bins)
            for i in range(len(actual_hist)): histogram_data.append({'temperature': temp, 'type': 'actual', 'bin_start': actual_bin_edges[i], 'bin_end': actual_bin_edges[i+1], 'count': actual_hist[i]})
        if not predicted_values.empty:
            predicted_hist, predicted_bin_edges = np.histogram(predicted_values, bins=common_bins)
            for i in range(len(predicted_hist)): histogram_data.append({'temperature': temp, 'type': f'predicted_{primary_model}', 'bin_start': predicted_bin_edges[i], 'bin_end': predicted_bin_edges[i+1], 'count': predicted_hist[i]})
    generated_data['histogram_data'] = pd.DataFrame(histogram_data) if histogram_data else None
    if generated_data['histogram_data'] is not None:
        histogram_path = os.path.join(output_dir, "histogram_data.csv"); logger.info(f"Saving histogram data to {histogram_path}"); generated_data['histogram_data'].to_csv(histogram_path, index=False)
    else: logger.warning("No histogram data generated.")

    # --- Final Summary ---
    prep_time = time.time() - start_prep_time
    logger.info(f"Temperature comparison preparation finished in {prep_time:.2f} seconds.")
    logger.info(f"Generated comparison files in: {output_dir}")
    return generated_data
### Utility Files ###
---------------------------------------------------------
===== FILE: flexseq/utils/__init__.py =====
"""
Utility modules for the FlexSeq ML pipeline.

This package contains utility functions for metrics, visualization, and
general helpers used throughout the pipeline.
"""

# Import key functions for easier access
from flexseq.utils.metrics import evaluate_predictions, cross_validate_model
from flexseq.utils.helpers import timer, ensure_dir, progress_bar, ProgressCallback
===== FILE: flexseq/utils/helpers.py =====
"""
Helper functions for the FlexSeq ML pipeline.

This module provides utility functions used throughout the pipeline.
"""

import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional, Union, Callable, Iterable, TypeVar
from functools import wraps
from time import time

import numpy as np
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

T = TypeVar('T')

def timer(func):
    """
    Decorator for measuring function execution time.
    
    Args:
        func: Function to wrap
        
    Returns:
        Wrapped function
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time()
        result = func(*args, **kwargs)
        end_time = time()
        logger.info(f"Function '{func.__name__}' executed in {end_time - start_time:.2f} seconds")
        return result
    return wrapper

def ensure_dir(directory: str) -> str:
    """
    Ensure a directory exists, creating it if necessary.
    
    Args:
        directory: Directory path
        
    Returns:
        Directory path
    """
    os.makedirs(directory, exist_ok=True)
    return directory

def estimate_memory_usage(df: pd.DataFrame) -> Tuple[float, str]:
    """
    Estimate memory usage of a DataFrame.
    
    Args:
        df: Input DataFrame
        
    Returns:
        Tuple of (size, unit) where size is a number and unit is a string
    """
    memory_bytes = df.memory_usage(deep=True).sum()
    
    if memory_bytes < 1024:
        return memory_bytes, "bytes"
    elif memory_bytes < 1024**2:
        return memory_bytes / 1024, "KB"
    elif memory_bytes < 1024**3:
        return memory_bytes / (1024**2), "MB"
    else:
        return memory_bytes / (1024**3), "GB"

def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:
    """
    Split a DataFrame into chunks of specified size.
    
    Args:
        df: Input DataFrame
        chunk_size: Number of rows per chunk
        
    Returns:
        List of DataFrame chunks
    """
    return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]

def safe_open(file_path: str, mode: str = 'r'):
    """
    Safely open a file with proper directory creation.
    
    Args:
        file_path: Path to file
        mode: File opening mode
        
    Returns:
        File object
    """
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
    return open(file_path, mode)

def truncate_filename(filename: str, max_length: int = 255) -> str:
    """
    Truncate a filename to ensure it doesn't exceed maximum path length.
    
    Args:
        filename: Original filename
        max_length: Maximum allowed length
        
    Returns:
        Truncated filename
    """
    if len(filename) <= max_length:
        return filename
    
    name, ext = os.path.splitext(filename)
    return name[:max_length - len(ext)] + ext

def safe_parse_float(value: Any) -> Optional[float]:
    """
    Safely parse a value to float, returning None if not possible.
    
    Args:
        value: Value to convert
        
    Returns:
        Float value or None if conversion fails
    """
    if pd.isna(value):
        return None
    
    try:
        return float(value)
    except (ValueError, TypeError):
        return None

def get_amino_acid_properties() -> Dict[str, Dict[str, Any]]:
    """
    Get dictionary of amino acid properties.
    
    Returns:
        Dictionary mapping amino acid codes to property dictionaries
    """
    properties = {
        # Hydrophobic residues
        'ALA': {'hydropathy': 1.8, 'volume': 88.6, 'charge': 0, 'group': 'hydrophobic'},
        'VAL': {'hydropathy': 4.2, 'volume': 140.0, 'charge': 0, 'group': 'hydrophobic'},
        'LEU': {'hydropathy': 3.8, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'ILE': {'hydropathy': 4.5, 'volume': 166.7, 'charge': 0, 'group': 'hydrophobic'},
        'MET': {'hydropathy': 1.9, 'volume': 162.9, 'charge': 0, 'group': 'hydrophobic'},
        'PHE': {'hydropathy': 2.8, 'volume': 189.9, 'charge': 0, 'group': 'hydrophobic'},
        'TRP': {'hydropathy': -0.9, 'volume': 227.8, 'charge': 0, 'group': 'hydrophobic'},
        'PRO': {'hydropathy': -1.6, 'volume': 112.7, 'charge': 0, 'group': 'special'},
        'GLY': {'hydropathy': -0.4, 'volume': 60.1, 'charge': 0, 'group': 'special'},
        
        # Polar residues
        'SER': {'hydropathy': -0.8, 'volume': 89.0, 'charge': 0, 'group': 'polar'},
        'THR': {'hydropathy': -0.7, 'volume': 116.1, 'charge': 0, 'group': 'polar'},
        'CYS': {'hydropathy': 2.5, 'volume': 108.5, 'charge': 0, 'group': 'polar'},
        'TYR': {'hydropathy': -1.3, 'volume': 193.6, 'charge': 0, 'group': 'polar'},
        'ASN': {'hydropathy': -3.5, 'volume': 111.1, 'charge': 0, 'group': 'polar'},
        'GLN': {'hydropathy': -3.5, 'volume': 143.8, 'charge': 0, 'group': 'polar'},
        
        # Charged residues
        'LYS': {'hydropathy': -3.9, 'volume': 168.6, 'charge': 1, 'group': 'positive'},
        'ARG': {'hydropathy': -4.5, 'volume': 173.4, 'charge': 1, 'group': 'positive'},
        'HIS': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'ASP': {'hydropathy': -3.5, 'volume': 111.1, 'charge': -1, 'group': 'negative'},
        'GLU': {'hydropathy': -3.5, 'volume': 138.4, 'charge': -1, 'group': 'negative'},
        
        # Non-standard
        'HSE': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSD': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 0.5, 'group': 'positive'},
        'HSP': {'hydropathy': -3.2, 'volume': 153.2, 'charge': 1, 'group': 'positive'},
        'UNK': {'hydropathy': 0.0, 'volume': 0.0, 'charge': 0, 'group': 'unknown'}
    }
    
    return properties

def is_glycine_or_proline(residue: str) -> bool:
    """
    Check if residue is Glycine or Proline, which significantly affect flexibility.
    
    Args:
        residue: Residue name
        
    Returns:
        True if residue is GLY or PRO
    """
    return residue in ["GLY", "PRO"]

def analyze_hydrogen_bonds(
    secondary_structure: str, 
    residue_name: str
) -> float:
    """
    Analyze potential hydrogen bonding based on secondary structure and residue type.
    Returns a relative measure of hydrogen bond stabilization (higher means more stable).
    
    Args:
        secondary_structure: DSSP code
        residue_name: Residue name
        
    Returns:
        Relative stability score (0-1)
    """
    # Secondary structure types have different H-bond patterns
    ss_stability = {
        'H': 1.0,  # Alpha helix - most stable H-bond pattern
        'G': 0.8,  # 3-10 helix
        'I': 0.9,  # Pi helix
        'E': 0.9,  # Beta sheet - very stable
        'B': 0.7,  # Beta bridge
        'T': 0.4,  # Turn
        'S': 0.3,  # Bend
        'C': 0.1   # Coil - least stable
    }
    
    # Some residues have different H-bond propensities
    residue_factors = {
        'SER': 1.2,  # Can form side-chain H-bonds
        'THR': 1.2,  # Can form side-chain H-bonds
        'ASN': 1.2,  # Can form side-chain H-bonds
        'GLN': 1.2,  # Can form side-chain H-bonds
        'TYR': 1.1,  # Can form side-chain H-bonds
        'PRO': 0.7,  # Disrupts H-bond patterns
        'GLY': 0.9   # More flexible backbone
    }
    
    # Get base stability from secondary structure
    stability = ss_stability.get(secondary_structure, 0.1)
    
    # Apply residue-specific factor
    factor = residue_factors.get(residue_name, 1.0)
    
    return min(1.0, stability * factor)

def calculate_sequence_complexity(sequence: List[str], window_size: int = 5) -> List[float]:
    """
    Calculate sequence complexity using Shannon entropy in a sliding window.
    Higher values indicate more diverse/complex sequence regions.
    
    Args:
        sequence: List of amino acid types
        window_size: Size of sliding window
        
    Returns:
        List of complexity values for each position
    """
    # Convert to single-letter amino acid codes if needed
    if all(len(aa) == 3 for aa in sequence if aa):
        three_to_one = {
            'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',
            'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',
            'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',
            'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V',
            'HSD': 'H', 'HSE': 'H', 'HSP': 'H', 'UNK': 'X'
        }
        sequence = [three_to_one.get(aa, 'X') for aa in sequence]
    
    # Compute complexity for each position
    complexity = []
    half_window = window_size // 2
    padded_seq = ['X'] * half_window + list(sequence) + ['X'] * half_window
    
    for i in range(half_window, len(padded_seq) - half_window):
        window = padded_seq[i - half_window:i + half_window + 1]
        
        # Calculate Shannon entropy
        aa_counts = {}
        for aa in window:
            if aa in aa_counts:
                aa_counts[aa] += 1
            else:
                aa_counts[aa] = 1
        
        entropy = 0
        for count in aa_counts.values():
            p = count / window_size
            entropy -= p * np.log2(p)
        
        # Normalize by maximum possible entropy (all different amino acids)
        max_entropy = np.log2(min(20, window_size))
        if max_entropy > 0:
            normalized_entropy = entropy / max_entropy
        else:
            normalized_entropy = 0
        
        complexity.append(normalized_entropy)
    
    return complexity

def progress_bar(
    iterable: Iterable[T],
    desc: str = None,
    total: Optional[int] = None,
    disable: bool = False,
    leave: bool = True,
    **kwargs
) -> Iterable[T]:
    """
    Create a progress bar for an iterable.
    
    Args:
        iterable: Iterable to track progress of
        desc: Description for the progress bar
        total: Total number of items (inferred if not provided)
        disable: Whether to disable the progress bar
        leave: Whether to leave the progress bar after completion
        **kwargs: Additional arguments to pass to tqdm
        
    Returns:
        Wrapped iterable with progress tracking
    """
    # Disable progress bar if verbose logging is not enabled
    log_level = logging.getLogger().getEffectiveLevel()
    if log_level > logging.INFO:
        disable = True
        
    return tqdm(
        iterable,
        desc=desc,
        total=total,
        disable=disable,
        leave=leave,
        ncols=100,  # Fixed width
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
        **kwargs
    )

class ProgressCallback:
    """
    Callback class to track progress of operations that don't use iterables.
    """
    
    def __init__(
        self, 
        total: int, 
        desc: str = None,
        disable: bool = False,
        leave: bool = True,
        **kwargs
    ):
        """
        Initialize progress callback.
        
        Args:
            total: Total number of steps
            desc: Description for the progress bar
            disable: Whether to disable the progress bar
            leave: Whether to leave the progress bar after completion
            **kwargs: Additional arguments to pass to tqdm
        """
        # Disable progress bar if verbose logging is not enabled
        log_level = logging.getLogger().getEffectiveLevel()
        if log_level > logging.INFO:
            disable = True
            
        self.pbar = tqdm(
            total=total,
            desc=desc,
            disable=disable,
            leave=leave,
            ncols=100,  # Fixed width
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            **kwargs
        )
    
    def update(self, n: int = 1):
        """Update the progress bar by n steps."""
        self.pbar.update(n)
    
    def set_description(self, desc: str):
        """Set the description of the progress bar."""
        self.pbar.set_description(desc)
    
    def set_postfix(self, **kwargs):
        """Set the postfix of the progress bar."""
        self.pbar.set_postfix(**kwargs)
    
    def close(self):
        """Close the progress bar."""
        self.pbar.close()
    
    def __enter__(self):
        """Context manager entry."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()

def format_time(seconds: float) -> str:
    """
    Format time in seconds to a human-readable string.
    
    Args:
        seconds: Time in seconds
        
    Returns:
        Formatted time string
    """
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} hours"

def get_temperature_color(temperature: Union[int, str]) -> str:
    """
    Get a color code for a temperature value.
    Colors range from blue (cold) to red (hot).
    
    Args:
        temperature: Temperature value
        
    Returns:
        Hex color code
    """
    # Handle special case for "average"
    if temperature == "average" or not isinstance(temperature, (int, float)):
        return "#7F7F7F"  # Gray
    
    # Temperature ranges for FlexSeq
    min_temp = 320
    max_temp = 450
    
    # Clamp temperature to range
    temp = max(min_temp, min(temperature, max_temp))
    
    # Normalize to 0-1 range
    normalized = (temp - min_temp) / (max_temp - min_temp)
    
    # Generate color (blue to red)
    r = int(255 * normalized)
    b = int(255 * (1 - normalized))
    g = int(100 * (1 - abs(2 * normalized - 1)))
    
    return f"#{r:02x}{g:02x}{b:02x}"

def make_model_color_map(model_names: List[str]) -> Dict[str, str]:
    """
    Create a consistent color mapping for models.
    
    Args:
        model_names: List of model names
        
    Returns:
        Dictionary mapping model names to color codes
    """
    # Standard colors for models
    standard_colors = [
        "#1f77b4",  # Blue
        "#ff7f0e",  # Orange
        "#2ca02c",  # Green
        "#d62728",  # Red
        "#9467bd",  # Purple
        "#8c564b",  # Brown
        "#e377c2",  # Pink
        "#7f7f7f",  # Gray
        "#bcbd22",  # Olive
        "#17becf"   # Cyan
    ]
    
    # Create mapping
    color_map = {}
    for i, model in enumerate(model_names):
        color_map[model] = standard_colors[i % len(standard_colors)]
    
    return color_map
===== FILE: flexseq/utils/metrics.py =====
"""
Evaluation metrics for the FlexSeq ML pipeline.

This module provides functions for evaluating model performance
and cross-validation.
"""

import logging
from typing import Dict, List, Any, Union, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    explained_variance_score,
    max_error,
    median_absolute_error
)
from sklearn.model_selection import KFold, cross_val_score
from scipy.stats import pearsonr, spearmanr

logger = logging.getLogger(__name__)

def evaluate_predictions(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    config: Dict[str, Any],
    X: Optional[np.ndarray] = None,
    n_features: Optional[int] = None
) -> Dict[str, float]:
    """
    Evaluate predictions using multiple metrics.
    
    Args:
        y_true: True target values
        y_pred: Predicted values
        config: Configuration dictionary with metrics settings
        X: Optional feature matrix for advanced metrics
        n_features: Optional number of features for adjusted R2
        
    Returns:
        Dictionary of metric names and values
    """
    results = {}
    metrics_config = config["evaluation"]["metrics"]
    
    # Root Mean Squared Error
    if metrics_config.get("rmse", True):
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        results["rmse"] = rmse
    
    # Mean Absolute Error
    if metrics_config.get("mae", True):
        mae = mean_absolute_error(y_true, y_pred)
        results["mae"] = mae
    
    # R-squared
    if metrics_config.get("r2", True):
        r2 = r2_score(y_true, y_pred)
        results["r2"] = r2
    
    # Pearson Correlation
    if metrics_config.get("pearson_correlation", True):
        pearson_corr, p_value = pearsonr(y_true, y_pred)
        results["pearson_correlation"] = pearson_corr
        results["pearson_p_value"] = p_value
    
    # Spearman Correlation
    if metrics_config.get("spearman_correlation", True):
        spearman_corr, p_value = spearmanr(y_true, y_pred)
        results["spearman_correlation"] = spearman_corr
        results["spearman_p_value"] = p_value
    
    # Explained Variance Score
    if metrics_config.get("explained_variance", False):
        ev = explained_variance_score(y_true, y_pred)
        results["explained_variance"] = ev
    
    # Max Error
    if metrics_config.get("max_error", False):
        me = max_error(y_true, y_pred)
        results["max_error"] = me
    
    # Median Absolute Error
    if metrics_config.get("median_absolute_error", False):
        medae = median_absolute_error(y_true, y_pred)
        results["median_absolute_error"] = medae
    
    # Adjusted R2
    if metrics_config.get("adjusted_r2", False) and n_features is not None:
        n = len(y_true)
        r2 = results.get("r2", r2_score(y_true, y_pred))
        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - n_features - 1)
        results["adjusted_r2"] = adj_r2
    
    # Root Mean Square Absolute Error
    if metrics_config.get("root_mean_square_absolute_error", False):
        rmsae = np.sqrt(np.mean(np.abs(y_true - y_pred)**2))
        results["root_mean_square_absolute_error"] = rmsae
    
    # Q2 (Cross-validated R2)
    if metrics_config.get("q2", False) and X is not None:
        from sklearn.linear_model import LinearRegression
        cv_r2 = cross_val_score(
            LinearRegression(), X, y_true, 
            cv=5, scoring='r2'
        ).mean()
        results["q2"] = cv_r2
    
    # Temperature-specific metrics (for OmniFlex mode)
    if config["mode"]["active"] == "omniflex":
        # Calculate additional metrics for temperature analysis
        temp = config["temperature"]["current"]
        if str(temp).isdigit():
            # Temperature coefficient (how well the model captures temperature effects)
            # This is a placeholder - in a real implementation, this would use
            # actual temperature coefficients from molecular dynamics
            temp_coef = np.corrcoef(y_pred, np.array(y_true) * int(temp)/400)[0, 1]
            results["temperature_coefficient"] = temp_coef
    
    return results

def cross_validate_model(
    model_class: Any,
    model_params: Dict[str, Any],
    data: pd.DataFrame,
    config: Dict[str, Any],
    n_folds: int = 5,
    return_predictions: bool = False
) -> Dict[str, float]:
    """
    Perform cross-validation for a model.
    
    Args:
        model_class: Model class to instantiate
        model_params: Parameters for model initialization
        data: DataFrame with features and target
        config: Configuration dictionary
        n_folds: Number of cross-validation folds
        return_predictions: Whether to return predictions
        
    Returns:
        Dictionary with cross-validation results
    """
    from flexseq.data.processor import prepare_data_for_model
    from flexseq.utils.helpers import progress_bar
    
    # Get target column
    target_col = config["dataset"]["target"]
    
    # Initialize metrics storage
    metrics = {
        "rmse": [],
        "mae": [],
        "r2": [],
        "pearson_correlation": []
    }
    
    # Add storage for predictions if requested
    all_predictions = []
    all_true_values = []
    
    # Create cross-validation folds
    stratify_by_domain = config["dataset"]["split"].get("stratify_by_domain", True)
    random_state = config["system"]["random_state"]
    
    if stratify_by_domain:
        # Domain-aware cross-validation
        unique_domains = data["domain_id"].unique()
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
        
        fold_domains = []
        for train_idx, test_idx in kf.split(unique_domains):
            # Get train and test domains
            train_domains = unique_domains[train_idx]
            test_domains = unique_domains[test_idx]
            fold_domains.append((train_domains, test_domains))
        
        for i, (train_domains, test_domains) in enumerate(
            progress_bar(fold_domains, desc=f"Cross-validation ({n_folds} folds)")
        ):
            # Split data by domains
            train_data = data[data["domain_id"].isin(train_domains)]
            test_data = data[data["domain_id"].isin(test_domains)]
            
            # Prepare data for model
            X_train, y_train, feature_names = prepare_data_for_model(train_data, config)
            X_test, y_test, _ = prepare_data_for_model(test_data, config)
            
            # Create and train model
            model = model_class(**model_params)
            model.fit(X_train, y_train)
            
            # Generate predictions
            y_pred = model.predict(X_test)
            
            # Evaluate predictions with feature count for adjusted R2
            n_features = X_train.shape[1]
            fold_metrics = evaluate_predictions(
                y_test, y_pred, config, X_test, n_features
            )
            
            # Store results
            for metric, value in fold_metrics.items():
                if metric in metrics:
                    metrics[metric].append(value)
                else:
                    metrics[metric] = [value]
            
            # Store predictions if requested
            if return_predictions:
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)
                
            # Get uncertainty if available
            if hasattr(model, 'predict_with_std'):
                try:
                    _, y_std = model.predict_with_std(X_test)
                    if "uncertainty_std" not in metrics:
                        metrics["uncertainty_std"] = []
                    metrics["uncertainty_std"].append(np.mean(y_std))
                except Exception as e:
                    logger.warning(f"Could not calculate uncertainty: {e}")
    else:
        # Regular cross-validation
        X, y, feature_names = prepare_data_for_model(data, config)
        
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
        
        fold_indices = []
        for train_idx, test_idx in kf.split(X):
            fold_indices.append((train_idx, test_idx))
        
        for i, (train_idx, test_idx) in enumerate(
            progress_bar(fold_indices, desc=f"Cross-validation ({n_folds} folds)")
        ):
            # Split data
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            
            # Create and train model
            model = model_class(**model_params)
            model.fit(X_train, y_train)
            
            # Generate predictions
            y_pred = model.predict(X_test)
            
            # Evaluate predictions with feature count for adjusted R2
            n_features = X_train.shape[1]
            fold_metrics = evaluate_predictions(
                y_test, y_pred, config, X_test, n_features
            )
            
            # Store results
            for metric, value in fold_metrics.items():
                if metric in metrics:
                    metrics[metric].append(value)
                else:
                    metrics[metric] = [value]
            
            # Store predictions if requested
            if return_predictions:
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)
                
            # Get uncertainty if available
            if hasattr(model, 'predict_with_std'):
                try:
                    _, y_std = model.predict_with_std(X_test)
                    if "uncertainty_std" not in metrics:
                        metrics["uncertainty_std"] = []
                    metrics["uncertainty_std"].append(np.mean(y_std))
                except Exception as e:
                    logger.warning(f"Could not calculate uncertainty: {e}")
    
    # Calculate statistics
    results = {}
    
    for metric, values in metrics.items():
        if values:
            results[f"mean_{metric}"] = np.mean(values)
            results[f"std_{metric}"] = np.std(values)
    
    # Add predictions if requested
    if return_predictions:
        results["predictions"] = np.array(all_predictions)
        results["true_values"] = np.array(all_true_values)
    
    return results

def calculate_residue_metrics(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str],
    include_uncertainty: bool = False,
    uncertainty_cols: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Calculate residue-level metrics for model predictions.
    
    Args:
        df: DataFrame with true values and predictions
        target_col: Column with true target values
        prediction_cols: Columns with model predictions
        include_uncertainty: Whether to include uncertainty metrics
        uncertainty_cols: Columns with prediction uncertainties
        
    Returns:
        DataFrame with residue-level metrics
    """
    results = []
    
    for (domain_id, resid), residue_df in df.groupby(["domain_id", "resid"]):
        residue_metrics = {
            "domain_id": domain_id,
            "resid": resid,
            "resname": residue_df["resname"].iloc[0] if "resname" in residue_df.columns else None
        }
        
        # Add structural features if available
        for feature in ["secondary_structure_encoded", "core_exterior_encoded"]:
            if feature in residue_df.columns:
                residue_metrics[feature] = residue_df[feature].iloc[0]
        
        # Calculate metrics for each model
        true_value = residue_df[target_col].iloc[0]
        residue_metrics["actual"] = true_value
        
        for pred_col in prediction_cols:
            pred_value = residue_df[pred_col].iloc[0]
            residue_metrics[pred_col] = pred_value
            
            # Calculate error
            error = pred_value - true_value
            abs_error = abs(error)
            
            model_name = pred_col.split("_predicted")[0]
            residue_metrics[f"{model_name}_error"] = error
            residue_metrics[f"{model_name}_abs_error"] = abs_error
            
            # Add uncertainty if available
            if include_uncertainty and uncertainty_cols is not None:
                unc_col = next((col for col in uncertainty_cols if model_name in col), None)
                if unc_col and unc_col in residue_df.columns:
                    unc_value = residue_df[unc_col].iloc[0]
                    residue_metrics[f"{model_name}_uncertainty"] = unc_value
                    
                    # Calculate normalized error (error / uncertainty)
                    if unc_value > 0:
                        normalized_error = abs_error / unc_value
                        residue_metrics[f"{model_name}_normalized_error"] = normalized_error
        
        results.append(residue_metrics)
    
    return pd.DataFrame(results)

def calculate_temperature_scaling_factors(
    df: pd.DataFrame,
    temperatures: List[Union[int, str]]
) -> Dict[str, float]:
    """
    Calculate scaling factors between RMSF values at different temperatures.
    
    Args:
        df: DataFrame with RMSF values at multiple temperatures
        temperatures: List of temperatures to analyze
        
    Returns:
        Dictionary mapping temperature pairs to scaling factors
    """
    # Convert string temperatures to int if numeric
    temp_list = []
    for temp in temperatures:
        if isinstance(temp, str) and temp.isdigit():
            temp_list.append(int(temp))
        elif isinstance(temp, int):
            temp_list.append(temp)
    
    # Sort temperatures
    temp_list.sort()
    
    # Calculate scaling factors
    scaling_factors = {}
    
    for i, temp1 in enumerate(temp_list):
        col1 = f"rmsf_{temp1}"
        if col1 not in df.columns:
            continue
            
        for j, temp2 in enumerate(temp_list[i+1:], i+1):
            col2 = f"rmsf_{temp2}"
            if col2 not in df.columns:
                continue
                
            # Calculate average ratio
            valid_mask = (df[col1] > 0) & (df[col2] > 0)
            ratios = df.loc[valid_mask, col2] / df.loc[valid_mask, col1]
            
            # Store average
            key = f"{temp1}_to_{temp2}"
            scaling_factors[key] = ratios.mean()
    
    return scaling_factors

def calculate_uncertainty_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_std: np.ndarray
) -> Dict[str, float]:
    """
    Calculate metrics for uncertainty quantification.
    
    Args:
        y_true: True target values
        y_pred: Predicted values
        y_std: Standard deviation of predictions (uncertainty)
        
    Returns:
        Dictionary of uncertainty metrics
    """
    # Calculate absolute errors
    errors = np.abs(y_true - y_pred)
    
    # Calculate percentage of true values within confidence intervals
    within_1std = np.mean(errors <= y_std)
    within_2std = np.mean(errors <= 2 * y_std)
    within_3std = np.mean(errors <= 3 * y_std)
    
    # Ideal percentages: 68% within 1 std, 95% within 2 std, 99.7% within 3 std
    # Calculate calibration error (how far from ideal)
    cal_error_1std = np.abs(within_1std - 0.68)
    cal_error_2std = np.abs(within_2std - 0.95)
    cal_error_3std = np.abs(within_3std - 0.997)
    
    # Average calibration error
    avg_cal_error = (cal_error_1std + cal_error_2std + cal_error_3std) / 3
    
    # Calculate negative log predictive density (NLPD)
    nlpd = np.mean(0.5 * np.log(2 * np.pi * y_std**2) + 
                   0.5 * ((y_true - y_pred)**2) / (y_std**2))
    
    # Calculate uncertainty-error correlation
    # Ideally, higher uncertainty should correlate with higher error
    unc_err_corr = np.corrcoef(y_std, errors)[0, 1]
    
    return {
        "within_1std": within_1std,
        "within_2std": within_2std,
        "within_3std": within_3std,
        "calibration_error_1std": cal_error_1std,
        "calibration_error_2std": cal_error_2std,
        "calibration_error_3std": cal_error_3std,
        "avg_calibration_error": avg_cal_error,
        "nlpd": nlpd,
        "uncertainty_error_correlation": unc_err_corr
    }

def calculate_domain_performance(
    df: pd.DataFrame,
    target_col: str,
    prediction_cols: List[str]
) -> pd.DataFrame:
    """
    Calculate performance metrics by domain.
    
    Args:
        df: DataFrame with predictions and actual values
        target_col: Target column name
        prediction_cols: Columns with model predictions
        
    Returns:
        DataFrame with domain performance metrics
    """
    results = []
    
    for domain_id, domain_df in df.groupby("domain_id"):
        row = {"domain_id": domain_id, "num_residues": len(domain_df)}
        
        # Add domain properties if available
        if "core_exterior_encoded" in domain_df.columns:
            row["percent_surface"] = domain_df["core_exterior_encoded"].mean() * 100
        
        if "secondary_structure_encoded" in domain_df.columns:
            # Map values to types
            ss_mapping = {0: "helix", 1: "sheet", 2: "loop"}
            ss_counts = domain_df["secondary_structure_encoded"].value_counts(normalize=True) * 100
            for ss_code, ss_type in ss_mapping.items():
                row[f"percent_{ss_type}"] = ss_counts[ss_code] if ss_code in ss_counts else 0
        
        # Calculate metrics for each model
        for pred_col in prediction_cols:
            model_name = pred_col.split("_predicted")[0]
            
            # Calculate metrics
            y_true = domain_df[target_col].values
            y_pred = domain_df[pred_col].values
            
            row[f"{model_name}_rmse"] = np.sqrt(mean_squared_error(y_true, y_pred))
            row[f"{model_name}_mae"] = mean_absolute_error(y_true, y_pred)
            row[f"{model_name}_r2"] = r2_score(y_true, y_pred)
            
            # Calculate Pearson correlation
            pearson_corr, _ = pearsonr(y_true, y_pred)
            row[f"{model_name}_pearson"] = pearson_corr
        
        results.append(row)
    
    return pd.DataFrame(results)
===== FILE: flexseq/utils/visualization.py =====
"""
Visualization functions for the FlexSeq ML pipeline.

This module provides placeholder functions for generating visualization data
to be used by external visualization tools. The functions primarily save data
in CSV format that can be visualized separately.
"""


from scipy import stats

from flexseq.data.loader import load_temperature_data
# Add these lines:
from flexseq.utils.helpers import progress_bar, ProgressCallback
import time # Optional, but useful for timing sections


import os
import logging
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

from flexseq.utils.helpers import (
    get_temperature_color,
    make_model_color_map,
    ensure_dir, 
    progress_bar, 
    ProgressCallback
)

logger = logging.getLogger(__name__)

def save_plot(plt, output_path: str, dpi: int = 300) -> None:
    """
    Save a matplotlib plot to disk.
    
    Args:
        plt: Matplotlib pyplot instance
        output_path: Path to save the plot
        dpi: Resolution in dots per inch
    """
    # Create directory if it doesn't exist
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    
    # Save the plot
    plt.tight_layout()
    plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
    plt.close()

def plot_temperature_comparison(
    temperatures: List[Union[int, str]],
    metrics: pd.DataFrame,
    output_path: str
) -> None:
    """
    Generate a plot comparing metrics across temperatures.
    
    Args:
        temperatures: List of temperature values
        metrics: DataFrame with metrics for each temperature
        output_path: Path to save the plot
    """
    # Prepare the data for plotting
    metrics_data = []
    for temp in temperatures:
        if str(temp) in metrics.index.astype(str):
            row = metrics.loc[str(temp)]
            metrics_data.append({
                'temperature': temp,
                'rmse': row.get('rmse', np.nan),
                'r2': row.get('r2', np.nan),
                'pearson_correlation': row.get('pearson_correlation', np.nan)
            })
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Save the data as CSV for external visualization
    metrics_df = pd.DataFrame(metrics_data)
    metrics_df.to_csv(output_path, index=False)
    
    logger.info(f"Temperature comparison data saved to {output_path}")

def plot_amino_acid_performance(
    data: pd.DataFrame,
    temperature: Union[int, str],
    output_path: str
) -> None:
    """
    Generate amino acid-specific performance data.
    
    Args:
        data: DataFrame with predictions and errors by amino acid
        temperature: Temperature value for the data
        output_path: Path to save the data
    """
    # Group data by amino acid
    if 'resname' in data.columns:
        error_cols = [col for col in data.columns if col.endswith('_abs_error')]
        aa_performance = []
        
        for resname, group in data.groupby('resname'):
            row = {'resname': resname, 'count': len(group)}
            
            for error_col in error_cols:
                model_name = error_col.split('_abs_error')[0]
                row[f"{model_name}_mean_error"] = group[error_col].mean()
                row[f"{model_name}_median_error"] = group[error_col].median()
                row[f"{model_name}_std_error"] = group[error_col].std()
            
            aa_performance.append(row)
        
        # Create DataFrame and save to CSV
        aa_df = pd.DataFrame(aa_performance)
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        aa_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid performance data saved to {output_path}")

def plot_feature_importance(
    importances: Dict[str, float],
    feature_names: List[str],
    output_path: str
) -> None:
    """
    Generate feature importance visualization data.
    
    Args:
        importances: Dictionary mapping features to importance values
        feature_names: List of feature names
        output_path: Path to save the data
    """
    # Create DataFrame from importance dictionary
    importance_data = []
    
    for feature, importance in importances.items():
        importance_data.append({
            'feature': feature,
            'importance': importance
        })
    
    importance_df = pd.DataFrame(importance_data)
    importance_df = importance_df.sort_values('importance', ascending=False)
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Save to CSV
    importance_df.to_csv(output_path, index=False)
    logger.info(f"Feature importance data saved to {output_path}")
    
    # Create a simple bar plot of the top 15 features
    plt.figure(figsize=(10, 6))
    top_features = importance_df.head(15)
    sns.barplot(x='importance', y='feature', data=top_features)
    plt.title('Top 15 Feature Importances')
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"Feature importance plot saved to {plot_path}")

def plot_model_metrics_table(
    metrics: Dict[str, Dict[str, float]],
    config: Dict[str, Any]
) -> None:
    """
    Generate a table comparing metrics across models.
    
    Args:
        metrics: Dictionary mapping model names to metrics
        config: Configuration dictionary
    """
    # Create DataFrame from metrics dictionary
    metrics_data = []
    
    for model_name, model_metrics in metrics.items():
        row = {'model': model_name}
        row.update(model_metrics)
        metrics_data.append(row)
    
    metrics_df = pd.DataFrame(metrics_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "model_metrics_table.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    metrics_df.to_csv(output_path, index=False)
    logger.info(f"Model metrics table saved to {output_path}")

def plot_r2_comparison(
    predictions: Dict[str, np.ndarray],
    target_values: np.ndarray,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate R² comparison data across models.
    
    Args:
        predictions: Dictionary mapping model names to predictions
        target_values: True target values
        model_names: List of model names
        config: Configuration dictionary
    """
    from sklearn.metrics import r2_score
    
    # Calculate R² for each model
    r2_data = []
    
    for model_name in model_names:
        if model_name in predictions:
            r2 = r2_score(target_values, predictions[model_name])
            r2_data.append({
                'model': model_name,
                'r2': r2
            })
    
    r2_df = pd.DataFrame(r2_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "comparisons", "r2_comparison.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    r2_df.to_csv(output_path, index=False)
    logger.info(f"R² comparison data saved to {output_path}")
    
    # Create a simple bar plot
    plt.figure(figsize=(8, 5))
    sns.barplot(x='model', y='r2', data=r2_df)
    plt.title('R² Comparison Across Models')
    plt.ylim(0, 1)  # R² is typically between 0 and 1
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"R² comparison plot saved to {plot_path}")

def plot_residue_level_rmsf(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate residue-level RMSF comparison data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    # Sample a single domain for visualization
    domains = df['domain_id'].unique()
    if len(domains) > 0:
        # Select the domain with the most residues for better visualization
        domain_counts = df.groupby('domain_id').size()
        selected_domain = domain_counts.idxmax()
        
        domain_df = df[df['domain_id'] == selected_domain].copy()
        
        # Add predictions from each model
        for model_name in model_names:
            if model_name in predictions:
                domain_df[f"{model_name}_predicted"] = np.nan
                
                # Match predictions to domain rows
                for i, idx in enumerate(df.index):
                    if idx in domain_df.index:
                        domain_df.loc[idx, f"{model_name}_predicted"] = predictions[model_name][i]
        
        # Sort by residue ID
        domain_df = domain_df.sort_values('resid')
        
        # Select columns for output
        output_cols = ['domain_id', 'resid', 'resname', target_col]
        output_cols.extend([f"{model_name}_predicted" for model_name in model_names if model_name in predictions])
        
        if 'secondary_structure_encoded' in domain_df.columns:
            output_cols.append('secondary_structure_encoded')
        
        if 'core_exterior_encoded' in domain_df.columns:
            output_cols.append('core_exterior_encoded')
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"domain_{selected_domain}_rmsf.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        domain_df[output_cols].to_csv(output_path, index=False)
        logger.info(f"Residue-level RMSF data saved to {output_path}")
        
        # Create a simple line plot
        plt.figure(figsize=(12, 6))
        
        # Plot actual values
        plt.plot(domain_df['resid'], domain_df[target_col], 'k-', label='Actual', linewidth=2)
        
        # Plot predictions
        colors = make_model_color_map(model_names)
        for model_name in model_names:
            if model_name in predictions and f"{model_name}_predicted" in domain_df.columns:
                plt.plot(domain_df['resid'], domain_df[f"{model_name}_predicted"], 
                         label=model_name, color=colors.get(model_name), linewidth=1.5)
        
        plt.xlabel('Residue ID')
        plt.ylabel('RMSF')
        plt.title(f'RMSF Profile for Domain {selected_domain}')
        plt.legend()
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Residue-level RMSF plot saved to {plot_path}")

def plot_amino_acid_error_analysis(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid error analysis data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns:
        logger.warning("Residue name information not available for amino acid analysis")
        return
    
    # Calculate errors for each model
    df_with_preds = df.copy()
    
    for model_name in model_names:
        if model_name in predictions:
            df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
            df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
            df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
    
    # Group by amino acid
    aa_errors = []
    
    for resname, group in df_with_preds.groupby('resname'):
        row = {'resname': resname, 'count': len(group)}
        
        for model_name in model_names:
            if model_name in predictions:
                error_col = f"{model_name}_abs_error"
                if error_col in group.columns:
                    row[f"{model_name}_mean_error"] = group[error_col].mean()
                    row[f"{model_name}_median_error"] = group[error_col].median()
                    row[f"{model_name}_std_error"] = group[error_col].std()
        
        aa_errors.append(row)
    
    aa_error_df = pd.DataFrame(aa_errors)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(output_dir, "residue_analysis", "amino_acid_errors.csv")
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    aa_error_df.to_csv(output_path, index=False)
    logger.info(f"Amino acid error analysis saved to {output_path}")
    
    # Create a simple bar plot for the model with the most predictions
    if model_names:
        model_name = model_names[0]  # Use first model
        
        if f"{model_name}_mean_error" in aa_error_df.columns:
            plt.figure(figsize=(10, 6))
            
            # Sort by error
            sorted_df = aa_error_df.sort_values(f"{model_name}_mean_error")
            
            # Plot
            sns.barplot(x='resname', y=f"{model_name}_mean_error", data=sorted_df)
            plt.title(f'Mean Absolute Error by Amino Acid Type ({model_name})')
            plt.xlabel('Amino Acid')
            plt.ylabel('Mean Absolute Error')
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + f"_{model_name}.png"
            save_plot(plt, plot_path)
            logger.info(f"Amino acid error plot saved to {plot_path}")

def plot_amino_acid_error_boxplot(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid error boxplot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns or not model_names:
        return
    
    # Calculate errors for the first model
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Create long-form data for boxplot
        error_data = []
        
        for _, row in df_with_preds.iterrows():
            error_data.append({
                'resname': row['resname'],
                'error': row[f"{model_name}_abs_error"]
            })
        
        error_df = pd.DataFrame(error_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"amino_acid_errors_boxplot_{model_name}.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        error_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid error boxplot data saved to {output_path}")
        
        # Create a boxplot
        plt.figure(figsize=(12, 6))
        
        # Get median errors for sorting
        median_errors = error_df.groupby('resname')['error'].median().sort_values()
        sorted_residues = median_errors.index.tolist()
        
        # Create boxplot with sorted residues
        sns.boxplot(x='resname', y='error', data=error_df, order=sorted_residues)
        plt.title(f'Absolute Error Distribution by Amino Acid Type ({model_name})')
        plt.xlabel('Amino Acid')
        plt.ylabel('Absolute Error')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Amino acid error boxplot saved to {plot_path}")

def plot_amino_acid_scatter_plot(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate amino acid scatter plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'resname' not in df.columns or not model_names:
        return
    
    # Use the first model for scatter plot
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        
        # Create scatter plot data
        scatter_data = []
        
        for _, row in df_with_preds.iterrows():
            scatter_data.append({
                'resname': row['resname'],
                'actual': row[target_col],
                'predicted': row[f"{model_name}_predicted"]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(output_dir, "residue_analysis", f"amino_acid_scatter_{model_name}.csv")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Amino acid scatter plot data saved to {output_path}")
        
        # Create a scatter plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Create a colormap for amino acids
        unique_residues = sampled_df['resname'].unique()
        cmap = plt.cm.get_cmap('tab20', len(unique_residues))
        residue_to_color = {res: cmap(i) for i, res in enumerate(unique_residues)}
        colors = [residue_to_color[res] for res in sampled_df['resname']]
        
        # Plot
        plt.scatter(sampled_df['actual'], sampled_df['predicted'], c=colors, alpha=0.7, s=30)
        
        # Add diagonal line
        min_val = min(sampled_df['actual'].min(), sampled_df['predicted'].min())
        max_val = max(sampled_df['actual'].max(), sampled_df['predicted'].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--')
        
        plt.xlabel('Actual RMSF')
        plt.ylabel('Predicted RMSF')
        plt.title(f'Actual vs Predicted RMSF by Amino Acid Type ({model_name})')
        
        # Add legend with the most common amino acids (top 10)
        residue_counts = df['resname'].value_counts().head(10)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                          markerfacecolor=residue_to_color[res], markersize=10, label=res) 
                         for res in residue_counts.index]
        plt.legend(handles=legend_elements, title='Amino Acid')
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Amino acid scatter plot saved to {plot_path}")

def plot_error_analysis_by_property(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate error analysis by property data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names:
        return
    
    # Use the first model for analysis
    model_name = model_names[0]
    
    if model_name in predictions:
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Properties to analyze
        properties = []
        
        if 'secondary_structure_encoded' in df_with_preds.columns:
            properties.append({
                'name': 'secondary_structure',
                'column': 'secondary_structure_encoded',
                'labels': {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other'}
            })
        
        if 'core_exterior_encoded' in df_with_preds.columns:
            properties.append({
                'name': 'surface_exposure',
                'column': 'core_exterior_encoded',
                'labels': {0: 'Core', 1: 'Surface'}
            })
        
        if 'normalized_resid' in df_with_preds.columns:
            # Create bins for normalized position
            df_with_preds['position_bin'] = pd.cut(
                df_with_preds['normalized_resid'], 
                bins=5, 
                labels=['N-term', 'N-quarter', 'Middle', 'C-quarter', 'C-term']
            )
            
            properties.append({
                'name': 'sequence_position',
                'column': 'position_bin',
                'labels': None  # Use the bin labels
            })
        
        # Analyze each property
        for prop in properties:
            property_data = []
            
            if prop['column'] in df_with_preds.columns:
                groupby_col = prop['column']
                
                for group_val, group in df_with_preds.groupby(groupby_col):
                    # Get label
                    if prop['labels'] is not None:
                        label = prop['labels'].get(group_val, str(group_val))
                    else:
                        label = str(group_val)
                    
                    # Calculate metrics
                    row = {
                        'property': prop['name'],
                        'value': label,
                        'count': len(group),
                        'mean_error': group[f"{model_name}_abs_error"].mean(),
                        'median_error': group[f"{model_name}_abs_error"].median(),
                        'std_error': group[f"{model_name}_abs_error"].std()
                    }
                    
                    property_data.append(row)
                
                # Create DataFrame
                prop_df = pd.DataFrame(property_data)
                
                # Save to CSV
                output_dir = config["paths"]["output_dir"]
                output_path = os.path.join(
                    output_dir, 
                    "residue_analysis", 
                    f"error_by_{prop['name']}_{model_name}.csv"
                )
                
                # Create output directory
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                prop_df.to_csv(output_path, index=False)
                logger.info(f"Error analysis by {prop['name']} saved to {output_path}")
                
                # Create a bar plot
                plt.figure(figsize=(8, 5))
                
                # Sort by value if not a categorical property
                if prop['name'] == 'sequence_position':
                    # Use the defined order for position bins
                    order = ['N-term', 'N-quarter', 'Middle', 'C-quarter', 'C-term']
                    sns.barplot(x='value', y='mean_error', data=prop_df, order=order)
                else:
                    sns.barplot(x='value', y='mean_error', data=prop_df)
                
                plt.title(f'Mean Absolute Error by {prop["name"].replace("_", " ").title()} ({model_name})')
                plt.xlabel(prop['name'].replace('_', ' ').title())
                plt.ylabel('Mean Absolute Error')
                plt.tight_layout()
                
                # Save the plot
                plot_path = os.path.splitext(output_path)[0] + '.png'
                save_plot(plt, plot_path)
                logger.info(f"Error analysis plot for {prop['name']} saved to {plot_path}")

def plot_r2_comparison_scatter(
    predictions: Dict[str, np.ndarray],
    target_values: np.ndarray,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate R² comparison scatter plot data.
    
    Args:
        predictions: Dictionary mapping model names to predictions
        target_values: True target values
        model_names: List of model names
        config: Configuration dictionary
    """
    if len(model_names) < 2:
        return
    
    # Get pairs of models
    model_pairs = []
    
    for i, model1 in enumerate(model_names):
        for model2 in model_names[i+1:]:
            if model1 in predictions and model2 in predictions:
                model_pairs.append((model1, model2))
    
    # Create scatter plot data for each pair
    for model1, model2 in model_pairs:
        scatter_data = []
        
        for i in range(len(target_values)):
            scatter_data.append({
                'actual': target_values[i],
                f"{model1}_predicted": predictions[model1][i],
                f"{model2}_predicted": predictions[model2][i]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "comparisons", 
            f"scatter_{model1}_vs_{model2}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Scatter plot data for {model1} vs {model2} saved to {output_path}")
        
        # Create a scatter plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Plot
        plt.scatter(
            sampled_df[f"{model1}_predicted"], 
            sampled_df[f"{model2}_predicted"], 
            c=sampled_df['actual'], 
            cmap='viridis', 
            alpha=0.7, 
            s=30
        )
        
        # Add diagonal line
        min_val = min(sampled_df[f"{model1}_predicted"].min(), sampled_df[f"{model2}_predicted"].min())
        max_val = max(sampled_df[f"{model1}_predicted"].max(), sampled_df[f"{model2}_predicted"].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--')
        
        plt.xlabel(f'{model1} Predicted')
        plt.ylabel(f'{model2} Predicted')
        plt.title(f'Prediction Comparison: {model1} vs {model2}')
        plt.colorbar(label='Actual RMSF')
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Scatter plot for {model1} vs {model2} saved to {plot_path}")

def plot_scatter_with_density_contours(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate scatter plot with density contours.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Create data for plotting
        scatter_data = []
        
        for i, idx in enumerate(df.index):
            scatter_data.append({
                'actual': df.loc[idx, target_col],
                'predicted': predictions[model_name][i]
            })
        
        scatter_df = pd.DataFrame(scatter_data)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "comparisons", 
            f"density_scatter_{model_name}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        scatter_df.to_csv(output_path, index=False)
        logger.info(f"Density scatter plot data for {model_name} saved to {output_path}")
        
        # Create the plot
        plt.figure(figsize=(8, 8))
        
        # Sample up to 5000 points for better visibility
        sample_size = min(5000, len(scatter_df))
        sampled_df = scatter_df.sample(sample_size, random_state=config["system"]["random_state"])
        
        # Create plot with density contours
        sns.kdeplot(
            x='actual',
            y='predicted',
            data=sampled_df,
            fill=True,
            cmap='Blues',
            alpha=0.5,
            levels=10
        )
        
        plt.scatter(
            sampled_df['actual'],
            sampled_df['predicted'],
            alpha=0.3,
            s=20,
            c='darkblue'
        )
        
        # Add diagonal line
        min_val = min(sampled_df['actual'].min(), sampled_df['predicted'].min())
        max_val = max(sampled_df['actual'].max(), sampled_df['predicted'].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--')
        
        plt.xlabel('Actual RMSF')
        plt.ylabel('Predicted RMSF')
        plt.title(f'Actual vs Predicted RMSF with Density Contours ({model_name})')
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Density scatter plot for {model_name} saved to {plot_path}")

def plot_flexibility_vs_dihedral_angles(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate flexibility vs dihedral angles plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'phi_norm' not in df.columns or 'psi_norm' not in df.columns:
        return
    
    # Use first model
    model_name = model_names[0] if model_names else None
    
    # Prepare data for plotting
    plot_data = []
    
    for i, idx in enumerate(df.index):
        row = {
            'phi': df.loc[idx, 'phi_norm'],
            'psi': df.loc[idx, 'psi_norm'],
            'actual': df.loc[idx, target_col]
        }
        
        if model_name and model_name in predictions:
            row['predicted'] = predictions[model_name][i]
        
        plot_data.append(row)
    
    plot_df = pd.DataFrame(plot_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(
        output_dir, 
        "residue_analysis", 
        "flexibility_vs_dihedral_angles.csv"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    plot_df.to_csv(output_path, index=False)
    logger.info(f"Flexibility vs dihedral angles data saved to {output_path}")
    
    # Create the plot
    plt.figure(figsize=(10, 8))
    
    # Sample up to 5000 points for better visibility
    sample_size = min(5000, len(plot_df))
    sampled_df = plot_df.sample(sample_size, random_state=config["system"]["random_state"])
    
    # Create heatmap scatter plot
    plt.scatter(
        sampled_df['phi'],
        sampled_df['psi'],
        c=sampled_df['actual'],
        cmap='viridis',
        alpha=0.7,
        s=30
    )
    
    plt.xlabel('Normalized Phi Angle')
    plt.ylabel('Normalized Psi Angle')
    plt.title('Protein Flexibility in Dihedral Angle Space')
    plt.colorbar(label='RMSF')
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.splitext(output_path)[0] + '.png'
    save_plot(plt, plot_path)
    logger.info(f"Flexibility vs dihedral angles plot saved to {plot_path}")

def plot_flexibility_sequence_neighborhood(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate flexibility vs sequence neighborhood plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    # Check if we have window features
    window_cols = [col for col in df.columns if '_offset_' in col]
    
    if not window_cols:
        return
    
    # Find a specific domain with good data
    domains = df['domain_id'].unique()
    
    if len(domains) > 0:
        # Select a domain with at least 50 residues
        domain_sizes = df.groupby('domain_id').size()
        valid_domains = domain_sizes[domain_sizes >= 50].index
        
        if len(valid_domains) > 0:
            selected_domain = valid_domains[0]
            domain_df = df[df['domain_id'] == selected_domain].copy()
            
            # Sort by residue ID
            domain_df = domain_df.sort_values('resid')
            
            # Select a window around a highly flexible residue
            flexible_idx = domain_df[target_col].idxmax()
            flexible_resid = domain_df.loc[flexible_idx, 'resid']
            
            # Select residues within 10 positions
            window_size = 10
            min_resid = max(0, flexible_resid - window_size)
            max_resid = flexible_resid + window_size
            
            window_df = domain_df[
                (domain_df['resid'] >= min_resid) & 
                (domain_df['resid'] <= max_resid)
            ].copy()
            
            # Add predictions if available
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                window_df[f"{model_name}_predicted"] = np.nan
                
                # Match predictions to domain rows
                for i, idx in enumerate(df.index):
                    if idx in window_df.index:
                        window_df.loc[idx, f"{model_name}_predicted"] = predictions[model_name][i]
            
            # Save to CSV
            output_dir = config["paths"]["output_dir"]
            output_path = os.path.join(
                output_dir, 
                "residue_analysis", 
                f"sequence_neighborhood_domain_{selected_domain}.csv"
            )
            
            # Create output directory
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # Select relevant columns
            relevant_cols = ['domain_id', 'resid', 'resname', target_col, 'secondary_structure_encoded']
            
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                if f"{model_name}_predicted" in window_df.columns:
                    relevant_cols.append(f"{model_name}_predicted")
            
            window_df[relevant_cols].to_csv(output_path, index=False)
            logger.info(f"Sequence neighborhood data saved to {output_path}")
            
            # Create the plot
            plt.figure(figsize=(10, 6))
            
            # Plot actual values
            plt.plot(
                window_df['resid'], 
                window_df[target_col], 
                'k-', 
                linewidth=2, 
                label='Actual'
            )
            
            # Plot predictions if available
            if model_names and model_names[0] in predictions:
                model_name = model_names[0]
                if f"{model_name}_predicted" in window_df.columns:
                    plt.plot(
                        window_df['resid'], 
                        window_df[f"{model_name}_predicted"], 
                        'r--', 
                        linewidth=1.5, 
                        label='Predicted'
                    )
            
            # Add secondary structure if available
            if 'secondary_structure_encoded' in window_df.columns:
                # Create secondary structure bars at the bottom
                for i, row in window_df.iterrows():
                    ss = row['secondary_structure_encoded']
                    resid = row['resid']
                    
                    if ss == 0:  # Helix
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='red', ymin=0, ymax=0.05)
                    elif ss == 1:  # Sheet
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='blue', ymin=0, ymax=0.05)
                    else:  # Loop/Other
                        plt.axvspan(resid-0.4, resid+0.4, alpha=0.2, color='green', ymin=0, ymax=0.05)
                
                # Add legend for secondary structure
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='red', alpha=0.2, label='Helix'),
                    Patch(facecolor='blue', alpha=0.2, label='Sheet'),
                    Patch(facecolor='green', alpha=0.2, label='Loop/Other')
                ]
                
                plt.legend(handles=legend_elements, loc='upper right')
            
            plt.xlabel('Residue ID')
            plt.ylabel('RMSF')
            plt.title(f'Flexibility in Sequence Neighborhood (Domain {selected_domain})')
            plt.grid(alpha=0.3)
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + '.png'
            save_plot(plt, plot_path)
            logger.info(f"Sequence neighborhood plot saved to {plot_path}")

def plot_error_response_surface(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate error response surface plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if not model_names or 'normalized_resid' not in df.columns:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Calculate errors
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Create bins for normalized position and secondary structure
        if 'secondary_structure_encoded' in df_with_preds.columns:
            # Create data for heatmap
            heatmap_data = []
            
            # Create position bins
            df_with_preds['position_bin'] = pd.cut(
                df_with_preds['normalized_resid'], 
                bins=10, 
                labels=range(10)
            )
            
            # Group by position bin and secondary structure
            grouped = df_with_preds.groupby(['position_bin', 'secondary_structure_encoded'])
            
            for (pos_bin, ss), group in grouped:
                heatmap_data.append({
                    'position_bin': pos_bin,
                    'secondary_structure': ss,
                    'mean_error': group[f"{model_name}_abs_error"].mean(),
                    'count': len(group)
                })
            
            heatmap_df = pd.DataFrame(heatmap_data)
            
            # Save to CSV
            output_dir = config["paths"]["output_dir"]
            output_path = os.path.join(
                output_dir, 
                "residue_analysis", 
                f"error_response_surface_{model_name}.csv"
            )
            
            # Create output directory
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            heatmap_df.to_csv(output_path, index=False)
            logger.info(f"Error response surface data saved to {output_path}")
            
            # Create a pivot table for the heatmap
            pivot_df = heatmap_df.pivot(
                index='secondary_structure', 
                columns='position_bin', 
                values='mean_error'
            )
            
            # Create the plot
            plt.figure(figsize=(10, 6))
            
            # Create heatmap
            sns.heatmap(
                pivot_df, 
                cmap='viridis', 
                annot=True, 
                fmt=".2f", 
                linewidths=0.5
            )
            
            # Set labels
            plt.xlabel('Normalized Position Bin')
            plt.ylabel('Secondary Structure (0=Helix, 1=Sheet, 2=Loop)')
            plt.title(f'Error Response Surface: Position vs Structure ({model_name})')
            plt.tight_layout()
            
            # Save the plot
            plot_path = os.path.splitext(output_path)[0] + '.png'
            save_plot(plt, plot_path)
            logger.info(f"Error response surface plot saved to {plot_path}")

def plot_secondary_structure_error_correlation(
    df: pd.DataFrame,
    predictions: Dict[str, np.ndarray],
    target_col: str,
    model_names: List[str],
    config: Dict[str, Any]
) -> None:
    """
    Generate secondary structure error correlation plot data.
    
    Args:
        df: DataFrame with protein data
        predictions: Dictionary mapping model names to predictions
        target_col: Target column name
        model_names: List of model names
        config: Configuration dictionary
    """
    if 'secondary_structure_encoded' not in df.columns or not model_names:
        return
    
    # Use first model
    model_name = model_names[0]
    
    if model_name in predictions:
        # Calculate errors
        df_with_preds = df.copy()
        df_with_preds[f"{model_name}_predicted"] = predictions[model_name]
        df_with_preds[f"{model_name}_error"] = predictions[model_name] - df_with_preds[target_col]
        df_with_preds[f"{model_name}_abs_error"] = np.abs(df_with_preds[f"{model_name}_error"])
        
        # Group by secondary structure
        ss_errors = []
        
        for ss, group in df_with_preds.groupby('secondary_structure_encoded'):
            ss_name = {0: 'Helix', 1: 'Sheet', 2: 'Loop/Other'}.get(ss, str(ss))
            
            # Calculate metrics
            row = {
                'secondary_structure': ss_name,
                'count': len(group),
                'mean_actual': group[target_col].mean(),
                'mean_predicted': group[f"{model_name}_predicted"].mean(),
                'mean_error': group[f"{model_name}_abs_error"].mean(),
                'std_error': group[f"{model_name}_abs_error"].std()
            }
            
            ss_errors.append(row)
        
        ss_error_df = pd.DataFrame(ss_errors)
        
        # Save to CSV
        output_dir = config["paths"]["output_dir"]
        output_path = os.path.join(
            output_dir, 
            "residue_analysis", 
            f"secondary_structure_errors_{model_name}.csv"
        )
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        ss_error_df.to_csv(output_path, index=False)
        logger.info(f"Secondary structure error correlation data saved to {output_path}")
        
        # Create the plot
        plt.figure(figsize=(10, 6))
        
        # Create grouped bar plot
        x = np.arange(len(ss_error_df))
        width = 0.35
        
        plt.bar(
            x - width/2, 
            ss_error_df['mean_actual'], 
            width, 
            label='Actual RMSF'
        )
        
        plt.bar(
            x + width/2, 
            ss_error_df['mean_predicted'], 
            width, 
            label='Predicted RMSF'
        )
        
        plt.xlabel('Secondary Structure')
        plt.ylabel('Mean RMSF')
        plt.title(f'Actual vs Predicted RMSF by Secondary Structure ({model_name})')
        plt.xticks(x, ss_error_df['secondary_structure'])
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.splitext(output_path)[0] + '.png'
        save_plot(plt, plot_path)
        logger.info(f"Secondary structure comparison plot saved to {plot_path}")

def plot_training_validation_curves(
    train_metrics: Dict[str, List[float]],
    val_metrics: Dict[str, List[float]],
    model_name: str,
    config: Dict[str, Any]
) -> None:
    """
    Generate training and validation curves.
    
    Args:
        train_metrics: Dictionary of training metrics by epoch
        val_metrics: Dictionary of validation metrics by epoch
        model_name: Name of the model
        config: Configuration dictionary
    """
    if not train_metrics or not val_metrics:
        return
    
    # Convert to DataFrame
    epochs = len(train_metrics.get('train_loss', []))
    
    if epochs == 0:
        return
    
    curve_data = []
    
    for i in range(epochs):
        row = {'epoch': i}
        
        for metric, values in train_metrics.items():
            if i < len(values):
                metric_name = metric.replace('train_', '')
                row[f"train_{metric_name}"] = values[i]
        
        for metric, values in val_metrics.items():
            if i < len(values):
                metric_name = metric.replace('val_', '')
                row[f"val_{metric_name}"] = values[i]
        
        curve_data.append(row)
    
    curve_df = pd.DataFrame(curve_data)
    
    # Save to CSV
    output_dir = config["paths"]["output_dir"]
    output_path = os.path.join(
        output_dir, 
        "training_performance", 
        f"{model_name}_training_curves.csv"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    curve_df.to_csv(output_path, index=False)
    logger.info(f"Training and validation curves data saved to {output_path}")
    
    # Create the plots
    metrics_to_plot = []
    
    if 'train_loss' in train_metrics and 'val_loss' in val_metrics:
        metrics_to_plot.append(('loss', 'Loss'))
    
    if 'train_r2' in train_metrics and 'val_r2' in val_metrics:
        metrics_to_plot.append(('r2', 'R²'))
    
    for metric, title in metrics_to_plot:
        plt.figure(figsize=(10, 6))
        
        plt.plot(
            curve_df['epoch'], 
            curve_df[f"train_{metric}"], 
            'b-', 
            label=f'Training {title}'
        )
        
        plt.plot(
            curve_df['epoch'], 
            curve_df[f"val_{metric}"], 
            'r-', 
            label=f'Validation {title}'
        )
        
        plt.xlabel('Epoch')
        plt.ylabel(title)
        plt.title(f'{title} Curves for {model_name}')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.join(
            output_dir, 
            "training_performance", 
            f"{model_name}_{metric}_curves.png"
        )
        
        save_plot(plt, plot_path)
        logger.info(f"{title} curves plot saved to {plot_path}")

def plot_neural_network_learning_dynamics(
    train_metrics: Dict[str, List[float]],
    val_metrics: Dict[str, List[float]],
    model_name: str,
    config: Dict[str, Any]
) -> None:
    """
    Generate neural network learning dynamics visualization.
    
    Args:
        train_metrics: Dictionary of training metrics by epoch
        val_metrics: Dictionary of validation metrics by epoch
        model_name: Name of the model
        config: Configuration dictionary
    """
    if not train_metrics or not val_metrics:
        return
    
    # Convert to DataFrame
    epochs = len(train_metrics.get('train_loss', []))
    
    if epochs == 0:
        return
    
    # Create a combined plot with multiple metrics
    plt.figure(figsize=(12, 8))
    
    # Create subplots
    if 'train_loss' in train_metrics and 'val_loss' in val_metrics:
        ax1 = plt.subplot(2, 1, 1)
        
        # Plot loss
        ax1.plot(
            range(epochs), 
            train_metrics['train_loss'], 
            'b-', 
            label='Training Loss'
        )
        
        ax1.plot(
            range(epochs), 
            val_metrics['val_loss'], 
            'r-', 
            label='Validation Loss'
        )
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title(f'Loss Curves for {model_name}')
        ax1.legend()
        ax1.grid(alpha=0.3)
    
    if 'train_r2' in train_metrics and 'val_r2' in val_metrics:
        ax2 = plt.subplot(2, 1, 2)
        
        # Plot R²
        ax2.plot(
            range(epochs), 
            train_metrics['train_r2'], 
            'g-', 
            label='Training R²'
        )
        
        ax2.plot(
            range(epochs), 
            val_metrics['val_r2'], 
            'y-', 
            label='Validation R²'
        )
        
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('R²')
        ax2.set_title(f'R² Curves for {model_name}')
        ax2.legend()
        ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    
    # Save the plot
    output_dir = config["paths"]["output_dir"]
    plot_path = os.path.join(
        output_dir, 
        "training_performance", 
        f"{model_name}_learning_dynamics.png"
    )
    
    # Create output directory
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    
    save_plot(plt, plot_path)
    logger.info(f"Learning dynamics plot saved to {plot_path}")
### Output Utils Files ###
---------------------------------------------------------
==========================================================
Output Result Files (First 15 lines of each file)
==========================================================

Finding output result files...
No output result files found.
==========================================================
End of FlexSeq Context Document
==========================================================
